import os
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("dotenv æ¨¡å—æœªå®‰è£…ï¼Œå°†ç›´æ¥ä½¿ç”¨ç¯å¢ƒå˜é‡")

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import requests
from datetime import datetime, timezone
import openai
from bs4 import BeautifulSoup
from typing import List, Dict
import time
from common.storage import save_structured_items, build_item_id
from common.scoring import score_content

# AI è¿‡æ»¤å…³é”®å­—
AI_KEYWORDS = [
    'ai', 'artificial intelligence', 'machine learning', 'ml',
    'deep learning', 'neural network', 'llm', 'gpt', 'claude',
    'agent', 'autonomous', 'chatbot', 'assistant', 'copilot',
    'generative', 'diffusion', 'transformer', 'embedding',
    'rag', 'retrieval', 'vector', 'semantic search',
    'proactive ai', 'agent infra', 'autonomous agents', 'multi-agent', 'openclaw', 'mcp',
    'context', 'hijacking', 'laude code sdk', 'cowork', 'vibe coding', 'agent-friendly tooling',
    'human-in-the-loop', 'online learning', 'reward model', 'reward fine-tune', 'sft', 'rlhf',
    'rlaif', 'dpo', 'hero user', 'product self-iteration', 'intent prediction', 'non-consensus ai',
    'ai-density', 'agentic workflow', 'workflow'
]

EXCLUDE_KEYWORDS = [
    'crypto', 'nft', 'blockchain', 'web3', 'token',
    'game', 'gaming', 'casino', 'betting',
    'adult', 'dating', 'porn'
]

def _get_int_env(name: str, default: int) -> int:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    try:
        value = int(raw)
    except ValueError:
        print(f"è­¦å‘Š: ç¯å¢ƒå˜é‡ {name}={raw!r} ä¸æ˜¯æ•´æ•°ï¼Œå›é€€é»˜è®¤å€¼ {default}")
        return default
    if value <= 0:
        print(f"è­¦å‘Š: ç¯å¢ƒå˜é‡ {name}={raw!r} å¿…é¡»å¤§äº 0ï¼Œå›é€€é»˜è®¤å€¼ {default}")
        return default
    return value


def _contains_any(text: str, keywords) -> bool:
    if not text:
        return False
    lowered = text.lower()
    return any(k in lowered for k in keywords)


def is_ai_related(*fields: str) -> bool:
    merged = " ".join(filter(None, fields))
    if not merged:
        return False
    if _contains_any(merged, EXCLUDE_KEYWORDS):
        return False
    return _contains_any(merged, AI_KEYWORDS)

# åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºç¿»è¯‘ï¼‰
api_key = os.getenv('OPENAI_API_KEY')
base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
if api_key:
    try:
        client = openai.Client(api_key=api_key, base_url=base_url)
        print(f"æˆåŠŸåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (ä½¿ç”¨APIåœ°å€: {base_url})")
    except Exception as e:
        print(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å¤±è´¥: {e}")
        client = None
else:
    print("æœªè®¾ç½® OPENAI_API_KEYï¼Œå°†ä¸è¿›è¡Œç¿»è¯‘")
    client = None

class GitHubRepo:
    def __init__(self, data: Dict):
        """ä»çˆ¬å–çš„æ•°æ®åˆå§‹åŒ–ä»“åº“å¯¹è±¡"""
        self.name = data.get('name', '')
        self.author = data.get('author', '')
        self.url = data.get('url', '')
        self.description = data.get('description', '')
        self.language = data.get('language', 'Unknown')
        self.stars = data.get('stars', 0)
        self.forks = data.get('forks', 0)
        self.stars_today = data.get('stars_today', 0)
        
        # ç¿»è¯‘æè¿°
        self.translated_description = self.translate_description()
        # å…³é”®è¯
        self.keywords = self.generate_keywords()
        # è¯„åˆ†
        self.score = score_content(
            f"ä»“åº“: {self.author}/{self.name}\næè¿°: {self.description}\nä¸­æ–‡ä»‹ç»: {self.translated_description}\nå…³é”®è¯: {self.keywords}",
            client,
            kind="github",
        )

    def to_content_item(self, rank: int, date_str: str) -> dict:
        merged = f"{self.author}/{self.name} {self.description}".lower()
        hit_keywords = [kw for kw in AI_KEYWORDS if kw in merged]
        return {
            "id": build_item_id("gh", date_str, rank),
            "source": "github",
            "date": date_str,
            "rank": rank,
            "title": f"{self.author}/{self.name}",
            "url": self.url,
            "detail_url": self.url,
            "description_en": self.description,
            "description_zh": self.translated_description,
            "keywords": [k.strip() for k in self.keywords.split(",") if k.strip()],
            "tags": [self.language],
            "metrics": {
                "stars": self.stars,
                "forks": self.forks,
                "stars_today": self.stars_today,
            },
            "media": {"image": None},
            "ai_flags": {"is_ai": True, "hit_keywords": hit_keywords, "hit_excludes": []},
            "score": self.score,
            "raw": {},
        }
    
    def translate_description(self) -> str:
        """ä½¿ç”¨AIç¿»è¯‘é¡¹ç›®æè¿°"""
        if not self.description or client is None:
            return self.description
        
        try:
            print(f"æ­£åœ¨ç¿»è¯‘ {self.author}/{self.name} çš„æè¿°...")
            response = client.chat.completions.create(
                model=os.getenv('GITHUB_MODEL_NAME', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æŠ€æœ¯äº§å“è¯´æ˜ä¹¦æ’°å†™è€…ã€‚"},
                    {"role": "user", "content": (
                        "è¯·å°†ä¸‹é¢çš„ GitHub é¡¹ç›®ç®€ä»‹ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶åœ¨ 2-3 å¥é‡Œè¡¥å……ï¼šä¸»è¦åŠŸèƒ½ã€ç›®æ ‡ç”¨æˆ·/åœºæ™¯ã€ä½¿ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ï¼ˆå°¤å…¶æ˜¯ AI ç›¸å…³ï¼‰ã€‚"
                        "ä¿æŒç®€æ´ã€å‡†ç¡®ã€ä¿¡æ¯é‡æ›´é«˜ã€‚\n\n"
                        f"{self.description}"
                    )}
                ],
                max_tokens=200,
                temperature=0.7,
            )
            translated = response.choices[0].message.content.strip()
            print(f"æˆåŠŸç¿»è¯‘")
            return translated
        except Exception as e:
            print(f"ç¿»è¯‘å¤±è´¥: {e}")
            return self.description

    def generate_keywords(self) -> str:
        """
        ç”Ÿæˆä¸­æ–‡ä¸ºä¸»çš„ AI ç›¸å…³å…³é”®è¯ï¼Œè‹±æ–‡é€—å·åˆ†éš”ã€‚
        ä¸è¾“å‡ºå•ç‹¬â€œAI/äººå·¥æ™ºèƒ½â€ï¼Œè¿‡æ»¤æ’é™¤è¯ï¼Œ5-10 ä¸ªã€‚
        """
        try:
            base_text = f"ä»“åº“: {self.author}/{self.name}\næè¿°: {self.description}"
            if client is None:
                words = (self.name + ", " + self.description).replace("&", ",").replace("|", ",").replace("-", ",").split(",")
                filtered = [w.strip() for w in words if w.strip()]
                filtered = [w for w in filtered if is_ai_related(w)]
                filtered = [w for w in filtered if w.lower() != 'ai' and w != 'äººå·¥æ™ºèƒ½']
                return ", ".join(dict.fromkeys(filtered))

            prompt = (
                "ç”Ÿæˆä»…é™ AI ç›¸å…³çš„ä¸­æ–‡å…³é”®è¯ï¼ˆä¸“æœ‰åè¯å¯ä¿ç•™è‹±æ–‡ï¼‰ï¼Œè‹±æ–‡é€—å·åˆ†éš”ï¼š\n"
                "- è‡³å°‘å« 1 ä¸ª AI_KEYWORDS çš„è¯æˆ–åŒä¹‰è¯ï¼Œä½†ä¸è¦è¾“å‡ºå•ç‹¬â€œAI/äººå·¥æ™ºèƒ½â€ã€‚\n"
                "- ä¸å« EXCLUDE_KEYWORDSã€‚\n"
                "- è¡¥å…… 2-3 ä¸ªåŸºäºé¡¹ç›®åç§°/æŠ€æœ¯/åŠŸèƒ½/æ¶æ„çš„çŸ­å…³é”®è¯ã€‚\n"
                "- æ€»æ•° 5-10 ä¸ªï¼Œå»é‡ã€å»ç©ºæ ¼ã€‚\n"
                f"AI_KEYWORDS: {', '.join(AI_KEYWORDS)}\n"
                f"EXCLUDE_KEYWORDS: {', '.join(EXCLUDE_KEYWORDS)}\n"
                f"{base_text}"
            )
            resp = client.chat.completions.create(
                model=os.getenv('GITHUB_MODEL_NAME', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "ç”¨ä¸­æ–‡è¾“å‡ºå…³é”®è¯ï¼Œæ»¡è¶³ç»™å®šçº¦æŸã€‚"},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=80,
                temperature=0.5,
            )
            keywords = resp.choices[0].message.content.strip()
            if ',' not in keywords:
                keywords = ', '.join(keywords.split())
            items = [k.strip() for k in keywords.split(',') if k.strip()]
            items = [k for k in items if k.lower() not in ('ai',) and k != 'äººå·¥æ™ºèƒ½']
            items = [k for k in items if not _contains_any(k, EXCLUDE_KEYWORDS)]
            if not any(_contains_any(k, AI_KEYWORDS) for k in items):
                fallback = next((kw for kw in AI_KEYWORDS if kw != 'ai' and kw in (self.name + ' ' + self.description).lower()), 'agent')
                items.append(fallback)
            return ", ".join(dict.fromkeys(items))
        except Exception as e:
            print(f"å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def to_markdown(self, rank: int) -> str:
        """è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        # æ ¼å¼åŒ–æ˜Ÿæ ‡æ•°
        def format_number(num):
            if num >= 1000:
                return f"{num/1000:.1f}k"
            return str(num)
        
        stars_display = format_number(self.stars)
        forks_display = format_number(self.forks)
        
        # ä»Šæ—¥æ–°å¢æ˜Ÿæ ‡
        today_stars = f" (+{format_number(self.stars_today)} today)" if self.stars_today > 0 else ""
        
        return f"""## [{rank}. {self.author}/{self.name}]({self.url})

**è¯­è¨€**: {self.language}  
**Stars**: â­ {stars_display}{today_stars} | **Forks**: ğŸ”± {forks_display}

**åŸå§‹æè¿°**: {self.description}

**ä¸­æ–‡ä»‹ç»ï¼ˆå«åŠŸèƒ½/åœºæ™¯/æŠ€æœ¯ï¼‰**: {self.translated_description}

**å…³é”®è¯**: {self.keywords}

**è¯„åˆ†**: {self.score.get('total', 0)}

**é¡¹ç›®åœ°å€**: [GitHub]({self.url})

---

"""

def fetch_github_trending(language: str = "", since: str = "daily") -> List[GitHubRepo]:
    """
    çˆ¬å–GitHub Trendingé¡µé¢
    
    å‚æ•°:
        language: ç¼–ç¨‹è¯­è¨€è¿‡æ»¤ (å¦‚ "python", "javascript", "" è¡¨ç¤ºæ‰€æœ‰)
        since: æ—¶é—´èŒƒå›´ ("daily", "weekly", "monthly")
    """
    base_url = "https://github.com/trending"
    
    # æ„å»ºURL
    if language:
        url = f"{base_url}/{language}"
    else:
        url = base_url
    
    params = {"since": since}
    
    print(f"æ­£åœ¨ä»GitHub Trendingè·å–æ•°æ®...")
    print(f"è¯­è¨€: {language if language else 'å…¨éƒ¨'}, æ—¶é—´èŒƒå›´: {since}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }
    
    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰ä»“åº“æ¡ç›®
        repos = []
        articles = soup.find_all('article', class_='Box-row')
        
        print(f"æ‰¾åˆ° {len(articles)} ä¸ªä»“åº“")
        
        for article in articles:
            try:
                # æå–ä»“åº“ä¿¡æ¯
                h2 = article.find('h2', class_='h3')
                if not h2:
                    continue
                
                a_tag = h2.find('a')
                if not a_tag:
                    continue
                
                # ä»“åº“å…¨å (author/repo)
                full_name = a_tag.get('href', '').strip('/')
                if '/' not in full_name:
                    continue
                
                author, name = full_name.split('/', 1)
                
                # æè¿°
                desc_tag = article.find('p', class_='col-9')
                description = desc_tag.get_text(strip=True) if desc_tag else ''
                
                # è¯­è¨€
                lang_tag = article.find('span', attrs={'itemprop': 'programmingLanguage'})
                language = lang_tag.get_text(strip=True) if lang_tag else 'Unknown'
                
                # æ˜Ÿæ ‡æ•°
                star_tag = article.find('svg', class_='octicon-star')
                if star_tag:
                    star_parent = star_tag.find_parent('a')
                    stars_text = star_parent.get_text(strip=True) if star_parent else '0'
                    stars = parse_number(stars_text)
                else:
                    stars = 0
                
                # Forkæ•°
                fork_tag = article.find('svg', class_='octicon-repo-forked')
                if fork_tag:
                    fork_parent = fork_tag.find_parent('a')
                    forks_text = fork_parent.get_text(strip=True) if fork_parent else '0'
                    forks = parse_number(forks_text)
                else:
                    forks = 0
                
                # ä»Šæ—¥æ–°å¢æ˜Ÿæ ‡
                today_stars_tag = article.find('span', class_='d-inline-block float-sm-right')
                stars_today = 0
                if today_stars_tag:
                    today_text = today_stars_tag.get_text(strip=True)
                    stars_today = parse_number(today_text.split()[0])
                
                repo_data = {
                    'name': name,
                    'author': author,
                    'url': f"https://github.com/{full_name}",
                    'description': description,
                    'language': language,
                    'stars': stars,
                    'forks': forks,
                    'stars_today': stars_today
                }

                # AI ç›¸å…³ç­›é€‰
                if not is_ai_related(name, description):
                    print(f"è·³è¿‡éAIä»“åº“: {author}/{name}")
                    continue
                
                repos.append(GitHubRepo(repo_data))
                
            except Exception as e:
                print(f"è§£æä»“åº“ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                continue
        
        if not repos:
            print("æœªèƒ½è§£æåˆ°ä»»ä½•ä»“åº“ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
            return get_mock_repos()
        
        print(f"æˆåŠŸè§£æ {len(repos)} ä¸ªä»“åº“")
        return repos
        
    except Exception as e:
        print(f"è·å–GitHub Trendingå¤±è´¥: {e}")
        print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
        return get_mock_repos()

def parse_number(text: str) -> int:
    """è§£æGitHubä¸Šçš„æ•°å­—æ ¼å¼ (å¦‚ "1.2k" -> 1200)"""
    text = text.strip().replace(',', '')
    if 'k' in text.lower():
        return int(float(text.lower().replace('k', '')) * 1000)
    try:
        return int(text)
    except:
        return 0

def get_mock_repos() -> List[GitHubRepo]:
    """è¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
    mock_data = [
        {
            'name': 'awesome-project',
            'author': 'test-user',
            'url': 'https://github.com/test-user/awesome-project',
            'description': 'An awesome project for demonstration purposes',
            'language': 'Python',
            'stars': 12500,
            'forks': 2300,
            'stars_today': 150
        }
    ]
    return [GitHubRepo(data) for data in mock_data]

def generate_markdown(repos: List[GitHubRepo], date_str: str, language: str = "", since: str = "daily"):
    """ç”ŸæˆMarkdownæ–‡ä»¶å¹¶ä¿å­˜åˆ°data/githubç›®å½•"""
    
    # æ—¶é—´èŒƒå›´ä¸­æ–‡
    since_cn = {"daily": "æ—¥æ¦œ", "weekly": "å‘¨æ¦œ", "monthly": "æœˆæ¦œ"}.get(since, "æ—¥æ¦œ")
    
    # è¯­è¨€è¿‡æ»¤
    lang_display = f"{language.upper()} " if language else ""
    
    markdown_content = f"# GitHub Trending {lang_display}{since_cn} | {date_str}\n\n"
    markdown_content += f"> å…± {len(repos)} ä¸ªé¡¹ç›®\n\n"
    
    # æŒ‰è¯­è¨€åˆ†ç»„
    repos_by_language = {}
    for repo in repos:
        lang = repo.language
        if lang not in repos_by_language:
            repos_by_language[lang] = []
        repos_by_language[lang].append(repo)
    
    # ç”Ÿæˆç›®å½•
    markdown_content += "## ğŸ“‘ ç›®å½•\n\n"
    for lang in sorted(repos_by_language.keys()):
        markdown_content += f"- [{lang}](#{lang.replace(' ', '-').replace('+', 'plus').replace('#', 'sharp')}) ({len(repos_by_language[lang])} ä¸ªé¡¹ç›®)\n"
    markdown_content += "\n---\n\n"
    
    # æŒ‰è¯­è¨€ç”Ÿæˆå†…å®¹
    rank = 1
    structured_items = []
    for lang in sorted(repos_by_language.keys()):
        markdown_content += f"## {lang}\n\n"
        for repo in repos_by_language[lang]:
            markdown_content += repo.to_markdown(rank)
            structured_items.append(repo.to_content_item(rank, date_str))
            rank += 1
    
    # ç¡®ä¿data/githubç›®å½•å­˜åœ¨
    os.makedirs('data/github', exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶
    lang_suffix = f"-{language}" if language else ""
    file_name = f"data/github/github-trending{lang_suffix}-{date_str}.md"
    
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸã€‚")
    save_structured_items(date_str, structured_items)

def main():
    """ä¸»å‡½æ•°"""
    today = datetime.now(timezone.utc)
    date_str = today.strftime('%Y-%m-%d')
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    language = os.getenv('GITHUB_LANGUAGE', '')  # ç•™ç©ºè¡¨ç¤ºæ‰€æœ‰è¯­è¨€
    since = os.getenv('GITHUB_SINCE', 'daily')  # daily, weekly, monthly
    max_results = _get_int_env('GITHUB_MAX_RESULTS', 25)
    
    print(f"=== GitHub Trending çˆ¬å–å¼€å§‹ ===")
    print(f"æ—¥æœŸ: {date_str}")
    print(f"è¯­è¨€: {language if language else 'å…¨éƒ¨'}")
    print(f"æ—¶é—´èŒƒå›´: {since}")
    
    # è·å–trendingä»“åº“
    repos = fetch_github_trending(language=language, since=since)
    
    # é™åˆ¶æ•°é‡
    repos = repos[:max_results]
    
    # ç”ŸæˆMarkdown
    generate_markdown(repos, date_str, language, since)
    
    print(f"=== GitHub Trending çˆ¬å–å®Œæˆ ===")

if __name__ == "__main__":
    main()
