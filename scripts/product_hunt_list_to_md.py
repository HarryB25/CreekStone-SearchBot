import os
import signal
try:
    from dotenv import load_dotenv
    # åŠ è½½ .env æ–‡ä»¶
    load_dotenv()
except ImportError:
    # åœ¨ GitHub Actions ç­‰ç¯å¢ƒä¸­ï¼Œç¯å¢ƒå˜é‡å·²ç»è®¾ç½®å¥½ï¼Œä¸éœ€è¦ dotenv
    print("dotenv æ¨¡å—æœªå®‰è£…ï¼Œå°†ç›´æ¥ä½¿ç”¨ç¯å¢ƒå˜é‡")

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import requests
from datetime import datetime, timedelta, timezone
import openai
from bs4 import BeautifulSoup
import pytz
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from common.storage import save_structured_items, build_item_id
from common.scoring import score_content
from common.keyword_utils import (
    investor_keyword_prompt,
    investor_keyword_repair_prompt,
    extract_keywords_from_response,
    finalize_keywords,
    keyword_quality_check,
    synthesize_keywords_from_context,
)

# AI ç›¸å…³ç­›é€‰ä¸å…³é”®è¯æ§åˆ¶
AI_KEYWORDS = [
    'ai', 'artificial intelligence', 'machine learning', 'ml',
    'deep learning', 'neural network', 'llm', 'gpt', 'claude',
    'agent', 'autonomous', 'chatbot', 'assistant', 'copilot',
    'generative', 'diffusion', 'transformer', 'embedding',
    'rag', 'retrieval', 'vector', 'semantic search',
    # æ–°å¢ agent/AI ç›¸å…³é•¿å°¾è¯
    'proactive ai', 'agent infra', 'autonomous agents', 'multi-agent', 'openclaw', 'mcp',
    'context', 'hijacking', 'laude code sdk', 'cowork', 'vibe coding', 'agent-friendly tooling',
    'human-in-the-loop', 'online learning', 'reward model', 'reward fine-tune', 'sft', 'rlhf',
    'rlaif', 'dpo', 'hero user', 'product self-iteration', 'intent prediction', 'non-consensus ai',
    'ai-density', 'agentic workflow', 'workflow'
]

EXCLUDE_KEYWORDS = [
    'crypto', 'nft', 'blockchain', 'web3', 'porn'
]

def _allow_mock_data() -> bool:
    return os.getenv("ALLOW_MOCK_DATA", "").strip().lower() == "true"


def _get_request_timeout() -> float:
    raw = os.getenv("OPENAI_REQUEST_TIMEOUT", "60").strip()
    try:
        value = float(raw)
        if value > 0:
            return value
    except Exception:
        pass
    return 60.0


def _get_base_url(default: str = "https://api.openai.com/v1") -> str:
    raw = os.getenv("OPENAI_BASE_URL")
    if raw is None or raw.strip() == "":
        return default
    return raw.strip()


def _get_model_name() -> str:
    raw = os.getenv("OPENAI_MODEL", "").strip()
    return raw or "gpt-5-2025-08-07"


def _is_gpt5_model(model_name: str) -> bool:
    return model_name.startswith("gpt-5")


def _call_with_hard_timeout(fn, timeout_sec: float):
    if timeout_sec <= 0:
        return fn()
    if not hasattr(signal, "setitimer") or not hasattr(signal, "SIGALRM"):
        return fn()

    def _handler(_signum, _frame):
        raise TimeoutError(f"request timed out after {timeout_sec}s")

    old = signal.getsignal(signal.SIGALRM)
    signal.signal(signal.SIGALRM, _handler)
    signal.setitimer(signal.ITIMER_REAL, timeout_sec)
    try:
        return fn()
    finally:
        signal.setitimer(signal.ITIMER_REAL, 0)
        signal.signal(signal.SIGALRM, old)


def _looks_mostly_ascii(text: str) -> bool:
    if not text:
        return True
    ascii_chars = sum(1 for ch in text if ch.isascii())
    return ascii_chars / max(1, len(text)) > 0.85


def _chat_json_content(messages, max_tokens: int, temperature: float) -> str:
    """å…¼å®¹éƒ¨åˆ†ç½‘å…³å¯¹ response_format çš„ä¸å®Œæ•´æ”¯æŒã€‚"""
    model_name = _get_model_name()
    timeout = _get_request_timeout()
    kwargs = {
        "model": model_name,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "response_format": {"type": "json_object"},
        "timeout": timeout,
    }
    if _is_gpt5_model(model_name):
        kwargs["reasoning_effort"] = "low"
    try:
        resp = _call_with_hard_timeout(lambda: client.chat.completions.create(**kwargs), timeout + 2)
    except TypeError:
        kwargs.pop("reasoning_effort", None)
        resp = _call_with_hard_timeout(lambda: client.chat.completions.create(**kwargs), timeout + 2)
    content = (resp.choices[0].message.content or "").strip()
    if content:
        return content
    retry_kwargs = dict(kwargs)
    retry_kwargs.pop("response_format", None)
    retry_kwargs["max_tokens"] = max(max_tokens, 1200 if _is_gpt5_model(model_name) else 300)
    resp = _call_with_hard_timeout(lambda: client.chat.completions.create(**retry_kwargs), timeout + 2)
    return (resp.choices[0].message.content or "").strip()


def _get_http_timeout(default: float = 45.0) -> float:
    raw = os.getenv("PRODUCTHUNT_HTTP_TIMEOUT", "").strip()
    if not raw:
        return default
    try:
        value = float(raw)
    except ValueError:
        return default
    return value if value > 0 else default


def _get_max_posts(default: int = 30) -> int:
    raw = os.getenv("PRODUCTHUNT_MAX_POSTS", "").strip()
    if not raw:
        return default
    try:
        value = int(raw)
    except Exception:
        return default
    return max(1, min(100, value))


def _contains_any(text: str, keywords) -> bool:
    if not text:
        return False
    lowered = text.lower()
    return any(k in lowered for k in keywords)


def is_ai_related(*fields: str) -> bool:
    """Return True when text contains at least one AI keyword and no excluded keyword."""
    merged = " ".join(filter(None, fields))
    if not merged:
        return False
    if _contains_any(merged, EXCLUDE_KEYWORDS):
        return False
    return _contains_any(merged, AI_KEYWORDS)

# åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
api_key = os.getenv('OPENAI_API_KEY')
base_url = _get_base_url()  # é»˜è®¤ä½¿ç”¨å®˜æ–¹APIåœ°å€
if not api_key:
    print("è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œå°†æ— æ³•ä½¿ç”¨ OpenAI æœåŠ¡")
    client = None
else:
    openai.api_key = api_key
    try:
        client = openai.Client(api_key=api_key, base_url=base_url)  # æ”¯æŒè‡ªå®šä¹‰APIåœ°å€
        print(f"æˆåŠŸåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (ä½¿ç”¨APIåœ°å€: {base_url})")
    except Exception as e:
        print(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å¤±è´¥: {e}")
        client = None

class Product:
    def __init__(self, id: str, name: str, tagline: str, description: str, votesCount: int, createdAt: str, featuredAt: str, website: str, url: str, media=None, **kwargs):
        self.name = name
        self.tagline = tagline
        self.description = description
        self.votes_count = votesCount
        self.created_at = self.convert_to_beijing_time(createdAt)
        self.featured = "æ˜¯" if featuredAt else "å¦"
        self.website = website
        self.url = url
        self.og_image_url = self.get_image_url_from_media(media)
        # å…ˆç¿»è¯‘ï¼Œå†æŠ½å…³é”®è¯ï¼Œå‡å°‘è‹±æ–‡ç¢è¯æ±¡æŸ“
        self.translated_tagline = self.translate_text(self.tagline)
        self.translated_description = self.translate_text(self.description)
        self.keyword = self.generate_keywords()

    def get_image_url_from_media(self, media):
        """ä»APIè¿”å›çš„mediaå­—æ®µä¸­è·å–å›¾ç‰‡URL"""
        try:
            if media and isinstance(media, list) and len(media) > 0:
                # ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡
                image_url = media[0].get('url', '')
                if image_url:
                    print(f"æˆåŠŸä»APIè·å–å›¾ç‰‡URL: {self.name}")
                    return image_url
            
            # å¦‚æœAPIæ²¡æœ‰è¿”å›å›¾ç‰‡ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            print(f"APIæœªè¿”å›å›¾ç‰‡ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {self.name}")
            backup_url = self.fetch_og_image_url()
            if backup_url:
                print(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è·å–å›¾ç‰‡URLæˆåŠŸ: {self.name}")
                return backup_url
            else:
                print(f"æ— æ³•è·å–å›¾ç‰‡URL: {self.name}")
                
            return ""
        except Exception as e:
            print(f"è·å–å›¾ç‰‡URLæ—¶å‡ºé”™: {self.name}, é”™è¯¯: {e}")
            return ""

    def fetch_og_image_url(self) -> str:
        """è·å–äº§å“çš„Open Graphå›¾ç‰‡URLï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            response = requests.get(self.url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                # æŸ¥æ‰¾og:image metaæ ‡ç­¾
                og_image = soup.find("meta", property="og:image")
                if og_image:
                    return og_image["content"]
                # å¤‡ç”¨:æŸ¥æ‰¾twitter:image metaæ ‡ç­¾
                twitter_image = soup.find("meta", name="twitter:image") 
                if twitter_image:
                    return twitter_image["content"]
            return ""
        except Exception as e:
            print(f"è·å–OGå›¾ç‰‡URLæ—¶å‡ºé”™: {self.name}, é”™è¯¯: {e}")
            return ""

    def generate_keywords(self) -> str:
        """ç”Ÿæˆä¸­æ€§å…³é”®è¯ï¼šè´´åˆå†…å®¹ï¼Œæœ¯è¯­ç»Ÿä¸€ï¼Œå‡å°‘è‹±æ–‡å™ªéŸ³ã€‚"""
        try:
            zh_tagline = (self.translated_tagline or "").strip()
            zh_desc = (self.translated_description or "").strip()
            context = f"{self.name}\n{zh_tagline or self.tagline}\n{zh_desc or self.description}"
            prompt_desc = self.description
            if zh_desc:
                prompt_desc = f"{self.description}\nä¸­æ–‡å‚è€ƒ: {zh_desc}"

            # å¦‚æœ OpenAI å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            if client is None:
                print(f"OpenAI å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯ç”Ÿæˆæ–¹æ³•: {self.name}")
                words = (self.name + ", " + self.tagline + ", " + self.description).replace("&", ",").replace("|", ",").replace("-", ",").split(",")
                rough = [w.strip() for w in words if w.strip()]
                normalized = finalize_keywords(rough, context, source="producthunt")
                ok, _ = keyword_quality_check(normalized, context, source="producthunt")
                if not ok:
                    normalized = synthesize_keywords_from_context(context, source="producthunt", max_items=10)
                return ", ".join(normalized)

            try:
                print(f"æ­£åœ¨ä¸º {self.name} ç”Ÿæˆå…³é”®è¯...")
                raw = _chat_json_content(
                    messages=investor_keyword_prompt(
                        subject="Product",
                        title=self.name,
                        subtitle=zh_tagline or self.tagline,
                        description=prompt_desc,
                    ),
                    max_tokens=1200 if _is_gpt5_model(_get_model_name()) else 220,
                    temperature=0.3,
                )
                keywords_list = extract_keywords_from_response(raw)
                keywords_list = finalize_keywords(keywords_list, context, source="producthunt")
                ok, reason = keyword_quality_check(keywords_list, context, source="producthunt")
                if not ok:
                    repaired_raw = _chat_json_content(
                        messages=investor_keyword_repair_prompt(
                            subject="Product",
                            title=self.name,
                            subtitle=zh_tagline or self.tagline,
                            description=prompt_desc,
                            bad_keywords=keywords_list,
                            reason=reason,
                        ),
                        max_tokens=1000 if _is_gpt5_model(_get_model_name()) else 220,
                        temperature=0.2,
                    )
                    repaired = extract_keywords_from_response(repaired_raw)
                    repaired = finalize_keywords(repaired, context, source="producthunt")
                    repaired_ok, _ = keyword_quality_check(repaired, context, source="producthunt")
                    if repaired_ok:
                        keywords_list = repaired
                    else:
                        keywords_list = synthesize_keywords_from_context(context, source="producthunt", max_items=10)
                if not keywords_list:
                    keywords_list = synthesize_keywords_from_context(context, source="producthunt", max_items=10)
                keywords = ", ".join(keywords_list)
                print(f"æˆåŠŸä¸º {self.name} ç”Ÿæˆå…³é”®è¯")
                return keywords
            except Exception as e:
                print(f"OpenAI API è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯ç”Ÿæˆæ–¹æ³•: {e}")
                # å¤‡ç”¨æ–¹æ³•ï¼šä»æ ‡é¢˜å’Œæ ‡è¯­ä¸­æå–å…³é”®è¯
                words = (self.name + ", " + self.tagline + ", " + self.description).replace("&", ",").replace("|", ",").replace("-", ",").split(",")
                filtered = finalize_keywords([w.strip() for w in words if w.strip()], context, source="producthunt")
                ok, _ = keyword_quality_check(filtered, context, source="producthunt")
                if not ok:
                    filtered = synthesize_keywords_from_context(context, source="producthunt", max_items=10)
                return ", ".join(filtered)
        except Exception as e:
            print(f"å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}")
            fallback_context = f"{self.name}\n{self.tagline}\n{self.description}"
            fallback = finalize_keywords([self.name, self.tagline], fallback_context, source="producthunt")
            ok, _ = keyword_quality_check(fallback, fallback_context, source="producthunt")
            if not ok:
                fallback = synthesize_keywords_from_context(fallback_context, source="producthunt", max_items=10)
            return ", ".join(fallback)

    def translate_text(self, text: str) -> str:
        """ä½¿ç”¨OpenAIç¿»è¯‘æ–‡æœ¬å†…å®¹"""
        try:
            if not text or not text.strip():
                return text
            # å¦‚æœ OpenAI å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›åŸæ–‡
            if client is None:
                print(f"OpenAI å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•ç¿»è¯‘: {self.name}")
                return text
                
            try:
                print(f"æ­£åœ¨ç¿»è¯‘ {self.name} çš„å†…å®¹...")
                for _ in range(3):
                    kwargs = {
                        "model": _get_model_name(),
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šä¸­è‹±ç¿»è¯‘ã€‚è¯·å°†è¾“å…¥å‡†ç¡®ç¿»è¯‘æˆåœ°é“ä¸­æ–‡ï¼Œä¿ç•™å¿…è¦æŠ€æœ¯æœ¯è¯­ã€‚åªè¾“å‡ºç¿»è¯‘ç»“æœã€‚"},
                            {"role": "user", "content": text},
                        ],
                        "max_tokens": 1200 if _is_gpt5_model(_get_model_name()) else 500,
                        "temperature": 0.2,
                        "timeout": _get_request_timeout(),
                    }
                    if _is_gpt5_model(_get_model_name()):
                        kwargs["reasoning_effort"] = "low"
                    try:
                        response = client.chat.completions.create(**kwargs)
                    except TypeError:
                        kwargs.pop("reasoning_effort", None)
                        response = client.chat.completions.create(**kwargs)
                    translated_text = (response.choices[0].message.content or "").strip()
                    if translated_text and len(translated_text) >= 4:
                        # è‹¥å‡ ä¹å…¨è‹±æ–‡ï¼Œç»§ç»­é‡è¯•ä¸€æ¬¡
                        if _looks_mostly_ascii(translated_text):
                            continue
                        print(f"æˆåŠŸç¿»è¯‘ {self.name} çš„å†…å®¹")
                        return translated_text
                print(f"ç¿»è¯‘ç»“æœä¸ºç©ºæˆ–è´¨é‡ä¸è¶³ï¼Œå›é€€åŸæ–‡: {self.name}")
                return text
            except Exception as e:
                print(f"OpenAI API ç¿»è¯‘å¤±è´¥: {e}")
                # å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸæ–‡
                return text
        except Exception as e:
            print(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return text

    def convert_to_beijing_time(self, utc_time_str: str) -> str:
        """å°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´"""
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        beijing_tz = pytz.timezone('Asia/Shanghai')
        beijing_time = utc_time.replace(tzinfo=pytz.utc).astimezone(beijing_tz)
        return beijing_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %p%I:%M (åŒ—äº¬æ—¶é—´)')

    def to_markdown(self, rank: int) -> str:
        """è¿”å›äº§å“æ•°æ®çš„Markdownæ ¼å¼"""
        og_image_markdown = f"![{self.name}]({self.og_image_url})"
        # ç®€çŸ­è¯„åˆ†å±•ç¤º
        score = score_content(
            f"åç§°: {self.name}\næ ‡è¯­: {self.tagline}\næè¿°: {self.description}\nå…³é”®è¯: {self.keyword}",
            client,
            kind="producthunt",
        )
        total_score = score.get("total", 0)
        return (
            f"## [{rank}. {self.name}]({self.url})\n"
            f"**æ ‡è¯­**ï¼š{self.translated_tagline}\n"
            f"**ä»‹ç»**ï¼š{self.translated_description}\n"
            f"**äº§å“ç½‘ç«™**: [ç«‹å³è®¿é—®]({self.website})\n"
            f"**Product Hunt**: [View on Product Hunt]({self.url})\n\n"
            f"{og_image_markdown}\n\n"
            f"**å…³é”®è¯**ï¼š{self.keyword}\n"
            f"**ç¥¨æ•°**: ğŸ”º{self.votes_count}\n"
            f"**æ˜¯å¦ç²¾é€‰**ï¼š{self.featured}\n"
            f"**å‘å¸ƒæ—¶é—´**ï¼š{self.created_at}\n"
            f"**è¯„åˆ†**ï¼š{total_score}\n\n"
            f"---\n\n"
        )

    def to_content_item(self, rank: int, date_str: str) -> dict:
        merged_text = f"{self.name} {self.tagline} {self.description}".lower()
        hit_keywords = [kw for kw in AI_KEYWORDS if kw in merged_text]
        score = score_content(
            f"åç§°: {self.name}\næ ‡è¯­: {self.tagline}\næè¿°: {self.description}\nå…³é”®è¯: {self.keyword}",
            client,
            kind="producthunt",
        )
        return {
            "id": build_item_id("ph", date_str, rank),
            "source": "producthunt",
            "date": date_str,
            "rank": rank,
            "title": self.name,
            "url": self.url,
            "detail_url": self.website,
            "description_en": self.description,
            "description_zh": self.translated_description,
            "keywords": [k.strip() for k in self.keyword.split(",") if k.strip()],
            "tags": ["Product Hunt"],
            "metrics": {"votes": self.votes_count, "featured": self.featured},
            "media": {"image": self.og_image_url},
            "ai_flags": {"is_ai": True, "hit_keywords": hit_keywords, "hit_excludes": []},
            "score": score,
            "raw": {
                "tagline": self.tagline,
                "created_at": self.created_at,
            },
        }

def get_producthunt_token():
    """è·å– Product Hunt è®¿é—®ä»¤ç‰Œ"""
    # ä¼˜å…ˆä½¿ç”¨ PRODUCTHUNT_DEVELOPER_TOKEN ç¯å¢ƒå˜é‡
    developer_token = os.getenv('PRODUCTHUNT_DEVELOPER_TOKEN')
    if developer_token:
        print("ä½¿ç”¨ PRODUCTHUNT_DEVELOPER_TOKEN ç¯å¢ƒå˜é‡")
        return developer_token
    
    # å¦‚æœæ²¡æœ‰ developer tokenï¼Œå°è¯•ä½¿ç”¨ client credentials è·å–è®¿é—®ä»¤ç‰Œ
    client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
    client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')
    
    if not client_id or not client_secret:
        raise Exception("Product Hunt client ID or client secret not found in environment variables")
    
    # ä½¿ç”¨ client credentials è·å–è®¿é—®ä»¤ç‰Œ
    token_url = "https://api.producthunt.com/v2/oauth/token"
    payload = {
        "client_id": client_id,
        "client_secret": client_secret,
        "grant_type": "client_credentials"
    }
    
    try:
        response = requests.post(token_url, json=payload, timeout=_get_http_timeout())
        response.raise_for_status()
        token_data = response.json()
        return token_data.get("access_token")
    except Exception as e:
        print(f"è·å– Product Hunt è®¿é—®ä»¤ç‰Œæ—¶å‡ºé”™: {e}")
        raise Exception(f"Failed to get Product Hunt access token: {e}")

def fetch_product_hunt_data(target_date: str = ""):
    """ä»Product Huntè·å–æŒ‡å®šæ—¥æœŸçš„Top 30æ•°æ®ï¼ˆé»˜è®¤å‰ä¸€å¤©ï¼‰"""
    token = get_producthunt_token()
    if target_date:
        date_str = target_date
    else:
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        date_str = yesterday.strftime('%Y-%m-%d')
    url = "https://api.producthunt.com/v2/api/graphql"
    
    # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´ä¿¡æ¯
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}",
        "User-Agent": "DecohackBot/1.0 (https://decohack.com)",
        "Origin": "https://decohack.com",
        "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
        "Connection": "keep-alive"
    }

    # è®¾ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=3,  # æœ€å¤šé‡è¯•3æ¬¡
        backoff_factor=1,  # é‡è¯•é—´éš”æ—¶é—´
        status_forcelist=[429, 500, 502, 503, 504]  # éœ€è¦é‡è¯•çš„HTTPçŠ¶æ€ç 
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session = requests.Session()
    session.mount("https://", adapter)

    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
          media {
            url
            type
            videoUrl
          }
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """

    max_posts = _get_max_posts(30)
    all_posts = []
    has_next_page = True
    cursor = ""

    while has_next_page and len(all_posts) < max_posts:
        query = base_query % (date_str, date_str, cursor)
        try:
            response = session.post(
                url,
                headers=headers,
                json={"query": query},
                timeout=_get_http_timeout(),
            )
            response.raise_for_status()  # æŠ›å‡ºé200çŠ¶æ€ç çš„å¼‚å¸¸
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            raise Exception(f"Failed to fetch data from Product Hunt: {e}")

        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)

        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']

    # ä¿ç•™å‰ N ä¸ªäº§å“ï¼ˆé»˜è®¤ 30ï¼‰
    return [Product(**post) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:max_posts]]

def fetch_mock_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
    print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
    mock_products = [
        {
            "id": "1",
            "name": "Venice",
            "tagline": "Private & censorship-resistant AI | Unlock unlimited intelligence",
            "description": "Venice is a private, censorship-resistant AI platform powered by open-source models and decentralized infrastructure. The app combines the benefits of decentralized blockchain technology with the power of generative AI.",
            "votesCount": 566,
            "createdAt": "2025-03-07T16:01:00Z",
            "featuredAt": "2025-03-07T16:01:00Z",
            "website": "https://www.producthunt.com/r/4D6Z6F7I3SXTGN",
            "url": "https://www.producthunt.com/posts/venice-3",
            "media": [
                {
                    "url": "https://ph-files.imgix.net/97baee49-6dda-47f5-8a47-91d2c56e1976.jpeg",
                    "type": "image",
                    "videoUrl": None
                }
            ]
        },
        {
            "id": "2",
            "name": "Mistral OCR",
            "tagline": "Introducing the world's most powerful document understanding API",
            "description": "Introducing Mistral OCRâ€”an advanced, lightweight optical character recognition model focused on speed, accuracy, and efficiency. Whether extracting text from images or digitizing documents, it delivers top-tier performance with ease.",
            "votesCount": 477,
            "createdAt": "2025-03-07T16:01:00Z",
            "featuredAt": "2025-03-07T16:01:00Z",
            "website": "https://www.producthunt.com/r/SPXNTAWQSVRLGH",
            "url": "https://www.producthunt.com/posts/mistral-ocr",
            "media": [
                {
                    "url": "https://ph-files.imgix.net/4224517b-29e4-4944-98c9-2eee59374870.png",
                    "type": "image",
                    "videoUrl": None
                }
            ]
        }
    ]
    return [Product(**product) for product in mock_products]

def generate_markdown(products, date_str):
    """ç”ŸæˆMarkdownå†…å®¹å¹¶ä¿å­˜åˆ°data/producthuntç›®å½•"""
    markdown_content = f"# PHä»Šæ—¥çƒ­æ¦œ | {date_str}\n\n"
    rank = 1
    structured_items = []
    for product in products:
        if not is_ai_related(product.name, product.tagline, product.description):
            print(f"è·³è¿‡éAIäº§å“: {product.name}")
            continue
        markdown_content += product.to_markdown(rank)
        structured_items.append(product.to_content_item(rank, date_str))
        rank += 1

    # ç¡®ä¿ data/producthunt ç›®å½•å­˜åœ¨
    os.makedirs('data/producthunt', exist_ok=True)

    # ä¿®æ”¹æ–‡ä»¶ä¿å­˜è·¯å¾„åˆ° data/producthunt ç›®å½•
    file_name = f"data/producthunt/producthunt-daily-{date_str}.md"
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸå¹¶å·²è¦†ç›–ã€‚")
    save_structured_items(date_str, structured_items)


def main():
    # æ”¯æŒæŠ“å–æ—¥æœŸå’Œè¾“å‡ºæ—¥æœŸåˆ†ç¦»ï¼š
    # - PRODUCTHUNT_TARGET_DATE/PRODUCTHUNT_FETCH_DATE: å®é™…æŠ“å–çª—å£
    # - PRODUCTHUNT_OUTPUT_DATE: å†™å…¥ç»“æ„åŒ–æ•°æ®ä¸æ–‡ä»¶æ—¥æœŸ
    fetch_date = (
        os.getenv("PRODUCTHUNT_FETCH_DATE", "").strip()
        or os.getenv("PRODUCTHUNT_TARGET_DATE", "").strip()
    )
    output_date = os.getenv("PRODUCTHUNT_OUTPUT_DATE", "").strip()

    if fetch_date:
        target_date = fetch_date
    else:
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        target_date = yesterday.strftime('%Y-%m-%d')

    if output_date:
        date_str = output_date
    else:
        date_str = target_date

    print(f"Product Hunt æŠ“å–æ—¥æœŸ: {target_date}, å†™å…¥æ—¥æœŸ: {date_str}")

    try:
        # å°è¯•è·å–Product Huntæ•°æ®
        products = fetch_product_hunt_data(target_date)
    except Exception as e:
        print(f"è·å–Product Huntæ•°æ®å¤±è´¥: {e}")
        if _allow_mock_data():
            print("ALLOW_MOCK_DATA=trueï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç»§ç»­...")
            products = fetch_mock_data()
        else:
            raise

    # ç”ŸæˆMarkdownæ–‡ä»¶
    generate_markdown(products, date_str)

if __name__ == "__main__":
    main()
