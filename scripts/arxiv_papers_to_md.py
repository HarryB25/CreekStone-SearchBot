import os
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("dotenv æ¨¡å—æœªå®‰è£…ï¼Œå°†ç›´æ¥ä½¿ç”¨ç¯å¢ƒå˜é‡")

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import requests
from datetime import datetime, timezone, timedelta
import openai
import json
from typing import List, Dict
from common.storage import save_structured_items, build_item_id
from common.scoring import score_content

# AI è¿‡æ»¤å…³é”®è¯ï¼ˆç”¨äºåå¤„ç†ä¿æŠ¤ï¼Œä¸€èˆ¬å·²ç”±åˆ†ç±»è¿‡æ»¤ï¼‰
AI_KEYWORDS = [
    'ai', 'artificial intelligence', 'machine learning', 'ml',
    'deep learning', 'neural network', 'llm', 'gpt', 'claude',
    'agent', 'autonomous', 'chatbot', 'assistant', 'copilot',
    'generative', 'diffusion', 'transformer', 'embedding',
    'rag', 'retrieval', 'vector', 'semantic search',
    'proactive ai', 'agent infra', 'autonomous agents', 'multi-agent', 'openclaw', 'mcp',
    'context', 'hijacking', 'laude code sdk', 'cowork', 'vibe coding', 'agent-friendly tooling',
    'human-in-the-loop', 'online learning', 'reward model', 'reward fine-tune', 'sft', 'rlhf',
    'rlaif', 'dpo', 'hero user', 'product self-iteration', 'intent prediction', 'non-consensus ai',
    'ai-density', 'agentic workflow', 'workflow'
]

EXCLUDE_KEYWORDS = [
    'crypto', 'nft', 'blockchain', 'web3', 'token',
    'game', 'gaming', 'casino', 'betting',
    'adult', 'dating', 'porn'
]

def _allow_mock_data() -> bool:
    return os.getenv("ALLOW_MOCK_DATA", "").strip().lower() == "true"

def _get_int_env(name: str, default: int) -> int:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    try:
        value = int(raw)
    except ValueError:
        print(f"è­¦å‘Š: ç¯å¢ƒå˜é‡ {name}={raw!r} ä¸æ˜¯æ•´æ•°ï¼Œå›é€€é»˜è®¤å€¼ {default}")
        return default
    if value <= 0:
        print(f"è­¦å‘Š: ç¯å¢ƒå˜é‡ {name}={raw!r} å¿…é¡»å¤§äº 0ï¼Œå›é€€é»˜è®¤å€¼ {default}")
        return default
    return value


def _get_request_timeout() -> float:
    raw = os.getenv("OPENAI_REQUEST_TIMEOUT", "60").strip()
    try:
        value = float(raw)
        if value > 0:
            return value
    except Exception:
        pass
    return 60.0


def _contains_any(text: str, keywords) -> bool:
    if not text:
        return False
    lowered = text.lower()
    return any(k in lowered for k in keywords)


def is_ai_related(*fields: str) -> bool:
    merged = " ".join(filter(None, fields))
    if not merged:
        return False
    if _contains_any(merged, EXCLUDE_KEYWORDS):
        return False
    return _contains_any(merged, AI_KEYWORDS)

# åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
api_key = os.getenv('OPENAI_API_KEY')
base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
if not api_key:
    print("è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    client = None
else:
    try:
        client = openai.Client(api_key=api_key, base_url=base_url)
        print(f"æˆåŠŸåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (ä½¿ç”¨APIåœ°å€: {base_url})")
    except Exception as e:
        print(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å¤±è´¥: {e}")
        client = None

class ArxivPaper:
    def __init__(self, entry):
        """ä»arXiv APIè¿”å›çš„entryåˆå§‹åŒ–è®ºæ–‡å¯¹è±¡"""
        self.id = entry.get('id', '').split('/')[-1]
        self.title = entry.get('title', '').replace('\n', ' ').strip()
        self.authors = [author.get('name', '') for author in entry.get('author', [])]
        self.summary = entry.get('summary', '').replace('\n', ' ').strip()
        self.published = entry.get('published', '')
        self.url = f"https://arxiv.org/abs/{self.id}"
        self.pdf_url = f"https://arxiv.org/pdf/{self.id}.pdf"
        
        # æå–åˆ†ç±»
        categories = entry.get('category', [])
        if isinstance(categories, dict):
            categories = [categories]
        self.categories = [cat.get('@term', '') for cat in categories]
        
        # AIå¢å¼ºå­—æ®µ
        self.ai_summary = self.generate_ai_summary()
        self.keywords = self.generate_keywords()
        self.published_at = entry.get('published', '')
        self.score = score_content(
            f"æ ‡é¢˜: {self.title}\næ‘˜è¦: {self.summary}\nAIæ€»ç»“: {self.ai_summary}\nå…³é”®è¯: {self.keywords}",
            client,
            kind="arxiv",
        )

    def generate_keywords(self) -> str:
        """
        ä¸ºè®ºæ–‡ç”Ÿæˆå…³é”®è¯ï¼ˆä¸­æ–‡ä¸ºä¸»ï¼Œä¸“æœ‰åè¯å¯ä¿ç•™è‹±æ–‡ï¼‰ï¼Œä¸è¾“å‡ºå•ç‹¬çš„ AI/äººå·¥æ™ºèƒ½ã€‚
        è§„åˆ™ï¼šå»é™¤æ’é™¤è¯ï¼Œä¿æŒ 5-10 ä¸ªï¼Œè‹±æ–‡é€—å·åˆ†éš”ã€‚
        """
        try:
            base_text = f"æ ‡é¢˜: {self.title}\næ‘˜è¦: {self.summary}"
            if client is None:
                words = (self.title + ", " + self.summary).replace("&", ",").replace("|", ",").replace("-", ",").split(",")
                filtered = [w.strip() for w in words if w.strip()]
                filtered = [w for w in filtered if is_ai_related(w)]
                filtered = [w for w in filtered if w.lower() != 'ai' and w != 'äººå·¥æ™ºèƒ½']
                return ", ".join(dict.fromkeys(filtered))
            
            prompt = (
                "ä½ æ˜¯ä¸€åè®ºæ–‡ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚\n"
                "è¯·ç”Ÿæˆä»…é™ AI ç›¸å…³çš„ä¸­æ–‡å…³é”®è¯ï¼Œè‹±æ–‡é€—å·åˆ†éš”ï¼Œè¦æ±‚ï¼š\n"
                "- è‡³å°‘å« 1 ä¸ª AI_KEYWORDS åˆ—è¡¨ä¸­çš„è¯æˆ–åŒä¹‰è¯ï¼Œä½†ä¸è¦è¾“å‡ºå•ç‹¬çš„â€œAIâ€æˆ–â€œäººå·¥æ™ºèƒ½â€ã€‚\n"
                "- ä¸åŒ…å« EXCLUDE_KEYWORDS ä¸­ä»»ä¸€é¡¹ã€‚\n"
                "- åœ¨æ»¡è¶³è§„åˆ™åï¼Œå†è¡¥å…… 2-3 ä¸ªåŸºäºè®ºæ–‡æ–¹æ³•/åœºæ™¯/æŠ€æœ¯çš„çŸ­å…³é”®è¯ï¼Œä¸­æ–‡ä¸ºä¸»ï¼Œä¸“æœ‰åè¯å¯ä¿ç•™è‹±æ–‡ã€‚\n"
                "- æ€»æ•° 5-10 ä¸ªï¼Œå»é‡ã€å»ç©ºæ ¼ã€‚\n"
                f"AI_KEYWORDS: {', '.join(AI_KEYWORDS)}\n"
                f"EXCLUDE_KEYWORDS: {', '.join(EXCLUDE_KEYWORDS)}\n"
                f"{base_text}"
            )
            resp = client.chat.completions.create(
                model=os.getenv('ARXIV_MODEL_NAME', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "ç”¨ä¸­æ–‡è¾“å‡ºå…³é”®è¯ï¼Œæ»¡è¶³ç»™å®šçº¦æŸã€‚"},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=80,
                temperature=0.5,
                timeout=_get_request_timeout(),
            )
            keywords = resp.choices[0].message.content.strip()
            if ',' not in keywords:
                keywords = ', '.join(keywords.split())
            items = [k.strip() for k in keywords.split(',') if k.strip()]
            items = [k for k in items if k.lower() not in ('ai',) and k != 'äººå·¥æ™ºèƒ½']
            items = [k for k in items if not _contains_any(k, EXCLUDE_KEYWORDS)]
            if not any(_contains_any(k, AI_KEYWORDS) for k in items):
                fallback = next((kw for kw in AI_KEYWORDS if kw != 'ai' and kw in (self.title + ' ' + self.summary).lower()), 'agent')
                items.append(fallback)
            return ", ".join(dict.fromkeys(items))
        except Exception as e:
            print(f"å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def generate_ai_summary(self) -> Dict[str, str]:
        """ä½¿ç”¨AIç”Ÿæˆç»“æ„åŒ–æ‘˜è¦"""
        if client is None:
            return {
                "tldr": self.summary[:200] + "...",
                "motivation": "AIæœåŠ¡ä¸å¯ç”¨",
                "method": "AIæœåŠ¡ä¸å¯ç”¨",
                "conclusion": "AIæœåŠ¡ä¸å¯ç”¨"
            }

        prompt = f"""è¯·åˆ†æä»¥ä¸‹AIè®ºæ–‡å¹¶ç”¨ä¸­æ–‡æä¾›ç»“æ„åŒ–æ€»ç»“ï¼ˆè‹¥æ£€æµ‹åˆ°ä»»ä½• EXCLUDE_KEYWORDS åˆ™å›ç­” "è·³è¿‡"ï¼‰ï¼š

æ ‡é¢˜: {self.title}

æ‘˜è¦: {self.summary}

è¯·æä¾›ä»¥ä¸‹å››ä¸ªæ–¹é¢çš„åˆ†æï¼ˆæ¯ä¸ªæ–¹é¢ç”¨ç®€æ´çš„1-2å¥è¯æ¦‚æ‹¬ï¼‰ï¼š
1. TLDRï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰
2. ç ”ç©¶åŠ¨æœºï¼ˆMotivationï¼‰
3. æ ¸å¿ƒæ–¹æ³•ï¼ˆMethodï¼‰
4. ä¸»è¦ç»“è®ºï¼ˆConclusionï¼‰

è¯·ç”¨JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{"tldr": "...", "motivation": "...", "method": "...", "conclusion": "..."}}
"""
        
        try:
            print(f"æ­£åœ¨ä¸ºè®ºæ–‡ {self.title[:50]}... ç”ŸæˆAIæ‘˜è¦")
            response = client.chat.completions.create(
                model=os.getenv('ARXIV_MODEL_NAME', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“è®ºæ–‡è¦ç‚¹ã€‚"},
                    {"role": "user", "content": prompt + "\nAI_KEYWORDS: " + ", ".join(AI_KEYWORDS) + "\nEXCLUDE_KEYWORDS: " + ", ".join(EXCLUDE_KEYWORDS)}
                ],
                max_tokens=800,
                temperature=0.7,
                response_format={"type": "json_object"},
                timeout=_get_request_timeout(),
            )
            
            result = json.loads(response.choices[0].message.content)
            print(f"æˆåŠŸç”ŸæˆAIæ‘˜è¦")
            return result
            
        except Exception as e:
            print(f"AIæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "tldr": self.summary[:200] + "...",
                "motivation": "è‡ªåŠ¨åˆ†æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡",
                "method": "è‡ªåŠ¨åˆ†æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡",
                "conclusion": "è‡ªåŠ¨åˆ†æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡"
            }
    
    def to_markdown(self, rank: int) -> str:
        """è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        authors_str = ", ".join(self.authors[:3])
        if len(self.authors) > 3:
            authors_str += f" ç­‰ {len(self.authors)} ä½ä½œè€…"
        
        categories_str = ", ".join(self.categories)
        
        return f"""## [{rank}. {self.title}]({self.url})

**ä½œè€…**ï¼š{authors_str}  
**åˆ†ç±»**ï¼š{categories_str}  
**å‘å¸ƒæ—¶é—´**ï¼š{self.published[:10]}

### ğŸ“„ è®ºæ–‡æ‘˜è¦

{self.summary}

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼š{self.ai_summary.get('tldr', 'N/A')}

**ç ”ç©¶åŠ¨æœº**ï¼š{self.ai_summary.get('motivation', 'N/A')}

**æ ¸å¿ƒæ–¹æ³•**ï¼š{self.ai_summary.get('method', 'N/A')}

**ä¸»è¦ç»“è®º**ï¼š{self.ai_summary.get('conclusion', 'N/A')}

**å…³é”®è¯**ï¼š{self.keywords}

**è¯„åˆ†**ï¼š{self.score.get('total', 0)}

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡]({self.url}) | [ä¸‹è½½PDF]({self.pdf_url})

---

"""

    def to_content_item(self, rank: int, date_str: str) -> dict:
        merged = f"{self.title} {self.summary}".lower()
        hit_keywords = [kw for kw in AI_KEYWORDS if kw in merged]
        return {
            "id": build_item_id("ax", date_str, rank),
            "source": "arxiv",
            "date": date_str,
            "rank": rank,
            "title": self.title,
            "url": self.url,
            "detail_url": self.pdf_url,
            "description_en": self.summary,
            "description_zh": self.ai_summary.get('tldr', ''),
            "keywords": [k.strip() for k in self.keywords.split(",") if k.strip()],
            "tags": self.categories,
            "metrics": {"authors": self.authors},
            "media": {"image": None},
            "ai_flags": {"is_ai": True, "hit_keywords": hit_keywords, "hit_excludes": []},
            "score": self.score,
            "raw": {
                "published": self.published_at,
                "ai_summary": self.ai_summary,
            },
        }

def fetch_arxiv_papers(
    categories: List[str] = None,
    max_results: int = 10,
    target_date: str = "",
) -> List[ArxivPaper]:
    """ä»arXiv APIè·å–æœ€æ–°è®ºæ–‡"""
    if categories is None:
        categories = ['cs.AI', 'cs.LG', 'cs.CL', 'cs.CV']
    
    # æ„å»ºæŸ¥è¯¢
    category_query = ' OR '.join([f'cat:{cat}' for cat in categories])
    
    # arXiv API endpoint
    base_url = 'http://export.arxiv.org/api/query'
    
    # è·å–æœ€è¿‘7å¤©çª—å£ï¼Œå¯é€šè¿‡ target_date å›ºå®šçª—å£ç»ˆç‚¹
    if target_date:
        try:
            end_date = datetime.strptime(target_date, '%Y-%m-%d').replace(tzinfo=timezone.utc) + timedelta(days=1)
        except ValueError:
            print(f"è­¦å‘Š: ARXIV_TARGET_DATE={target_date!r} æ ¼å¼æ— æ•ˆï¼Œå›é€€åˆ°å½“å‰æ—¥æœŸçª—å£")
            end_date = datetime.now(timezone.utc)
    else:
        end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=7)
    
    # æ³¨æ„ï¼šarXivçš„submittedDateæŸ¥è¯¢æœ‰æ—¶ä¸ç¨³å®šï¼Œæ”¹ç”¨æ›´ç®€å•çš„æŸ¥è¯¢æ–¹å¼
    # ç›´æ¥æŒ‰åˆ†ç±»æŸ¥è¯¢æœ€æ–°çš„è®ºæ–‡ï¼Œç„¶åæŒ‰æ—¶é—´æ’åº
    # ä¸ºäº†ç¡®ä¿è·å–è¶³å¤Ÿçš„AIç›¸å…³è®ºæ–‡ï¼Œæˆ‘ä»¬è·å–æ›´å¤šç»“æœå†ç­›é€‰
    params = {
        'search_query': category_query,
        'start': 0,
        'max_results': max_results * 3,  # è·å–3å€æ•°é‡ç”¨äºç­›é€‰
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    print(f"æ­£åœ¨ä»arXivè·å–è®ºæ–‡...")
    print(f"æŸ¥è¯¢åˆ†ç±»: {', '.join(categories)}")
    print(f"ç›®æ ‡æ•°é‡: {max_results}")
    
    try:
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()
        
        # è§£æXMLå“åº”ï¼ˆarXiv APIè¿”å›Atom XMLæ ¼å¼ï¼‰
        import xmltodict
        data = xmltodict.parse(response.text)
        
        entries = data.get('feed', {}).get('entry', [])
        if isinstance(entries, dict):
            entries = [entries]
        
        if not entries:
            if _allow_mock_data():
                print("æœªæ‰¾åˆ°è®ºæ–‡ï¼ŒALLOW_MOCK_DATA=trueï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
                return fetch_mock_papers()
            raise RuntimeError("æœªæ‰¾åˆ°è®ºæ–‡ï¼Œä¸”æœªå¯ç”¨ ALLOW_MOCK_DATA")
        
        print(f"APIè¿”å› {len(entries)} ç¯‡è®ºæ–‡")
        
        # è¿‡æ»¤æœ€è¿‘7å¤©çš„è®ºæ–‡ï¼ˆä¸”ä¸è¶…è¿‡ç›®æ ‡æ—¥æœŸï¼‰
        recent_entries = []
        cutoff_date = start_date.strftime('%Y-%m-%d')
        upper_date = (end_date - timedelta(days=1)).strftime('%Y-%m-%d')
        for entry in entries:
            published = entry.get('published', '')
            published_day = published[:10]
            if cutoff_date <= published_day <= upper_date:
                recent_entries.append(entry)
        
        print(f"æœ€è¿‘7å¤©çš„è®ºæ–‡: {len(recent_entries)} ç¯‡")
        
        # å…³é”®ï¼šç­›é€‰ä¸»åˆ†ç±»åœ¨ç›®æ ‡åˆ†ç±»åˆ—è¡¨ä¸­çš„è®ºæ–‡ï¼Œå¹¶å†åšä¸€æ¬¡å…³é”®è¯è¿‡æ»¤
        target_categories_set = set(categories)
        filtered_papers = []

        for entry in recent_entries:
            try:
                # æå–è®ºæ–‡çš„æ‰€æœ‰åˆ†ç±»
                cats = entry.get('category', [])
                if not cats:
                    continue
                
                if isinstance(cats, dict):
                    cats = [cats]
                elif isinstance(cats, str):
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡è¿™ä¸ªentry
                    continue
                
                paper_categories = [cat.get('@term', '') if isinstance(cat, dict) else str(cat) for cat in cats]
                
                # æ£€æŸ¥ä¸»åˆ†ç±»ï¼ˆç¬¬ä¸€ä¸ªåˆ†ç±»ï¼‰æ˜¯å¦åœ¨ç›®æ ‡åˆ†ç±»åˆ—è¡¨ä¸­
                if paper_categories and paper_categories[0] in target_categories_set:
                    title = entry.get('title', '')
                    summary = entry.get('summary', '')
                    if not is_ai_related(title, summary):
                        print(f"è·³è¿‡éAIè®ºæ–‡: {title[:40]}...")
                        continue
                    filtered_papers.append(ArxivPaper(entry))
                    if len(filtered_papers) >= max_results:
                        break
            except Exception as e:
                print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                continue
        
        if not filtered_papers:
            if _allow_mock_data():
                print("ç­›é€‰åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„AIè®ºæ–‡ï¼ŒALLOW_MOCK_DATA=trueï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return fetch_mock_papers()
            raise RuntimeError("ç­›é€‰åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„AIè®ºæ–‡ï¼Œä¸”æœªå¯ç”¨ ALLOW_MOCK_DATA")
        
        print(f"âœ… ç­›é€‰å‡ºä¸»åˆ†ç±»ä¸ºAIç›¸å…³çš„è®ºæ–‡: {len(filtered_papers)} ç¯‡")
        return filtered_papers
        
    except Exception as e:
        print(f"è·å–arXivè®ºæ–‡å¤±è´¥: {e}")
        if _allow_mock_data():
            print("ALLOW_MOCK_DATA=trueï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
            return fetch_mock_papers()
        raise

def fetch_mock_papers() -> List[ArxivPaper]:
    """è¿”å›æ¨¡æ‹Ÿè®ºæ–‡æ•°æ®ç”¨äºæµ‹è¯•"""
    mock_entry = {
        'id': 'http://arxiv.org/abs/2401.00001v1',
        'title': 'Sample AI Paper: Deep Learning for Time Series Forecasting',
        'author': [
            {'name': 'Zhang San'},
            {'name': 'Li Si'},
            {'name': 'Wang Wu'}
        ],
        'summary': 'This paper presents a novel deep learning approach for time series forecasting. We propose a new architecture that combines attention mechanisms with temporal convolutional networks to capture both short-term and long-term dependencies in sequential data. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves state-of-the-art performance.',
        'published': datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ'),
        'category': [
            {'@term': 'cs.LG'},
            {'@term': 'cs.AI'}
        ]
    }
    return [ArxivPaper(mock_entry)]

def generate_markdown(papers: List[ArxivPaper], date_str: str):
    """ç”ŸæˆMarkdownæ–‡ä»¶å¹¶ä¿å­˜åˆ°data/arxivç›®å½•"""
    markdown_content = f"# arXiv AI è®ºæ–‡æ—¥æŠ¥ | {date_str}\n\n"
    markdown_content += f"> å…± {len(papers)} ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“\n\n"
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    papers_by_category = {}
    for paper in papers:
        main_category = paper.categories[0] if paper.categories else 'Other'
        if main_category not in papers_by_category:
            papers_by_category[main_category] = []
        papers_by_category[main_category].append(paper)
    
    # ç”Ÿæˆç›®å½•
    markdown_content += "## ğŸ“‘ ç›®å½•\n\n"
    for category, category_papers in papers_by_category.items():
        markdown_content += f"- [{category}](#{category.replace('.', '')}) ({len(category_papers)} ç¯‡)\n"
    markdown_content += "\n---\n\n"
    
    # æŒ‰åˆ†ç±»ç”Ÿæˆå†…å®¹
    rank = 1
    structured_items = []
    for category in sorted(papers_by_category.keys()):
        markdown_content += f"## {category}\n\n"
        for paper in papers_by_category[category]:
            markdown_content += paper.to_markdown(rank)
            structured_items.append(paper.to_content_item(rank, date_str))
            rank += 1
    
    # ç¡®ä¿data/arxivç›®å½•å­˜åœ¨
    os.makedirs('data/arxiv', exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶åˆ°data/arxivç›®å½•
    file_name = f"data/arxiv/arxiv-daily-{date_str}.md"
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸã€‚")
    save_structured_items(date_str, structured_items)

def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤æŠ“å–å‰ä¸€å¤©ï¼Œå¯é€šè¿‡ ARXIV_TARGET_DATE è¦†ç›–
    target_date = os.getenv("ARXIV_TARGET_DATE", "").strip()
    if target_date:
        fetch_date_str = target_date
    else:
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        fetch_date_str = yesterday.strftime('%Y-%m-%d')

    # è¾“å‡ºæ—¥æœŸï¼šå¯æ˜¾å¼è¦†ç›–ï¼›é»˜è®¤åœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹å†™å½“å¤©
    output_date = os.getenv("ARXIV_OUTPUT_DATE", "").strip()
    if output_date:
        date_str = output_date
    elif target_date:
        # æ‰‹åŠ¨æŒ‡å®šæŠ“å–æ—¥æœŸä½†æœªæŒ‡å®šè¾“å‡ºæ—¥æœŸæ—¶ï¼Œä¿æŒå†å²è¡Œä¸º
        date_str = target_date
    else:
        today = datetime.now(timezone.utc)
        date_str = today.strftime('%Y-%m-%d')
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    default_categories = 'cs.AI,cs.LG,cs.CL,cs.CV'
    categories_str = os.getenv('ARXIV_CATEGORIES', default_categories)
    if categories_str is None or categories_str.strip() == "":
        categories_str = default_categories
    categories = [cat.strip() for cat in categories_str.split(',') if cat.strip()]
    if not categories:
        categories = [cat.strip() for cat in default_categories.split(',')]
    max_results = _get_int_env('ARXIV_MAX_RESULTS', 30)
    
    print(f"=== arXiv AI è®ºæ–‡çˆ¬å–å¼€å§‹ ===")
    print(f"æŠ“å–æ—¥æœŸ: {fetch_date_str}")
    print(f"å†™å…¥æ—¥æœŸ: {date_str}")
    print(f"åˆ†ç±»: {', '.join(categories)}")
    print(f"æœ€å¤§æ•°é‡: {max_results}")
    
    # è·å–è®ºæ–‡
    papers = fetch_arxiv_papers(
        categories=categories,
        max_results=max_results,
        target_date=fetch_date_str
    )
    
    # ç”ŸæˆMarkdown
    generate_markdown(papers, date_str)
    
    print(f"=== arXiv AI è®ºæ–‡çˆ¬å–å®Œæˆ ===")

if __name__ == "__main__":
    main()
