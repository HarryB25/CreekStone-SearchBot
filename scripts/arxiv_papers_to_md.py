import os
import time
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("dotenv æ¨¡å—æœªå®‰è£…ï¼Œå°†ç›´æ¥ä½¿ç”¨ç¯å¢ƒå˜é‡")

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime, timezone, timedelta
import openai
import json
from typing import List, Dict
from common.storage import save_structured_items, build_item_id
from common.scoring import score_content
from common.openai_fallback import chat_completion_content, get_openai_timeout, is_gpt5_model
from common.keyword_utils import (
    investor_keyword_prompt,
    investor_keyword_repair_prompt,
    extract_keywords_from_response,
    finalize_keywords,
    keyword_quality_check,
    synthesize_keywords_from_context,
)

# AI è¿‡æ»¤å…³é”®è¯ï¼ˆç”¨äºåå¤„ç†ä¿æŠ¤ï¼Œä¸€èˆ¬å·²ç”±åˆ†ç±»è¿‡æ»¤ï¼‰
AI_KEYWORDS = [
    'ai', 'artificial intelligence', 'machine learning', 'ml',
    'deep learning', 'neural network', 'llm', 'gpt', 'claude',
    'agent', 'autonomous', 'chatbot', 'assistant', 'copilot',
    'generative', 'diffusion', 'transformer', 'embedding',
    'rag', 'retrieval', 'vector', 'semantic search',
    'proactive ai', 'agent infra', 'autonomous agents', 'multi-agent', 'openclaw', 'mcp',
    'context', 'hijacking', 'laude code sdk', 'cowork', 'vibe coding', 'agent-friendly tooling',
    'human-in-the-loop', 'online learning', 'reward model', 'reward fine-tune', 'sft', 'rlhf',
    'rlaif', 'dpo', 'hero user', 'product self-iteration', 'intent prediction', 'non-consensus ai',
    'ai-density', 'agentic workflow', 'workflow'
]

EXCLUDE_KEYWORDS = [
    'crypto', 'nft', 'blockchain', 'web3', 'token',
    'game', 'gaming', 'casino', 'betting',
    'adult', 'dating', 'porn'
]

def _allow_mock_data() -> bool:
    return os.getenv("ALLOW_MOCK_DATA", "").strip().lower() == "true"


def _build_retry_session() -> requests.Session:
    """æ„å»ºå¸¦é‡è¯•ç­–ç•¥çš„ä¼šè¯ï¼Œé™ä½ arXiv åˆ†é¡µè¿‡ç¨‹ä¸­çš„ä¸´æ—¶ç½‘ç»œé”™è¯¯å½±å“ã€‚"""
    retry = Retry(
        total=3,
        connect=3,
        read=3,
        backoff_factor=1.0,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session = requests.Session()
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

def _get_int_env(name: str, default: int) -> int:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    try:
        value = int(raw)
    except ValueError:
        print(f"è­¦å‘Š: ç¯å¢ƒå˜é‡ {name}={raw!r} ä¸æ˜¯æ•´æ•°ï¼Œå›é€€é»˜è®¤å€¼ {default}")
        return default
    if value <= 0:
        print(f"è­¦å‘Š: ç¯å¢ƒå˜é‡ {name}={raw!r} å¿…é¡»å¤§äº 0ï¼Œå›é€€é»˜è®¤å€¼ {default}")
        return default
    return value


def _get_request_timeout() -> float:
    return get_openai_timeout(60.0)


def _get_model_name() -> str:
    raw = os.getenv("OPENAI_MODEL", "").strip()
    return raw or "gpt-5.2-2025-12-11"


def _chat_json_content(messages, max_tokens: int, temperature: float) -> str:
    model_name = _get_model_name()
    content, used_model = chat_completion_content(
        client=client,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        json_mode=True,
        default_model=model_name,
        timeout=_get_request_timeout(),
        retry_max_tokens=max(max_tokens, 1200 if is_gpt5_model(model_name) else 300),
    )
    if used_model != model_name:
        print(f"æ¨¡å‹å›é€€: {model_name} -> {used_model}")
    return content


def _get_base_url(default: str = "https://api.openai.com/v1") -> str:
    raw = os.getenv("OPENAI_BASE_URL")
    if raw is None or raw.strip() == "":
        return default
    return raw.strip()


def _contains_any(text: str, keywords) -> bool:
    if not text:
        return False
    lowered = text.lower()
    return any(k in lowered for k in keywords)


def is_ai_related(*fields: str) -> bool:
    merged = " ".join(filter(None, fields))
    if not merged:
        return False
    if _contains_any(merged, EXCLUDE_KEYWORDS):
        return False
    return _contains_any(merged, AI_KEYWORDS)

# åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
api_key = os.getenv('OPENAI_API_KEY')
base_url = _get_base_url()
if not api_key:
    print("è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    client = None
else:
    try:
        client = openai.Client(api_key=api_key, base_url=base_url)
        print(f"æˆåŠŸåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (ä½¿ç”¨APIåœ°å€: {base_url})")
    except Exception as e:
        print(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å¤±è´¥: {e}")
        client = None

class ArxivPaper:
    def __init__(self, entry):
        """ä»arXiv APIè¿”å›çš„entryåˆå§‹åŒ–è®ºæ–‡å¯¹è±¡"""
        self.id = entry.get('id', '').split('/')[-1]
        self.title = entry.get('title', '').replace('\n', ' ').strip()
        authors_raw = entry.get('author', [])
        if isinstance(authors_raw, dict):
            authors_raw = [authors_raw]
        elif isinstance(authors_raw, str):
            authors_raw = [{"name": authors_raw}]
        self.authors = []
        for author in authors_raw:
            if isinstance(author, dict):
                name = str(author.get('name', '')).strip()
            else:
                name = str(author).strip()
            if name:
                self.authors.append(name)
        self.summary = entry.get('summary', '').replace('\n', ' ').strip()
        self.published = entry.get('published', '')
        self.url = f"https://arxiv.org/abs/{self.id}"
        self.pdf_url = f"https://arxiv.org/pdf/{self.id}.pdf"
        
        # æå–åˆ†ç±»
        categories = entry.get('category', [])
        if isinstance(categories, dict):
            categories = [categories]
        elif isinstance(categories, str):
            categories = [{"@term": categories}]
        self.categories = []
        for cat in categories:
            if isinstance(cat, dict):
                term = str(cat.get('@term', '')).strip()
            else:
                term = str(cat).strip()
            if term:
                self.categories.append(term)
        
        # AIå¢å¼ºå­—æ®µ
        self.ai_summary = self.generate_ai_summary()
        self.keywords = self.generate_keywords()
        self.published_at = entry.get('published', '')
        self.score = score_content(
            f"æ ‡é¢˜: {self.title}\næ‘˜è¦: {self.summary}\nAIæ€»ç»“: {self.ai_summary}\nå…³é”®è¯: {self.keywords}",
            client,
            kind="arxiv",
        )

    def generate_keywords(self) -> str:
        """ä¸ºè®ºæ–‡ç”Ÿæˆä¸­æ€§å…³é”®è¯ï¼šè´´åˆå†…å®¹ï¼Œæœ¯è¯­ç»Ÿä¸€ï¼Œå‡å°‘è‹±æ–‡å™ªéŸ³ã€‚"""
        try:
            base_text = f"æ ‡é¢˜: {self.title}\næ‘˜è¦: {self.summary}"
            if client is None:
                words = (self.title + ", " + self.summary).replace("&", ",").replace("|", ",").replace("-", ",").split(",")
                filtered = finalize_keywords([w.strip() for w in words if w.strip()], base_text, source="arxiv")
                ok, _ = keyword_quality_check(filtered, base_text, source="arxiv")
                if not ok:
                    filtered = synthesize_keywords_from_context(base_text, source="arxiv", max_items=10)
                return ", ".join(filtered)

            raw = _chat_json_content(
                messages=investor_keyword_prompt(
                    subject="Research Paper",
                    title=self.title,
                    subtitle=", ".join(self.categories[:3]),
                    description=self.summary,
                ),
                max_tokens=1200 if is_gpt5_model(_get_model_name()) else 220,
                temperature=0.3,
            )
            items = extract_keywords_from_response(raw)
            items = finalize_keywords(items, base_text, source="arxiv")
            ok, reason = keyword_quality_check(items, base_text, source="arxiv")
            if not ok:
                repaired_raw = _chat_json_content(
                    messages=investor_keyword_repair_prompt(
                        subject="Research Paper",
                        title=self.title,
                        subtitle=", ".join(self.categories[:3]),
                        description=self.summary,
                        bad_keywords=items,
                        reason=reason,
                    ),
                    max_tokens=1000 if is_gpt5_model(_get_model_name()) else 220,
                    temperature=0.2,
                )
                repaired = extract_keywords_from_response(repaired_raw)
                repaired = finalize_keywords(repaired, base_text, source="arxiv")
                repaired_ok, _ = keyword_quality_check(repaired, base_text, source="arxiv")
                if repaired_ok:
                    items = repaired
                else:
                    items = synthesize_keywords_from_context(base_text, source="arxiv", max_items=10)
            if not items:
                items = synthesize_keywords_from_context(base_text, source="arxiv", max_items=10)
            return ", ".join(dict.fromkeys(items))
        except Exception as e:
            print(f"å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}")
            fallback = finalize_keywords([self.title, self.summary], f"{self.title}\n{self.summary}", source="arxiv")
            ok, _ = keyword_quality_check(fallback, base_text, source="arxiv")
            if not ok:
                fallback = synthesize_keywords_from_context(base_text, source="arxiv", max_items=10)
            return ", ".join(fallback)
    
    def generate_ai_summary(self) -> Dict[str, str]:
        """ä½¿ç”¨AIç”Ÿæˆç»“æ„åŒ–æ‘˜è¦"""
        if client is None:
            return {
                "tldr": self.summary[:200] + "...",
                "motivation": "AIæœåŠ¡ä¸å¯ç”¨",
                "method": "AIæœåŠ¡ä¸å¯ç”¨",
                "conclusion": "AIæœåŠ¡ä¸å¯ç”¨"
            }

        prompt = f"""è¯·åˆ†æä»¥ä¸‹AIè®ºæ–‡å¹¶ç”¨ä¸­æ–‡æä¾›ç»“æ„åŒ–æ€»ç»“ï¼ˆè‹¥æ£€æµ‹åˆ°ä»»ä½• EXCLUDE_KEYWORDS åˆ™å›ç­” "è·³è¿‡"ï¼‰ï¼š

æ ‡é¢˜: {self.title}

æ‘˜è¦: {self.summary}

è¯·æä¾›ä»¥ä¸‹å››ä¸ªæ–¹é¢çš„åˆ†æï¼ˆæ¯ä¸ªæ–¹é¢ç”¨ç®€æ´çš„1-2å¥è¯æ¦‚æ‹¬ï¼‰ï¼š
1. TLDRï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰
2. ç ”ç©¶åŠ¨æœºï¼ˆMotivationï¼‰
3. æ ¸å¿ƒæ–¹æ³•ï¼ˆMethodï¼‰
4. ä¸»è¦ç»“è®ºï¼ˆConclusionï¼‰

è¯·ç”¨JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{"tldr": "...", "motivation": "...", "method": "...", "conclusion": "..."}}
"""
        
        try:
            print(f"æ­£åœ¨ä¸ºè®ºæ–‡ {self.title[:50]}... ç”ŸæˆAIæ‘˜è¦")
            content = _chat_json_content(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIç ”ç©¶è®ºæ–‡åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“è®ºæ–‡è¦ç‚¹ã€‚"},
                    {"role": "user", "content": prompt + "\nAI_KEYWORDS: " + ", ".join(AI_KEYWORDS) + "\nEXCLUDE_KEYWORDS: " + ", ".join(EXCLUDE_KEYWORDS)}
                ],
                max_tokens=1800 if is_gpt5_model(_get_model_name()) else 800,
                temperature=0.7,
            )

            result = json.loads(content)
            print(f"æˆåŠŸç”ŸæˆAIæ‘˜è¦")
            return result
            
        except Exception as e:
            print(f"AIæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "tldr": self.summary[:200] + "...",
                "motivation": "è‡ªåŠ¨åˆ†æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡",
                "method": "è‡ªåŠ¨åˆ†æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡",
                "conclusion": "è‡ªåŠ¨åˆ†æå¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡"
            }
    
    def to_markdown(self, rank: int) -> str:
        """è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        authors_str = ", ".join(self.authors[:3])
        if len(self.authors) > 3:
            authors_str += f" ç­‰ {len(self.authors)} ä½ä½œè€…"
        
        categories_str = ", ".join(self.categories)
        
        return f"""## [{rank}. {self.title}]({self.url})

**ä½œè€…**ï¼š{authors_str}  
**åˆ†ç±»**ï¼š{categories_str}  
**å‘å¸ƒæ—¶é—´**ï¼š{self.published[:10]}

### ğŸ“„ è®ºæ–‡æ‘˜è¦

{self.summary}

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼š{self.ai_summary.get('tldr', 'N/A')}

**ç ”ç©¶åŠ¨æœº**ï¼š{self.ai_summary.get('motivation', 'N/A')}

**æ ¸å¿ƒæ–¹æ³•**ï¼š{self.ai_summary.get('method', 'N/A')}

**ä¸»è¦ç»“è®º**ï¼š{self.ai_summary.get('conclusion', 'N/A')}

**å…³é”®è¯**ï¼š{self.keywords}

**è¯„åˆ†**ï¼š{self.score.get('total', 0)}

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡]({self.url}) | [ä¸‹è½½PDF]({self.pdf_url})

---

"""

    def to_content_item(self, rank: int, date_str: str) -> dict:
        merged = f"{self.title} {self.summary}".lower()
        hit_keywords = [kw for kw in AI_KEYWORDS if kw in merged]
        return {
            "id": build_item_id("ax", date_str, rank),
            "source": "arxiv",
            "date": date_str,
            "rank": rank,
            "title": self.title,
            "url": self.url,
            "detail_url": self.pdf_url,
            "description_en": self.summary,
            "description_zh": self.ai_summary.get('tldr', ''),
            "keywords": [k.strip() for k in self.keywords.split(",") if k.strip()],
            "tags": self.categories,
            "metrics": {"authors": self.authors},
            "media": {"image": None},
            "ai_flags": {"is_ai": True, "hit_keywords": hit_keywords, "hit_excludes": []},
            "score": self.score,
            "raw": {
                "published": self.published_at,
                "ai_summary": self.ai_summary,
            },
        }

def fetch_arxiv_papers(
    categories: List[str] = None,
    max_results: int = 10,
    target_date: str = "",
) -> List[ArxivPaper]:
    """ä»arXiv APIè·å–æœ€æ–°è®ºæ–‡"""
    if categories is None:
        categories = ['cs.AI', 'cs.LG', 'cs.CL', 'cs.CV']
    
    # æ„å»ºæŸ¥è¯¢
    category_query = ' OR '.join([f'cat:{cat}' for cat in categories])
    
    # arXiv API endpoint
    base_url = 'http://export.arxiv.org/api/query'
    
    # è·å–æœ€è¿‘7å¤©çª—å£ï¼Œå¯é€šè¿‡ target_date å›ºå®šçª—å£ç»ˆç‚¹ï¼›
    # å½“ç»™å®š target_date æ—¶ï¼Œä¼˜å…ˆç”¨ submittedDate ç²¾ç¡®è¿‡æ»¤å½“å¤©ï¼Œé¿å…æ·±åˆ†é¡µè§¦å‘é™æµã€‚
    strict_day_mode = False
    target_day_dt = None
    if target_date:
        try:
            target_day_dt = datetime.strptime(target_date, '%Y-%m-%d').replace(tzinfo=timezone.utc)
            end_date = target_day_dt + timedelta(days=1)
            strict_day_mode = True
        except ValueError:
            print(f"è­¦å‘Š: ARXIV_TARGET_DATE={target_date!r} æ ¼å¼æ— æ•ˆï¼Œå›é€€åˆ°å½“å‰æ—¥æœŸçª—å£")
            end_date = datetime.now(timezone.utc)
    else:
        end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=1 if strict_day_mode else 7)

    search_query = category_query
    if strict_day_mode and target_day_dt is not None:
        day_token = target_day_dt.strftime('%Y%m%d')
        search_query = (
            f"({category_query}) AND "
            f"submittedDate:[{day_token}0000 TO {day_token}2359]"
        )
    
    # å†å²å›å¡«æ—¶ä¸èƒ½åªæŠ“ç¬¬ä¸€é¡µï¼›éœ€è¦åˆ†é¡µå‘åç¿»ï¼Œç›´åˆ°è¦†ç›–ç›®æ ‡çª—å£ã€‚
    batch_size = max(100, max_results * 4)
    batch_size = min(batch_size, 200)
    max_pages = _get_int_env("ARXIV_MAX_PAGES", 30)
    if strict_day_mode:
        max_pages = min(max_pages, 8)
    page_sleep = float(os.getenv("ARXIV_PAGE_SLEEP", "0.35") or "0.35")
    
    print(f"æ­£åœ¨ä»arXivè·å–è®ºæ–‡...")
    print(f"æŸ¥è¯¢åˆ†ç±»: {', '.join(categories)}")
    print(f"ç›®æ ‡æ•°é‡: {max_results}")
    
    try:
        import xmltodict
        session = _build_retry_session()

        # è¿‡æ»¤æœ€è¿‘7å¤©çš„è®ºæ–‡ï¼ˆä¸”ä¸è¶…è¿‡ç›®æ ‡æ—¥æœŸï¼‰
        recent_entries = []
        cutoff_date = start_date.strftime('%Y-%m-%d')
        upper_date = (end_date - timedelta(days=1)).strftime('%Y-%m-%d')

        start_idx = 0
        page = 0
        while page < max_pages:
            params = {
                'search_query': search_query,
                'start': start_idx,
                'max_results': batch_size,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending',
            }
            response = None
            for attempt in range(1, 6):
                response = session.get(base_url, params=params, timeout=30)
                if response.status_code == 429:
                    wait_seconds = min(30.0, 1.5 * (2 ** (attempt - 1)))
                    print(
                        f"arXiv é™æµ(429)ï¼Œç¬¬ {attempt}/5 æ¬¡é‡è¯•ï¼Œ"
                        f"ç­‰å¾… {wait_seconds:.1f}s åç»§ç»­..."
                    )
                    time.sleep(wait_seconds)
                    continue
                response.raise_for_status()
                break
            if response is None:
                raise RuntimeError("arXiv è¯·æ±‚å¤±è´¥: æœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
            if response.status_code == 429:
                raise RuntimeError("arXiv è¯·æ±‚å¤±è´¥: è¿ç»­ 5 æ¬¡è§¦å‘é™æµ(429)")
            data = xmltodict.parse(response.text)
            entries = data.get('feed', {}).get('entry', [])
            if isinstance(entries, dict):
                entries = [entries]
            if not entries:
                break

            page += 1
            page_days = []
            for entry in entries:
                published_day = str(entry.get('published', ''))[:10]
                if published_day:
                    page_days.append(published_day)
                if cutoff_date <= published_day <= upper_date:
                    recent_entries.append(entry)

            if page_days:
                newest_day = max(page_days)
                oldest_day = min(page_days)
                print(
                    f"ç¬¬ {page} é¡µ: {len(entries)} ç¯‡, æ—¥æœŸåŒºé—´ {oldest_day} ~ {newest_day}, "
                    f"çª—å£å‘½ä¸­ç´¯è®¡ {len(recent_entries)}"
                )
                # è¯¥é¡µæœ€æ—§æ—¥æœŸå·²ç»æ—©äºçª—å£ä¸‹ç•Œï¼Œåç»­åªä¼šæ›´æ—§ï¼Œå¯ä»¥åœæ­¢ã€‚
                if oldest_day < cutoff_date:
                    break

            # å®‰å…¨é™åˆ¶ï¼šçª—å£å‘½ä¸­è¶³å¤Ÿå¤šæ—¶æå‰åœæ­¢ã€‚
            if len(recent_entries) >= max_results * 8:
                break

            start_idx += len(entries)
            if page_sleep > 0:
                time.sleep(page_sleep)

        print(f"æœ€è¿‘7å¤©çš„è®ºæ–‡: {len(recent_entries)} ç¯‡")

        if not recent_entries:
            if _allow_mock_data():
                print("æœªæ‰¾åˆ°çª—å£å†…è®ºæ–‡ï¼ŒALLOW_MOCK_DATA=trueï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
                return fetch_mock_papers()
            raise RuntimeError(
                f"æœªæ‰¾åˆ° {cutoff_date}~{upper_date} çš„è®ºæ–‡ï¼›"
                "è¯·å¢å¤§ ARXIV_MAX_PAGES æˆ–æ£€æŸ¥ç›®æ ‡æ—¥æœŸã€‚"
            )
        
        # å…³é”®ï¼šç­›é€‰ä¸»åˆ†ç±»åœ¨ç›®æ ‡åˆ†ç±»åˆ—è¡¨ä¸­çš„è®ºæ–‡ï¼Œå¹¶å†åšä¸€æ¬¡å…³é”®è¯è¿‡æ»¤
        target_categories_set = set(categories)
        filtered_papers = []

        for entry in recent_entries:
            try:
                # æå–è®ºæ–‡çš„æ‰€æœ‰åˆ†ç±»
                cats = entry.get('category', [])
                if not cats:
                    continue
                
                if isinstance(cats, dict):
                    cats = [cats]
                elif isinstance(cats, str):
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡è¿™ä¸ªentry
                    continue
                
                paper_categories = [cat.get('@term', '') if isinstance(cat, dict) else str(cat) for cat in cats]
                
                # æ£€æŸ¥ä¸»åˆ†ç±»ï¼ˆç¬¬ä¸€ä¸ªåˆ†ç±»ï¼‰æ˜¯å¦åœ¨ç›®æ ‡åˆ†ç±»åˆ—è¡¨ä¸­
                if paper_categories and paper_categories[0] in target_categories_set:
                    title = entry.get('title', '')
                    summary = entry.get('summary', '')
                    if not is_ai_related(title, summary):
                        print(f"è·³è¿‡éAIè®ºæ–‡: {title[:40]}...")
                        continue
                    filtered_papers.append(ArxivPaper(entry))
                    if len(filtered_papers) >= max_results:
                        break
            except Exception as e:
                print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                continue
        
        if not filtered_papers:
            if _allow_mock_data():
                print("ç­›é€‰åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„AIè®ºæ–‡ï¼ŒALLOW_MOCK_DATA=trueï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return fetch_mock_papers()
            raise RuntimeError("ç­›é€‰åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„AIè®ºæ–‡ï¼Œä¸”æœªå¯ç”¨ ALLOW_MOCK_DATA")
        
        print(f"âœ… ç­›é€‰å‡ºä¸»åˆ†ç±»ä¸ºAIç›¸å…³çš„è®ºæ–‡: {len(filtered_papers)} ç¯‡")
        return filtered_papers
        
    except Exception as e:
        print(f"è·å–arXivè®ºæ–‡å¤±è´¥: {e}")
        if _allow_mock_data():
            print("ALLOW_MOCK_DATA=trueï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
            return fetch_mock_papers()
        raise

def fetch_mock_papers() -> List[ArxivPaper]:
    """è¿”å›æ¨¡æ‹Ÿè®ºæ–‡æ•°æ®ç”¨äºæµ‹è¯•"""
    mock_entry = {
        'id': 'http://arxiv.org/abs/2401.00001v1',
        'title': 'Sample AI Paper: Deep Learning for Time Series Forecasting',
        'author': [
            {'name': 'Zhang San'},
            {'name': 'Li Si'},
            {'name': 'Wang Wu'}
        ],
        'summary': 'This paper presents a novel deep learning approach for time series forecasting. We propose a new architecture that combines attention mechanisms with temporal convolutional networks to capture both short-term and long-term dependencies in sequential data. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves state-of-the-art performance.',
        'published': datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ'),
        'category': [
            {'@term': 'cs.LG'},
            {'@term': 'cs.AI'}
        ]
    }
    return [ArxivPaper(mock_entry)]

def generate_markdown(papers: List[ArxivPaper], date_str: str):
    """ç”ŸæˆMarkdownæ–‡ä»¶å¹¶ä¿å­˜åˆ°data/arxivç›®å½•"""
    markdown_content = f"# arXiv AI è®ºæ–‡æ—¥æŠ¥ | {date_str}\n\n"
    markdown_content += f"> å…± {len(papers)} ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“\n\n"
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    papers_by_category = {}
    for paper in papers:
        main_category = paper.categories[0] if paper.categories else 'Other'
        if main_category not in papers_by_category:
            papers_by_category[main_category] = []
        papers_by_category[main_category].append(paper)
    
    # ç”Ÿæˆç›®å½•
    markdown_content += "## ğŸ“‘ ç›®å½•\n\n"
    for category, category_papers in papers_by_category.items():
        markdown_content += f"- [{category}](#{category.replace('.', '')}) ({len(category_papers)} ç¯‡)\n"
    markdown_content += "\n---\n\n"
    
    # æŒ‰åˆ†ç±»ç”Ÿæˆå†…å®¹
    rank = 1
    structured_items = []
    for category in sorted(papers_by_category.keys()):
        markdown_content += f"## {category}\n\n"
        for paper in papers_by_category[category]:
            markdown_content += paper.to_markdown(rank)
            structured_items.append(paper.to_content_item(rank, date_str))
            rank += 1
    
    # ç¡®ä¿data/arxivç›®å½•å­˜åœ¨
    os.makedirs('data/arxiv', exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶åˆ°data/arxivç›®å½•
    file_name = f"data/arxiv/arxiv-daily-{date_str}.md"
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸã€‚")
    save_structured_items(date_str, structured_items)

def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤æŠ“å–å‰ä¸€å¤©ï¼Œå¯é€šè¿‡ ARXIV_TARGET_DATE è¦†ç›–
    target_date = os.getenv("ARXIV_TARGET_DATE", "").strip()
    if target_date:
        fetch_date_str = target_date
    else:
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        fetch_date_str = yesterday.strftime('%Y-%m-%d')

    # è¾“å‡ºæ—¥æœŸï¼šå¯æ˜¾å¼è¦†ç›–ï¼›é»˜è®¤åœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹å†™å½“å¤©
    output_date = os.getenv("ARXIV_OUTPUT_DATE", "").strip()
    if output_date:
        date_str = output_date
    elif target_date:
        # æ‰‹åŠ¨æŒ‡å®šæŠ“å–æ—¥æœŸä½†æœªæŒ‡å®šè¾“å‡ºæ—¥æœŸæ—¶ï¼Œä¿æŒå†å²è¡Œä¸º
        date_str = target_date
    else:
        today = datetime.now(timezone.utc)
        date_str = today.strftime('%Y-%m-%d')
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    default_categories = 'cs.AI,cs.LG,cs.CL,cs.CV'
    categories_str = os.getenv('ARXIV_CATEGORIES', default_categories)
    if categories_str is None or categories_str.strip() == "":
        categories_str = default_categories
    categories = [cat.strip() for cat in categories_str.split(',') if cat.strip()]
    if not categories:
        categories = [cat.strip() for cat in default_categories.split(',')]
    max_results = _get_int_env('ARXIV_MAX_RESULTS', 30)
    
    print(f"=== arXiv AI è®ºæ–‡çˆ¬å–å¼€å§‹ ===")
    print(f"æŠ“å–æ—¥æœŸ: {fetch_date_str}")
    print(f"å†™å…¥æ—¥æœŸ: {date_str}")
    print(f"åˆ†ç±»: {', '.join(categories)}")
    print(f"æœ€å¤§æ•°é‡: {max_results}")
    
    # è·å–è®ºæ–‡
    papers = fetch_arxiv_papers(
        categories=categories,
        max_results=max_results,
        target_date=fetch_date_str
    )
    
    # ç”ŸæˆMarkdown
    generate_markdown(papers, date_str)
    
    print(f"=== arXiv AI è®ºæ–‡çˆ¬å–å®Œæˆ ===")

if __name__ == "__main__":
    main()
