# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-20

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.LG](#csLG) (9 ç¯‡)
- [cs.CL](#csCL) (2 ç¯‡)
- [cs.AI](#csAI) (2 ç¯‡)
- [cs.CV](#csCV) (2 ç¯‡)

---

## cs.AI

## [1. Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System](https://arxiv.org/abs/2602.18640v1)

**ä½œè€…**ï¼šLongfei Yun, Yihan Wu, Haoran Liu ç­‰ 12 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS (Generative Engine for Agentic Ranking Systems), a framework that reframes ranking optimization as an autonomous discovery process within a programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near-Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šGEARS å°†å¤§è§„æ¨¡æ’åºä¼˜åŒ–é‡æ„ä¸ºåœ¨å¯ç¼–ç¨‹å®éªŒç¯å¢ƒä¸­çš„â€œè‡ªæ²»å‘ç°â€è¿‡ç¨‹ï¼Œç”¨å…·å¤‡ä¸“å®¶æŠ€èƒ½çš„ä»£ç†åœ¨é«˜å±‚æ„å›¾å¼•å¯¼ä¸‹æœç´¢è¿‘ Pareto æœ€ä¼˜ä¸”å¯ç¨³å®šä¸Šçº¿çš„ç­–ç•¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¤§è§„æ¨¡æ’åºç³»ç»Ÿçš„ç“¶é¢ˆä¸å†ä¸»è¦æ˜¯å»ºæ¨¡æŠ€å·§ï¼Œè€Œæ˜¯å·¥ç¨‹ä¸Šä¸‹æ–‡çº¦æŸï¼šæŠŠæ¨¡ç³Šçš„äº§å“æ„å›¾è½¬åŒ–ä¸ºå¯æ‰§è¡Œã€å¯éªŒè¯çš„å‡è®¾ä¸å®éªŒæ–¹æ¡ˆéå¸¸å›°éš¾ã€‚éœ€è¦ä¸€ç§èƒ½ç»“åˆä¸šåŠ¡/çº¦æŸ/ä¸“å®¶ç»éªŒå¹¶ä¿è¯ç”Ÿäº§å¯é æ€§çš„è‡ªåŠ¨åŒ–ä¼˜åŒ–æ¡†æ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º GEARSï¼ˆGenerative Engine for Agentic Ranking Systemsï¼‰ï¼Œä»¥â€œSpecialized Agent Skillsâ€å°è£…æ’åºä¸“å®¶çŸ¥è¯†ä¸ºå¯å¤ç”¨æ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒæ“ä½œè€…ç”¨é«˜å±‚æ„å›¾ï¼ˆvibe personalizationï¼‰é©±åŠ¨ç­–ç•¥æœç´¢ã€‚æ¡†æ¶å†…ç½®éªŒè¯ hooksï¼Œå¼ºåŒ–ç»Ÿè®¡ç¨³å¥æ€§å¹¶è¿‡æ»¤å¯¹çŸ­æœŸä¿¡å·è¿‡æ‹Ÿåˆçš„è„†å¼±ç­–ç•¥ï¼Œç¡®ä¿éƒ¨ç½²ç¨³å®šã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§äº§å“åœºæ™¯å®éªŒä¸­ï¼ŒGEARS èƒ½æŒç»­æ‰¾åˆ°æ›´ä¼˜ã€æ¥è¿‘ Pareto å‰æ²¿çš„æ’åºç­–ç•¥ï¼Œå¹¶åœ¨èåˆç®—æ³•ä¿¡å·ä¸æ·±åº¦ä¸šåŠ¡ä¸Šä¸‹æ–‡çš„åŒæ—¶ä¿æŒä¸¥æ ¼çš„ä¸Šçº¿ç¨³å®šæ€§ä¸å¯é æ€§ã€‚

**å…³é”®è¯**ï¼šå¤§è§„æ¨¡æ’åºç³»ç»Ÿ, å¤šç›®æ ‡ä¼˜åŒ–, è‡ªåŠ¨åŒ–å®éªŒå¹³å°, ä¸“å®¶çŸ¥è¯†æ¨¡å—åŒ–, æ„å›¾é©±åŠ¨è°ƒå‚, ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ, çº¿ä¸ŠA/Bå®éªŒ, ç”Ÿäº§éƒ¨ç½²ç¨³å®šæ€§, ä¸ªæ€§åŒ–æ¨èåœºæ™¯

**è¯„åˆ†**ï¼š52

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18640v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18640v1.pdf)

---

## [2. Feedback-based Automated Verification in Vibe Coding of CAS Adaptation Built on Constraint Logic](https://arxiv.org/abs/2602.18607v1)

**ä½œè€…**ï¼šMichal TÃ¶pfer, FrantiÅ¡ek PlÃ¡Å¡il, TomÃ¡Å¡ BureÅ¡ ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In CAS adaptation, a challenge is to define the dynamic architecture of the system and changes in its behavior. Implementation-wise, this is projected into an adaptation mechanism, typically realized as an Adaptation Manager (AM). With the advances of generative LLMs, generating AM code based on system specification and desired AM behavior (partially in natural language) is a tempting opportunity. The recent introduction of vibe coding suggests a way to target the problem of the correctness of generated code by iterative testing and vibe coding feedback loops instead of direct code inspection.   In this paper, we show that generating an AM via vibe coding feedback loops is a viable option when the verification of the generated AM is based on a very precise formulation of the functional requirements. We specify these as constraints in a novel temporal logic FCL that allows us to express the behavior of traces with much finer granularity than classical LTL enables.   Furthermore, we show that by combining the adaptation and vibe coding feedback loops where the FCL constraints are evaluated for the current system state, we achieved good results in the experiments with generating AMs for two example systems from the CAS domain. Typically, just a few feedback loop iterations were necessary, each feeding the LLM with reports describing detailed violations of the constraints. This AM testing was combined with high run path coverage achieved by different initial settings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºç”¨â€œvibe codingâ€åé¦ˆå›è·¯ç»“åˆåŸºäºFCLï¼ˆç»†ç²’åº¦æ—¶åºçº¦æŸé€»è¾‘ï¼‰çš„è‡ªåŠ¨éªŒè¯ï¼Œè¿­ä»£ç”Ÿæˆå¹¶éªŒè¯CASç³»ç»Ÿçš„é€‚é…ç®¡ç†å™¨ï¼ˆAMï¼‰ä»£ç ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šCASé€‚é…ä¸­AMéœ€è¦ç²¾ç¡®å®šä¹‰åŠ¨æ€æ¶æ„ä¸è¡Œä¸ºå˜åŒ–ï¼Œè€ŒLLMç”ŸæˆAMä»£ç å­˜åœ¨æ­£ç¡®æ€§éš¾ä»¥ä¿è¯çš„é—®é¢˜ï¼›ä½œè€…å¸Œæœ›ç”¨å¯æ‰§è¡Œã€å¯åé¦ˆçš„å½¢å¼åŒ–éœ€æ±‚æ¥æ›¿ä»£äººå·¥å®¡æŸ¥ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†AMåŠŸèƒ½éœ€æ±‚ç”¨æ–°æ—¶åºé€»è¾‘FCLè¡¨è¾¾ä¸ºçº¦æŸï¼ˆæ¯”LTLèƒ½æ›´ç»†ç²’åº¦åˆ»ç”»è½¨è¿¹è¡Œä¸ºï¼‰ï¼Œåœ¨è¿è¡Œä¸­å¯¹å½“å‰ç³»ç»ŸçŠ¶æ€/æ‰§è¡Œè½¨è¿¹è¯„ä¼°çº¦æŸå¹¶ç”Ÿæˆè¿è§„æŠ¥å‘Šï¼Œä½œä¸ºvibe codingåé¦ˆè¾“å…¥LLMè¿­ä»£ä¿®æ­£ä»£ç ï¼›åŒæ—¶é€šè¿‡ä¸åŒåˆå§‹è®¾ç½®æå‡è¿è¡Œè·¯å¾„è¦†ç›–ç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜è¯¥æµç¨‹å¯è¡Œï¼šåœ¨ä¸¤ä¸ªCASç¤ºä¾‹ä¸­é€šå¸¸åªéœ€å°‘é‡åé¦ˆè¿­ä»£å³å¯ç”Ÿæˆæ»¡è¶³çº¦æŸçš„AMï¼Œä¸”è¯¦ç»†çš„çº¦æŸè¿è§„æŠ¥å‘Šèƒ½æœ‰æ•ˆå¼•å¯¼LLMä¿®å¤ï¼Œé…åˆé«˜è·¯å¾„è¦†ç›–å¸¦æ¥è¾ƒå¥½çš„éªŒè¯æ•ˆæœã€‚

**å…³é”®è¯**ï¼šå¤æ‚è‡ªé€‚åº”ç³»ç»Ÿï¼ˆCASï¼‰, è‡ªé€‚åº”ç®¡ç†å™¨ï¼ˆAMï¼‰, åé¦ˆå¾ªç¯æµ‹è¯•, LLMä»£ç ç”Ÿæˆ, è‡ªåŠ¨åŒ–éªŒè¯, çº¦æŸè§„èŒƒ, æ—¶åºé€»è¾‘FCL, è½¨è¿¹è¡Œä¸ºéªŒè¯, è·¯å¾„è¦†ç›–ç‡

**è¯„åˆ†**ï¼š42

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18607v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18607v1.pdf)

---

## cs.CL

## [3. PolyFrame at MWE-2026 AdMIRe 2: When Words Are Not Enough: Multimodal Idiom Disambiguation](https://arxiv.org/abs/2602.18652v1)

**ä½œè€…**ï¼šNina Hosseini-Kivanani  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Multimodal models struggle with idiomatic expressions due to their non-compositional meanings, a challenge amplified in multilingual settings. We introduced PolyFrame, our system for the MWE-2026 AdMIRe2 shared task on multimodal idiom disambiguation, featuring a unified pipeline for both image+text ranking (Subtask A) and text-only caption ranking (Subtask B). All model variants retain frozen CLIP-style vision--language encoders and the multilingual BGE M3 encoder, training only lightweight modules: a logistic regression and LLM-based sentence-type predictor, idiom synonym substitution, distractor-aware scoring, and Borda rank fusion. Starting from a CLIP baseline (26.7% Top-1 on English dev, 6.7% on English test), adding idiom-aware paraphrasing and explicit sentence-type classification increased performance to 60.0% Top-1 on English and 60.0% Top-1 (0.822 NDCG@5) in zero-shot transfer to Portuguese. On the multilingual blind test, our systems achieved average Top-1/NDCG scores of 0.35/0.73 for Subtask A and 0.32/0.71 for Subtask B across 15 languages. Ablation results highlight idiom-aware rewriting as the main contributor to performance, while sentence-type prediction and multimodal fusion enhance robustness. These findings suggest that effective idiom disambiguation is feasible without fine-tuning large multimodal encoders.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPolyFrameé€šè¿‡â€œä¹ è¯­æ”¹å†™+å¥å‹åˆ¤åˆ«+å¤šæ¨¡å‹æ’åºèåˆâ€çš„è½»é‡åŒ–æµæ°´çº¿ï¼Œåœ¨ä¸å¾®è°ƒå¤§å‹å¤šæ¨¡æ€ç¼–ç å™¨çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡å¤šè¯­è¨€å¤šæ¨¡æ€ä¹ è¯­æ¶ˆæ­§è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¤šæ¨¡æ€æ¨¡å‹å¯¹ä¹ è¯­çš„éç»„åˆè¯­ä¹‰ç†è§£ä¸è¶³ï¼Œä¸”è·¨è¯­è¨€æ—¶æ­§ä¹‰ä¸æ•°æ®ç¨€ç¼ºä½¿é—®é¢˜æ›´éš¾ï¼›å› æ­¤éœ€è¦ä¸€ç§å¯è¿ç§»ã€ä½æˆæœ¬ã€æ— éœ€é‡è®­å¤§æ¨¡å‹çš„æ–¹æ¡ˆæ¥æå‡ä¹ è¯­æ¶ˆæ­§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä¿æŒCLIPå¼è§†è§‰-è¯­è¨€ç¼–ç å™¨ä¸å¤šè¯­BGE M3åµŒå…¥æ¨¡å‹å†»ç»“ï¼Œä»…è®­ç»ƒ/ä½¿ç”¨è½»é‡æ¨¡å—ï¼šé€»è¾‘å›å½’æ‰“åˆ†ã€LLMå¥å­ç±»å‹é¢„æµ‹ã€ä¹ è¯­åŒä¹‰æ›¿æ¢/é‡Šä¹‰æ”¹å†™ã€è€ƒè™‘å¹²æ‰°é¡¹çš„æ‰“åˆ†ç­–ç•¥ï¼Œå¹¶ç”¨Bordaè¿›è¡Œæ’åèåˆä»¥ç»Ÿä¸€å¤„ç†å›¾æ–‡æ’åºä¸çº¯æ–‡æœ¬æ’åºä¸¤å­ä»»åŠ¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒä¸æ¶ˆèè¡¨æ˜ä¹ è¯­æ„ŸçŸ¥çš„æ”¹å†™æ˜¯æ€§èƒ½æå‡çš„ä¸»è¦æ¥æºï¼Œå¥å‹é¢„æµ‹ä¸å¤šæ¨¡æ€èåˆè¿›ä¸€æ­¥å¢å¼ºé²æ£’æ€§ï¼›æ•´ä½“ç»“æœè¯æ˜æ— éœ€å¾®è°ƒå¤§å‹å¤šæ¨¡æ€ç¼–ç å™¨ä¹Ÿèƒ½å®ç°æœ‰æ•ˆçš„å¤šè¯­è¨€ä¹ è¯­æ¶ˆæ­§ä¸é›¶æ ·æœ¬è¿ç§»ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€æˆè¯­æ¶ˆæ­§, å¤šè¯­è¨€è¿ç§», å›¾æ–‡æ£€ç´¢æ’åº, æ–‡æœ¬æè¿°æ’åº, å†»ç»“è§†è§‰è¯­è¨€ç¼–ç å™¨, CLIPè§†è§‰è¯­è¨€æ¨¡å‹, BGE-M3å¤šè¯­è¨€ç¼–ç å™¨, å‚æ•°é«˜æ•ˆå¾®è°ƒ, å¥å­ç±»å‹åˆ†ç±», æˆè¯­é‡Šä¹‰æ”¹å†™, å¹²æ‰°é¡¹æ„ŸçŸ¥è¯„åˆ†

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18652v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18652v1.pdf)

---

## [4. DP-RFT: Learning to Generate Synthetic Text via Differentially Private Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.18633v1)

**ä½œè€…**ï¼šFangyuan Xu, Sihao Chen, Zinan Lin ç­‰ 16 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Differentially private (DP) synthetic data generation plays a pivotal role in developing large language models (LLMs) on private data, where data owners cannot provide eyes-on access to individual examples. Generating DP synthetic data typically involves a difficult trade-off. On one hand, DP finetuning methods train an LLM as a synthetic data generator with formal privacy guarantees, yet it still requires the raw content of private examples for model training. However, methods that avoid direct exposure to private data are bounded by an off-the-shelf, un-finetuned model, whose outputs often lack domain fidelity. Can we train an LLM to generate high-quality synthetic text without eyes-on access to individual private examples? In this work, we introduce Differentially Private Reinforcement Fine-Tuning (DP-RFT), an online reinforcement learning algorithm for synthetic data generation with LLMs. DP-RFT leverages DP-protected nearest-neighbor votes from an eyes-off private corpus as a reward signal for on-policy synthetic samples generated by an LLM. The LLM iteratively learns to generate synthetic data to maximize the expected DP votes through Proximal Policy Optimization (PPO). We evaluate DP-RFT for long-form and domain-specific synthetic data generation, such as news articles, meeting transcripts, and medical article abstracts. Our experiments show that DP-RFT closes the gap between private evolution and DP finetuning methods in terms of the fidelity and downstream utility of the generated synthetic data, while respecting the private data boundary.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šDP-RFTé€šè¿‡å·®åˆ†éšç§ä¿æŠ¤çš„è¿‘é‚»æŠ•ç¥¨ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒLLMï¼Œåœ¨ä¸â€œçœ¼è§â€ç§æœ‰æ ·æœ¬å†…å®¹çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜ä¿çœŸåˆæˆæ–‡æœ¬ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰DPå¾®è°ƒè™½æœ‰æ­£å¼éšç§ä¿è¯ä½†è®­ç»ƒä»éœ€ç›´æ¥æ¥è§¦ç§æœ‰æ–‡æœ¬ï¼Œè€Œä¸æ¥è§¦ç§æœ‰æ•°æ®çš„ç°æˆæ¨¡å‹åˆéš¾ä»¥ç”Ÿæˆè¶³å¤Ÿè´´åˆé¢†åŸŸçš„é«˜è´¨é‡æ–‡æœ¬ï¼›å› æ­¤éœ€è¦ä¸€ç§æ—¢ä¸æš´éœ²å•æ¡ç§æœ‰æ ·æœ¬åˆèƒ½æå‡é¢†åŸŸä¿çœŸçš„è®­ç»ƒæ–¹å¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºDP-RFTï¼šæ¨¡å‹æŒ‰ç­–ç•¥ç”Ÿæˆåˆæˆæ ·æœ¬åï¼Œåœ¨ç§æœ‰è¯­æ–™ä¸Šç”¨å·®åˆ†éšç§æœºåˆ¶åšæœ€è¿‘é‚»æŠ•ç¥¨ï¼ˆDP-protected nearest-neighbor votesï¼‰å¾—åˆ°å¥–åŠ±ï¼Œå¹¶ç”¨PPOè¿›è¡Œåœ¨çº¿åœ¨ç­–ç•¥ï¼ˆon-policyï¼‰å¼ºåŒ–å­¦ä¹ ï¼Œè¿­ä»£æœ€å¤§åŒ–æœŸæœ›DPæŠ•ç¥¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ–°é—»ã€ä¼šè®®è®°å½•ã€åŒ»å­¦æ‘˜è¦ç­‰é•¿æ–‡æœ¬ä¸é¢†åŸŸæ•°æ®ä¸Šï¼ŒDP-RFTåœ¨éµå®ˆç§æœ‰æ•°æ®è¾¹ç•Œçš„åŒæ—¶æ˜¾è‘—æå‡åˆæˆæ•°æ®çš„ä¿çœŸåº¦ä¸ä¸‹æ¸¸ä»»åŠ¡æ•ˆç”¨ï¼Œç¼©å°äº†â€œç§æœ‰æ¼”åŒ–/éDPæ–¹æ³•â€å’ŒDPå¾®è°ƒä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚

**å…³é”®è¯**ï¼šå·®åˆ†éšç§, éšç§ä¿æŠ¤åˆæˆæ–‡æœ¬, åˆæˆæ•°æ®ç”Ÿæˆ, å¼ºåŒ–å­¦ä¹ å¾®è°ƒ, åŸºäºå¥–åŠ±çš„æ–‡æœ¬ç”Ÿæˆ, è¿‘é‚»æŠ•ç¥¨å¥–åŠ±, æ— çœ¼è®¿é—®ç§æœ‰è¯­æ–™, é¢†åŸŸç‰¹å®šé•¿æ–‡æœ¬ç”Ÿæˆ

**è¯„åˆ†**ï¼š41

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18633v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18633v1.pdf)

---

## cs.CV

## [5. Narrating For You: Prompt-guided Audio-visual Narrating Face Generation Employing Multi-entangled Latent Space](https://arxiv.org/abs/2602.18618v1)

**ä½œè€…**ï¼šAashish Chandra, Aashutosh A, Abhijit Das  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present a novel approach for generating realistic speaking and talking faces by synthesizing a person's voice and facial movements from a static image, a voice profile, and a target text. The model encodes the prompt/driving text, the driving image, and the voice profile of an individual and then combines them to pass them to the multi-entangled latent space to foster key-value pairs and queries for the audio and video modality generation pipeline. The multi-entangled latent space is responsible for establishing the spatiotemporal person-specific features between the modalities. Further, entangled features are passed to the respective decoder of each modality for output audio and video generation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§åœ¨æç¤ºæ–‡æœ¬å¼•å¯¼ä¸‹ï¼Œä»é™æ€äººè„¸å›¾åƒã€ä¸ªä½“å£°éŸ³æ¡£æ¡ˆä¸ç›®æ ‡æ–‡æœ¬è”åˆç”ŸæˆåŒæ­¥çš„è¯´è¯éŸ³é¢‘ä¸å£å‹/è¡¨æƒ…è§†é¢‘çš„æ¡†æ¶ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è¯´è¯äººè„¸ç”Ÿæˆå¾€å¾€éš¾ä»¥åŒæ—¶ä¿æŒâ€œäººç‰©èº«ä»½ä¸€è‡´æ€§â€å’Œâ€œéŸ³ç”»æ—¶åºå¯¹é½â€ï¼Œä¸”éš¾ä»¥å°†æ–‡æœ¬è¯­ä¹‰ã€ä¸ªä½“å£°çº¹ä¸é¢éƒ¨è¿åŠ¨æœ‰æ•ˆè€¦åˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåˆ†åˆ«ç¼–ç é©±åŠ¨æ–‡æœ¬ã€é©±åŠ¨å›¾åƒä¸ç›®æ ‡äººç‰©çš„å£°éŸ³æ¡£æ¡ˆï¼Œå¹¶åœ¨â€œå¤šé‡çº ç¼ æ½œç©ºé—´â€ä¸­èåˆä¸ºè·¨æ¨¡æ€çš„queryä¸key-valueè¡¨å¾ï¼Œç”¨äºå»ºç«‹éŸ³é¢‘ä¸è§†é¢‘ä¹‹é—´çš„äººç‰©ç‰¹å®šæ—¶ç©ºå…³è”ï¼›éšåå°†çº ç¼ ç‰¹å¾é€å…¥å„è‡ªæ¨¡æ€è§£ç å™¨ç”ŸæˆéŸ³é¢‘ä¸è§†é¢‘è¾“å‡ºã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•é€šè¿‡å¤šé‡çº ç¼ æ½œç©ºé—´æ˜¾å¼å»ºæ¨¡è·¨æ¨¡æ€çš„äººç‰©ç‰¹å¾ä¸æ—¶åºå…³ç³»ï¼Œä»è€Œæå‡äº†ç”Ÿæˆç»“æœçš„çœŸå®æ„Ÿã€èº«ä»½ä¸€è‡´æ€§ä»¥åŠéŸ³ç”»åŒæ­¥æ€§ã€‚

**å…³é”®è¯**ï¼šéŸ³è§†è”åˆç”Ÿæˆ, è¯´è¯äººè„¸ç”Ÿæˆ, é™æ€å›¾åƒé©±åŠ¨, è¯­éŸ³æ¡ä»¶ç”Ÿæˆ, æ–‡æœ¬æç¤ºå¼•å¯¼, è·¨æ¨¡æ€å¯¹é½, å¤šçº ç¼ æ½œç©ºé—´, æ—¶ç©ºä¸€è‡´æ€§å»ºæ¨¡, èº«ä»½ä¿æŒç”Ÿæˆ, æ³¨æ„åŠ›é”®å€¼æŸ¥è¯¢

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18618v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18618v1.pdf)

---

## [6. Effect of Patch Size on Fine-Tuning Vision Transformers in Two-Dimensional and Three-Dimensional Medical Image Classification](https://arxiv.org/abs/2602.18614v1)

**ä½œè€…**ï¼šMassoud Dehghan, Ramona Woitek, Amirreza Mahbod  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Vision Transformers (ViTs) and their variants have become state-of-the-art in many computer vision tasks and are widely used as backbones in large-scale vision and vision-language foundation models. While substantial research has focused on architectural improvements, the impact of patch size, a crucial initial design choice in ViTs, remains underexplored, particularly in medical domains where both two-dimensional (2D) and three-dimensional (3D) imaging modalities exist.   In this study, using 12 medical imaging datasets from various imaging modalities (including seven 2D and five 3D datasets), we conduct a thorough evaluation of how different patch sizes affect ViT classification performance. Using a single graphical processing unit (GPU) and a range of patch sizes (1, 2, 4, 7, 14, 28), we fine-tune ViT models and observe consistent improvements in classification performance with smaller patch sizes (1, 2, and 4), which achieve the best results across nearly all datasets. More specifically, our results indicate improvements in balanced accuracy of up to 12.78% for 2D datasets (patch size 2 vs. 28) and up to 23.78% for 3D datasets (patch size 1 vs. 14), at the cost of increased computational expense. Moreover, by applying a straightforward ensemble strategy that fuses the predictions of the models trained with patch sizes 1, 2, and 4, we demonstrate a further boost in performance in most cases, especially for the 2D datasets. Our implementation is publicly available on GitHub: https://github.com/HealMaDe/MedViT

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šåœ¨12ä¸ª2D/3DåŒ»å­¦å½±åƒåˆ†ç±»æ•°æ®é›†ä¸Šç³»ç»Ÿè¯„ä¼°ViTçš„patch sizeï¼Œå‘ç°æ›´å°patchï¼ˆ1/2/4ï¼‰å‡ ä¹æ™®éå¸¦æ¥æ›´é«˜å‡†ç¡®ç‡ä½†è®¡ç®—æ›´è´µï¼Œä¸”å°patchæ¨¡å‹é›†æˆå¯è¿›ä¸€æ­¥æå‡è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šViTçš„patch sizeæ˜¯å…³é”®ä½†å¸¸è¢«å¿½è§†çš„è®¾è®¡é€‰æ‹©ï¼Œå°¤å…¶åœ¨åŒæ—¶å­˜åœ¨2Dä¸3Dæ¨¡æ€çš„åŒ»å­¦å½±åƒä¸­å…¶å½±å“ç¼ºä¹ç³»ç»Ÿç ”ç©¶ï¼›å› æ­¤éœ€è¦æ˜ç¡®ä¸åŒpatch sizeåœ¨åŒ»å­¦åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ•ˆæœä¸ä»£ä»·æƒè¡¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨å•GPUæ¡ä»¶ä¸‹ï¼Œå¯¹12ä¸ªåŒ»å­¦æ•°æ®é›†ï¼ˆ7ä¸ª2Dã€5ä¸ª3Dï¼‰ç”¨å¤šç§patch sizeï¼ˆ1,2,4,7,14,28ï¼‰å¾®è°ƒViTå¹¶æ¯”è¾ƒåˆ†ç±»æ€§èƒ½ï¼›åŒæ—¶å°†patch sizeä¸º1/2/4çš„æ¨¡å‹é¢„æµ‹è¿›è¡Œç®€å•èåˆåšé›†æˆè¯„ä¼°ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ›´å°çš„patch sizeï¼ˆ1/2/4ï¼‰åœ¨å‡ ä¹æ‰€æœ‰æ•°æ®é›†ä¸Šå–å¾—æœ€ä½³æˆ–æ¥è¿‘æœ€ä½³çš„balanced accuracyï¼Œç›¸æ¯”å¤§patchå¯å¸¦æ¥æœ€é«˜çº¦12.78%(2D)ä¸23.78%(3D)çš„æå‡ä½†å¢åŠ è®¡ç®—å¼€é”€ï¼›å°†å°patchæ¨¡å‹è¿›è¡Œé›†æˆé€šå¸¸è¿˜èƒ½è¿›ä¸€æ­¥æå‡æ•ˆæœï¼Œå°¤å…¶åœ¨2Dæ•°æ®é›†ä¸Šæ›´æ˜æ˜¾ã€‚

**å…³é”®è¯**ï¼šäºŒç»´åŒ»å­¦å½±åƒåˆ†ç±», ä¸‰ç»´åŒ»å­¦å½±åƒåˆ†ç±», å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®é›†, è®¡ç®—å¼€é”€ä¸æ€§èƒ½æƒè¡¡, å¤šæ¨¡å‹é›†æˆï¼ˆensembleï¼‰, å•GPUè®­ç»ƒè¯„ä¼°, å¼€æºä»£ç å®ç°, Effect

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18614v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18614v1.pdf)

---

## cs.LG

## [7. Large Causal Models for Temporal Causal Discovery](https://arxiv.org/abs/2602.18662v1)

**ä½œè€…**ï¼šNikolaos Kougioulis, Nikolaos Gkorgkolis, MingXue Wang ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Causal discovery for both cross-sectional and temporal data has traditionally followed a dataset-specific paradigm, where a new model is fitted for each individual dataset. Such an approach limits the potential of multi-dataset pretraining. The concept of large causal models (LCMs) envisions a class of pre-trained neural architectures specifically designed for temporal causal discovery. Prior approaches are constrained to small variable counts, degrade with larger inputs, and rely heavily on synthetic data, limiting generalization. We propose a principled framework for LCMs, combining diverse synthetic generators with realistic time-series datasets, allowing learning at scale. Extensive experiments on synthetic, semi-synthetic and realistic benchmarks show that LCMs scale effectively to higher variable counts and deeper architectures while maintaining strong performance. Trained models achieve competitive or superior accuracy compared to classical and neural baselines, particularly in out-of-distribution settings, while enabling fast, single-pass inference. Results demonstrate LCMs as a promising foundation-model paradigm for temporal causal discovery. Experiments and model weights are available at https://github.com/kougioulis/LCM-paper/.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºâ€œLarge Causal Models (LCMs)â€è¿™ä¸€é¢„è®­ç»ƒèŒƒå¼ï¼Œç”¨å•æ¬¡å‰å‘æ¨ç†å®ç°å¯æ‰©å±•çš„æ—¶é—´å› æœå‘ç°ï¼Œå¹¶åœ¨å¤šç±»åŸºå‡†ä¸Šå–å¾—å¼ºæ³›åŒ–è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿæ—¶é—´/æ¨ªæˆªé¢å› æœå‘ç°é€šå¸¸å¯¹æ¯ä¸ªæ•°æ®é›†å•ç‹¬è®­ç»ƒï¼Œéš¾ä»¥åˆ©ç”¨å¤šæ•°æ®é›†é¢„è®­ç»ƒå¸¦æ¥çš„è¿ç§»èƒ½åŠ›ï¼›ç°æœ‰ç¥ç»æ–¹æ³•è¿˜å¸¸å—å˜é‡æ•°é™åˆ¶ã€è§„æ¨¡å˜å¤§æ€§èƒ½ä¸‹é™ä¸”è¿‡åº¦ä¾èµ–åˆæˆæ•°æ®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºé¢å‘æ—¶é—´å› æœå‘ç°çš„LCMæ¡†æ¶ï¼šç»“åˆå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ç”Ÿæˆå™¨ä¸æ›´è´´è¿‘çœŸå®çš„æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼Œå¹¶é€šè¿‡æ›´æ·±/æ›´å¤§çš„ç¥ç»æ¶æ„å®ç°å¯¹é«˜å˜é‡ç»´åº¦è¾“å…¥çš„æœ‰æ•ˆå»ºæ¨¡ä¸å¿«é€Ÿå•æ¬¡æ¨ç†ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒæ˜¾ç¤ºLCMsåœ¨åˆæˆã€åŠåˆæˆä¸çœŸå®æ•°æ®é›†ä¸Šèƒ½éšå˜é‡æ•°ä¸æ¨¡å‹æ·±åº¦æ‰©å±•è€Œä¿æŒå¼ºæ€§èƒ½ï¼Œæ•´ä½“å‡†ç¡®ç‡ä¸ç»å…¸/ç¥ç»åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå°¤å…¶åœ¨åˆ†å¸ƒå¤–åœºæ™¯æ³›åŒ–æ›´å¥½ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºæ—¶é—´å› æœå‘ç°â€œåŸºç¡€æ¨¡å‹â€èŒƒå¼çš„æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šæ—¶åºå› æœå›¾å­¦ä¹ , å¤§è§„æ¨¡å› æœæ¨¡å‹ï¼ˆLCMï¼‰, å¤šæ•°æ®é›†é¢„è®­ç»ƒ, é«˜ç»´å˜é‡å› æœæ¨æ–­, åˆæˆæ•°æ®ç”Ÿæˆå™¨, åŠåˆæˆåŸºå‡†, åˆ†å¸ƒå¤–æ³›åŒ–ï¼ˆOODï¼‰, å•æ¬¡å‰å‘æ¨ç†, å¯æ‰©å±•ç¥ç»æ¶æ„

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18662v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18662v1.pdf)

---

## [8. Global Low-Rank, Local Full-Rank: The Holographic Encoding of Learned Algorithms](https://arxiv.org/abs/2602.18649v1)

**ä½œè€…**ï¼šYongzhong Xu  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Grokking -- the abrupt transition from memorization to generalization after extended training -- has been linked to the emergence of low-dimensional structure in learning dynamics. Yet neural network parameters inhabit extremely high-dimensional spaces. How can a low-dimensional learning process produce solutions that resist low-dimensional compression?   We investigate this question in multi-task modular arithmetic, training shared-trunk Transformers with separate heads for addition, multiplication, and a quadratic operation modulo 97. Across three model scales (315K--2.2M parameters) and five weight decay settings, we compare three reconstruction methods: per-matrix SVD, joint cross-matrix SVD, and trajectory PCA.   Across all conditions, grokking trajectories are confined to a 2--6 dimensional global subspace, while individual weight matrices remain effectively full-rank. Reconstruction from 3--5 trajectory PCs recovers over 95\% of final accuracy, whereas both per-matrix and joint SVD fail at sub-full rank. Even when static decompositions capture most spectral energy, they destroy task-relevant structure.   These results show that learned algorithms are encoded through dynamically coordinated updates spanning all matrices, rather than localized low-rank components. We term this the holographic encoding principle: grokked solutions are globally low-rank in the space of learning directions but locally full-rank in parameter space, with implications for compression, interpretability, and understanding how neural networks encode computation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æ¢è®¨äº†åœ¨å¤šä»»åŠ¡æ¨¡å—ç®—æœ¯ä¸­ä½ç»´å­¦ä¹ è¿‡ç¨‹å¦‚ä½•äº§ç”ŸæŠµæŠ—ä½ç»´å‹ç¼©çš„è§£å†³æ–¹æ¡ˆï¼Œæå‡ºäº†å…¨å±€ä½ç§©ä¸å±€éƒ¨å…¨ç§©çš„å…¨æ¯ç¼–ç åŸç†ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç ”ç©¶åŠ¨æœºåœ¨äºç†è§£åœ¨é«˜ç»´å‚æ•°ç©ºé—´ä¸­ï¼Œä½ç»´å­¦ä¹ åŠ¨æ€å¦‚ä½•å®ç°ä»è®°å¿†åˆ°æ³›åŒ–çš„è½¬å˜ï¼Œå³grokkingç°è±¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡è®­ç»ƒå…±äº«ä¸»å¹²çš„Transformeræ¨¡å‹ï¼Œæ¯”è¾ƒä¸åŒé‡å»ºæ–¹æ³•ï¼ˆå¦‚SVDå’ŒPCAï¼‰åœ¨å¤šä»»åŠ¡ç®—æœ¯ä¸­çš„è¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»“æœè¡¨æ˜ï¼Œå­¦ä¹ çš„ç®—æ³•é€šè¿‡åŠ¨æ€åè°ƒçš„æ›´æ–°æ¥ç¼–ç ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå±€éƒ¨ä½ç§©ç»„ä»¶ï¼Œæå‡ºçš„å…¨æ¯ç¼–ç åŸç†å¯¹å‹ç¼©ã€å¯è§£é‡Šæ€§åŠç¥ç»ç½‘ç»œè®¡ç®—çš„ç†è§£å…·æœ‰é‡è¦æ„ä¹‰ã€‚

**å…³é”®è¯**ï¼šè®°å¿†-æ³›åŒ–è½¬å˜, ä½ç»´å­¦ä¹ åŠ¨åŠ›å­¦, å‚æ•°æ›´æ–°å­ç©ºé—´, å…¨å±€ä½ç§©, å±€éƒ¨æ»¡ç§©, å˜æ¢å™¨å¤šä»»åŠ¡å­¦ä¹ , æ¨¡å—ç®—æœ¯ä»»åŠ¡, å…¨æ¯ç¼–ç åŸç†

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18649v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18649v1.pdf)

---

## [9. Information-Guided Noise Allocation for Efficient Diffusion Training](https://arxiv.org/abs/2602.18647v1)

**ä½œè€…**ï¼šGabriel Raya, Bac Nguyen, Georgios Batzolis ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CV, cs.IT  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Training diffusion models typically relies on manually tuned noise schedules, which can waste computation on weakly informative noise regions and limit transfer across datasets, resolutions, and representations. We revisit noise schedule allocation through an information-theoretic lens and propose the conditional entropy rate of the forward process as a theoretically grounded, data-dependent diagnostic for identifying suboptimal noise-level allocation in existing schedules. Based on these insight, we introduce InfoNoise, a principled data-adaptive training noise schedule that replaces heuristic schedule design with an information-guided noise sampling distribution derived from entropy-reduction rates estimated from denoising losses already computed during training. Across natural-image benchmarks, InfoNoise matches or surpasses tuned EDM-style schedules, in some cases with a substantial training speedup (about $1.4\times$ on CIFAR-10). On discrete datasets, where standard image-tuned schedules exhibit significant mismatch, it reaches superior quality in up to $3\times$ fewer training steps. Overall, InfoNoise makes noise scheduling data-adaptive, reducing the need for per-dataset schedule design as diffusion models expand across domains.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºInfoNoiseï¼šç”¨ä¿¡æ¯è®ºæŒ‡æ ‡è‡ªé€‚åº”åˆ†é…æ‰©æ•£è®­ç»ƒä¸­çš„å™ªå£°é‡‡æ ·ï¼Œä»è€Œå‡å°‘æ— æ•ˆè®¡ç®—å¹¶æå‡è·¨æ•°æ®é›†/è¡¨ç¤ºçš„å¯è¿ç§»æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ‰‹å·¥è°ƒå™ªå£°æ—¥ç¨‹å¸¸åœ¨ä¿¡æ¯é‡ä½çš„å™ªå£°åŒºé—´æµªè´¹è®­ç»ƒæ­¥æ•°ï¼Œä¸”åœ¨ä¸åŒæ•°æ®é›†ã€åˆ†è¾¨ç‡æˆ–ç¦»æ•£è¡¨ç¤ºä¸Šå®¹æ˜“å¤±é…ï¼Œéœ€è¦ä¸€ç§æ•°æ®ä¾èµ–ã€å¯è¯Šæ–­ä¸”å¯è¿ç§»çš„å™ªå£°åˆ†é…åŸåˆ™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥å‰å‘æ‰©æ•£è¿‡ç¨‹çš„æ¡ä»¶ç†µç‡ï¼ˆconditional entropy rateï¼‰ä½œä¸ºè¯Šæ–­é‡ï¼Œè¡¡é‡ä¸åŒå™ªå£°æ°´å¹³å¸¦æ¥çš„ä¿¡æ¯å‡å°‘/å­¦ä¹ æ”¶ç›Šï¼›æ®æ­¤ä»è®­ç»ƒä¸­å·²è®¡ç®—çš„å»å™ªæŸå¤±ä¼°è®¡å„å™ªå£°æ®µçš„ç†µå‡å°‘ç‡ï¼Œå¹¶æ„é€ ä¿¡æ¯å¼•å¯¼çš„å™ªå£°é‡‡æ ·åˆ†å¸ƒï¼ˆInfoNoiseï¼‰æ›¿ä»£å¯å‘å¼å›ºå®šscheduleã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨è‡ªç„¶å›¾åƒä¸ŠInfoNoiseè¾¾åˆ°æˆ–è¶…è¿‡ç²¾è°ƒçš„EDMå¼scheduleï¼Œå¹¶å¸¦æ¥æ˜¾è‘—åŠ é€Ÿï¼ˆå¦‚CIFAR-10çº¦1.4Ã—ï¼‰ï¼›åœ¨ç¦»æ•£æ•°æ®ä¸Šç¼“è§£å›¾åƒè°ƒå‚scheduleçš„å¤±é…ï¼Œå¯ç”¨æœ€å¤š3Ã—æ›´å°‘è®­ç»ƒæ­¥æ•°è·å¾—æ›´å¥½è´¨é‡ï¼Œé™ä½æŒ‰æ•°æ®é›†å•ç‹¬è®¾è®¡å™ªå£°æ—¥ç¨‹çš„éœ€æ±‚ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹è®­ç»ƒ, å™ªå£°è°ƒåº¦, æ•°æ®è‡ªé€‚åº”å™ªå£°é‡‡æ ·, ä¿¡æ¯è®ºè¯Šæ–­, æ¡ä»¶ç†µç‡, å‰å‘æ‰©æ•£è¿‡ç¨‹, ç†µå‡ç‡ä¼°è®¡, å»å™ªæŸå¤±, è®­ç»ƒæ•ˆç‡åŠ é€Ÿ, è·¨æ•°æ®é›†è¿ç§», ç¦»æ•£æ•°æ®ç”Ÿæˆ

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18647v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18647v1.pdf)

---

## [10. Adaptive Time Series Reasoning via Segment Selection](https://arxiv.org/abs/2602.18645v1)

**ä½œè€…**ï¼šShvat Messica, Jiawen Zhang, Kevin Li ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Time series reasoning tasks often start with a natural language question and require targeted analysis of a time series. Evidence may span the full series or appear in a few short intervals, so the model must decide what to inspect. Most existing approaches encode the entire time series into a fixed representation before inference, regardless of whether or not the entire sequence is relevant. We introduce ARTIST, which formulates time-series reasoning as a sequential decision problem. ARTIST interleaves reasoning with adaptive temporal segment selection. It adopts a controller-reasoner architecture and uses reinforcement learning to train the controller role to select informative segments and the reasoner role to generate segment-conditioned reasoning traces and final answers. During inference, the model actively acquires task-relevant information instead of relying on a static summary of the full sequence. We use a novel hierarchical policy optimization approach for post-training that allows the model to excel in both segment selection and question-answering behavior. We evaluate ARTIST on six time-series reasoning benchmarks and compare it with large language models, vision-language models, and prior time-series reasoning systems. ARTIST improves average accuracy by 6.46 absolute percentage points over the strongest baseline. The largest gains appear on rare event localization and multi-segment reasoning tasks. Supervised fine-tuning improves performance, and reinforcement learning provides additional gains by optimizing question-adaptive segment selection. These results show that selective data use drives effective time-series reasoning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šARTISTé€šè¿‡è‡ªé€‚åº”æ—¶é—´æ®µé€‰æ‹©æ¥å¢å¼ºæ—¶é—´åºåˆ—æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†å‡†ç¡®æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ–¹æ³•å¾€å¾€å¯¹æ•´ä¸ªæ—¶é—´åºåˆ—è¿›è¡Œå›ºå®šè¡¨ç¤ºï¼Œè€Œå®é™…ä¸Šåªæœ‰éƒ¨åˆ†ç‰‡æ®µå¯èƒ½ä¸é—®é¢˜ç›¸å…³ï¼Œå› æ­¤éœ€è¦æ”¹è¿›æ¨ç†ç­–ç•¥ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šARTISTå°†æ—¶é—´åºåˆ—æ¨ç†è§†ä¸ºä¸€ä¸ªé¡ºåºå†³ç­–é—®é¢˜ï¼Œé‡‡ç”¨æ§åˆ¶å™¨-æ¨ç†å™¨æ¶æ„ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¿¡æ¯é€‰æ‹©å’Œæ¨ç†ç”Ÿæˆè¿‡ç¨‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šARTISTåœ¨å¤šä¸ªæ—¶é—´åºåˆ—æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨ç¨€æœ‰äº‹ä»¶å®šä½å’Œå¤šç‰‡æ®µæ¨ç†ä»»åŠ¡ä¸­ï¼Œè¯æ˜äº†é€‰æ‹©æ€§æ•°æ®ä½¿ç”¨å¯¹æœ‰æ•ˆæ¨ç†çš„é‡è¦æ€§ã€‚

**å…³é”®è¯**ï¼šæ—¶é—´åºåˆ—æ¨ç†, æ—¶é—´ç‰‡æ®µé€‰æ‹©, è‡ªé€‚åº”ä¿¡æ¯è·å–, åºåˆ—å†³ç­–, å¼ºåŒ–å­¦ä¹ , åˆ†å±‚ç­–ç•¥ä¼˜åŒ–, æ§åˆ¶å™¨-æ¨ç†å™¨æ¶æ„, ç‰‡æ®µæ¡ä»¶æ¨ç†, æ—¶é—´åºåˆ—é—®ç­”, ç¨€æœ‰äº‹ä»¶å®šä½, å¤šç‰‡æ®µæ¨ç†, ç›‘ç£å¾®è°ƒ

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18645v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18645v1.pdf)

---

## [11. Learning Invariant Visual Representations for Planning with Joint-Embedding Predictive World Models](https://arxiv.org/abs/2602.18639v1)

**ä½œè€…**ï¼šLeonardo F. Toso, Davit Shadunts, Yunyang Lu ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, math.OC  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

World models learned from high-dimensional visual observations allow agents to make decisions and plan directly in latent space, avoiding pixel-level reconstruction. However, recent latent predictive architectures (JEPAs), including the DINO world model (DINO-WM), display a degradation in test time robustness due to their sensitivity to "slow features". These include visual variations such as background changes and distractors that are irrelevant to the task being solved. We address this limitation by augmenting the predictive objective with a bisimulation encoder that enforces control-relevant state equivalence, mapping states with similar transition dynamics to nearby latent states while limiting contributions from slow features. We evaluate our model on a simple navigation task under different test-time background changes and visual distractors. Across all benchmarks, our model consistently improves robustness to slow features while operating in a reduced latent space, up to 10x smaller than that of DINO-WM. Moreover, our model is agnostic to the choice of pretrained visual encoder and maintains robustness when paired with DINOv2, SimDINOv2, and iBOT features.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šåœ¨è”åˆåµŒå…¥é¢„æµ‹ä¸–ç•Œæ¨¡å‹ï¼ˆJEPAï¼‰ä¸­å¼•å…¥åŒæ¨¡æ‹Ÿï¼ˆbisimulationï¼‰ç¼–ç çº¦æŸï¼Œæ˜¾è‘—æå‡å¯¹èƒŒæ™¯å˜åŒ–ä¸å¹²æ‰°ç‰©ç­‰â€œæ…¢ç‰¹å¾â€çš„æµ‹è¯•æ—¶é²æ£’æ€§ï¼Œå¹¶å¯ç”¨æ›´å°çš„æ½œç©ºé—´å®Œæˆè§„åˆ’ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰JEPAç±»è§†è§‰ä¸–ç•Œæ¨¡å‹ï¼ˆå¦‚DINO-WMï¼‰å¯¹ä¸ä»»åŠ¡æ— å…³çš„æ…¢ç‰¹å¾æ•æ„Ÿï¼Œå¯¼è‡´æµ‹è¯•æ—¶é‡åˆ°èƒŒæ™¯/å¹²æ‰°å˜åŒ–é²æ£’æ€§ä¸‹é™ï¼Œä»è€Œå½±å“æ½œç©ºé—´è§„åˆ’çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨é¢„æµ‹å¼æ½œè¡¨ç¤ºå­¦ä¹ ç›®æ ‡ä¸Šå¢æ·»bisimulation encoderï¼Œä½¿å…·æœ‰ç›¸ä¼¼æ§åˆ¶ç›¸å…³è½¬ç§»åŠ¨æ€çš„çŠ¶æ€åœ¨æ½œç©ºé—´ä¸­æ›´æ¥è¿‘ï¼Œä»è€ŒæŠ‘åˆ¶æ…¢ç‰¹å¾å¯¹è¡¨ç¤ºçš„è´¡çŒ®ï¼›å¹¶éªŒè¯è¯¥æ–¹æ³•å¯ä¸ä¸åŒé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ç‰¹å¾ï¼ˆDINOv2/SimDINOv2/iBOTï¼‰ç»„åˆä½¿ç”¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¯¼èˆªä»»åŠ¡çš„å¤šç§èƒŒæ™¯å˜åŒ–ä¸è§†è§‰å¹²æ‰°è¯„æµ‹ä¸­ï¼Œè¯¥æ¨¡å‹ç›¸è¾ƒDINO-WMæ›´ç¨³å¥ï¼ŒåŒæ—¶æ½œç©ºé—´ç»´åº¦å¯ç¼©å°è‡³å…¶çº¦1/10ï¼Œä¸”å¯¹æ‰€é€‰é¢„è®­ç»ƒè§†è§‰ç‰¹å¾å…·å¤‡è‰¯å¥½æ³›åŒ–ä¸å…¼å®¹æ€§ã€‚

**å…³é”®è¯**ï¼šè§†è§‰ä¸–ç•Œæ¨¡å‹, æ½œå˜é‡è§„åˆ’, è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJEPAï¼‰, æ§åˆ¶ç›¸å…³è¡¨å¾, æ…¢ç‰¹å¾æŠ‘åˆ¶, æµ‹è¯•æ—¶é²æ£’æ€§, è§†è§‰åˆ†å¸ƒåç§», èƒŒæ™¯å˜åŒ–ä¸å¹²æ‰°ç‰©, è‡ªç›‘ç£è§†è§‰ç‰¹å¾

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18639v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18639v1.pdf)

---

## [12. Online decoding of rat self-paced locomotion speed from EEG using recurrent neural networks](https://arxiv.org/abs/2602.18637v1)

**ä½œè€…**ï¼šAlejandro de Miguel, Nelson Totah, Uri Maoz  
**åˆ†ç±»**ï¼šcs.LG, q-bio.NC  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

$\textit{Objective.}$ Accurate neural decoding of locomotion holds promise for advancing rehabilitation, prosthetic control, and understanding neural correlates of action. Recent studies have demonstrated decoding of locomotion kinematics across species on motorized treadmills. However, efforts to decode locomotion speed in more natural contexts$-$where pace is self-selected rather than externally imposed$-$are scarce, generally achieve only modest accuracy, and require intracranial implants. Here, we aim to decode self-paced locomotion speed non-invasively and continuously using cortex-wide EEG recordings from rats. $\textit{Approach.}$ We introduce an asynchronous brain$-$computer interface (BCI) that processes a stream of 32-electrode skull-surface EEG (0.01$-$45 Hz) to decode instantaneous speed from a non-motorized treadmill during self-paced locomotion in head-fixed rats. Using recurrent neural networks and a dataset of over 133 h of recordings, we trained decoders to map ongoing EEG activity to treadmill speed. $\textit{Main results.}$ Our decoding achieves a correlation of 0.88 ($R^2$ = 0.78) for speed, primarily driven by visual cortex electrodes and low-frequency ($< 8$ Hz) oscillations. Moreover, pre-training on a single session permitted decoding on other sessions from the same rat, suggesting uniform neural signatures that generalize across sessions but fail to transfer across animals. Finally, we found that cortical states not only carry information about current speed, but also about future and past dynamics, extending up to 1000 ms. $\textit{Significance.}$ These findings demonstrate that self-paced locomotion speed can be decoded accurately and continuously from non-invasive, cortex-wide EEG. Our approach provides a framework for developing high-performing, non-invasive BCI systems and contributes to understanding distributed neural representations of action dynamics.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶é€šè¿‡éä¾µå…¥æ€§è„‘ç”µå›¾ï¼ˆEEGï¼‰å‡†ç¡®è§£ç å¤§é¼ è‡ªæˆ‘é€‰æ‹©çš„è¿åŠ¨é€Ÿåº¦ï¼Œå±•ç¤ºäº†é«˜æ•ˆçš„è„‘-è®¡ç®—æœºæ¥å£ï¼ˆBCIï¼‰æ½œåŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå‡†ç¡®è§£ç è¿åŠ¨é€Ÿåº¦æœ‰åŠ©äºæ¨è¿›åº·å¤ã€å‡è‚¢æ§åˆ¶åŠç†è§£åŠ¨ä½œçš„ç¥ç»ç›¸å…³æ€§ï¼Œä½†åœ¨è‡ªç„¶ç¯å¢ƒä¸­è§£ç çš„ç ”ç©¶è¾ƒå°‘ä¸”ç²¾åº¦æœ‰é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé‡‡ç”¨é€’å½’ç¥ç»ç½‘ç»œå¯¹32ç”µæè„‘ç”µå›¾æ•°æ®è¿›è¡Œå¤„ç†ï¼Œä»¥è§£ç å›ºå®šå¤´éƒ¨å¤§é¼ åœ¨éç”µåŠ¨è·‘æ­¥æœºä¸Šçš„è‡ªæˆ‘é€‰æ‹©é€Ÿåº¦ï¼Œè®­ç»ƒæ•°æ®è¶…è¿‡133å°æ—¶ã€‚

**ä¸»è¦ç»“è®º**ï¼šç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡éä¾µå…¥æ€§EEGå¯ä»¥æŒç»­è€Œå‡†ç¡®åœ°è§£ç è‡ªæˆ‘é€‰æ‹©çš„è¿åŠ¨é€Ÿåº¦ï¼Œä¸”è¿™ä¸€æ–¹æ³•ä¸ºå¼€å‘é«˜æ€§èƒ½çš„éä¾µå…¥æ€§BCIç³»ç»Ÿæä¾›äº†æ¡†æ¶ã€‚

**å…³é”®è¯**ï¼šè‡ªé€‰æ­¥é€Ÿè§£ç , è¿åŠ¨é€Ÿåº¦è§£ç , è„‘æœºæ¥å£, å¼‚æ­¥è„‘æœºæ¥å£, éä¾µå…¥å¼EEG, å¤§é¼ è„‘ç”µ, å¤å‘ç¥ç»ç½‘ç»œ, æ—¶åºå›å½’, ä½é¢‘æŒ¯è¡, è§†è§‰çš®å±‚è´¡çŒ®

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18637v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18637v1.pdf)

---

## [13. Non-Interfering Weight Fields: Treating Model Parameters as a Continuously Extensible Function](https://arxiv.org/abs/2602.18628v1)

**ä½œè€…**ï¼šSarim Chaudhry  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models store all learned knowledge in a single, fixed weight vector. Teaching a model new capabilities requires modifying those same weights, inevitably degrading previously acquired knowledge. This fundamental limitation, known as catastrophic forgetting, has resisted principled solutions for decades. Existing approaches treat weights as immutable artifacts that must be protected through techniques like regularization heuristics, replay buffers, or isolated adapter modules. The problem is none of these provide a structural guarantee against forgetting. In this work, we propose Non-Interfering Weight Fields (NIWF), a framework that replaces the fixed weight paradigm with a learned function that generates weight configurations on demand from a continuous capability coordinate space. After training on a task, we commit the occupied coordinate region by snapshotting the fields outputs on anchor points to enforce a functional lock during all future training. We validate NIWF on sequential instructionfollowing and code generation tasks using Mistral-7B, demonstrating zero forgetting on committed tasks with competitive perplexity on new tasks. The framework introduces the notion of software-like versioning for neural network intelligence, where capabilities can be committed, extended, composed, and rolled back without retraining.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºNIWFå°†æ¨¡å‹å‚æ•°ä»å›ºå®šæƒé‡å‘é‡æ”¹ä¸ºå¯ç”Ÿæˆæƒé‡çš„è¿ç»­å‡½æ•°ï¼Œå¹¶é€šè¿‡â€œæäº¤/é”å®šâ€å·²å­¦èƒ½åŠ›åŒºåŸŸå®ç°é¡ºåºå­¦ä¹ ä¸‹çš„é›¶é—å¿˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»ŸLLMæŠŠæ‰€æœ‰çŸ¥è¯†å‹åœ¨åŒä¸€ç»„æƒé‡é‡Œï¼Œæ–°ä»»åŠ¡å¾®è°ƒä¼šä¸å¯é¿å…åœ°å¹²æ‰°æ—§çŸ¥è¯†å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼›ç°æœ‰æ­£åˆ™åŒ–ã€å›æ”¾ã€é€‚é…å™¨ç­‰æ–¹æ³•ç¼ºä¹ç»“æ„æ€§çš„ä¸é—å¿˜ä¿è¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç”¨â€œæƒé‡åœºâ€å‡½æ•°ä»è¿ç»­çš„èƒ½åŠ›åæ ‡ç©ºé—´æŒ‰éœ€ç”Ÿæˆå¯¹åº”çš„æƒé‡é…ç½®ï¼›æ¯å­¦å®Œä¸€ä¸ªä»»åŠ¡å°±åœ¨å…¶å ç”¨çš„åæ ‡åŒºåŸŸé€‰å–é”šç‚¹å¹¶å¿«ç…§æƒé‡åœºè¾“å‡ºï¼Œåç»­è®­ç»ƒå¯¹è¿™äº›é”šç‚¹æ–½åŠ çº¦æŸä»¥åŠŸèƒ½æ€§é”å®šå·²æäº¤èƒ½åŠ›ï¼Œå®ç°ç±»ä¼¼è½¯ä»¶ç‰ˆæœ¬ç®¡ç†çš„æäº¤/æ‰©å±•/ç»„åˆ/å›æ»šã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨Mistral-7Bä¸Šçš„é¡ºåºæŒ‡ä»¤è·Ÿéšä¸ä»£ç ç”Ÿæˆå®éªŒè¡¨æ˜ï¼Œå¯¹å·²æäº¤ä»»åŠ¡å¯å®ç°é›¶é—å¿˜ï¼ŒåŒæ—¶åœ¨æ–°ä»»åŠ¡ä¸Šä¿æŒæœ‰ç«äº‰åŠ›çš„å›°æƒ‘åº¦ï¼Œå±•ç¤ºäº†ç”¨â€œç‰ˆæœ¬åŒ–èƒ½åŠ›åæ ‡â€æŒç»­æ‰©å±•æ¨¡å‹èƒ½åŠ›çš„å¯è¡Œæ€§ã€‚

**å…³é”®è¯**ï¼šç¾éš¾æ€§é—å¿˜, æŒç»­å­¦ä¹ , éå¹²æ‰°æƒé‡åœºï¼ˆNIWFï¼‰, æƒé‡åœºå‡½æ•°, èƒ½åŠ›åæ ‡ç©ºé—´, æƒé‡å¿«ç…§é”šç‚¹, åŠŸèƒ½é”å®š, é¡ºåºæŒ‡ä»¤å¾®è°ƒ, é¡ºåºä»£ç ç”Ÿæˆ, é›¶é—å¿˜è¯„æµ‹, èƒ½åŠ›ç‰ˆæœ¬æ§åˆ¶, èƒ½åŠ›ç»„åˆä¸å›æ»š

**è¯„åˆ†**ï¼š39

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18628v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18628v1.pdf)

---

## [14. Diagnosing LLM Reranker Behavior Under Fixed Evidence Pools](https://arxiv.org/abs/2602.18613v1)

**ä½œè€…**ï¼šBaris Arat, Emre Sefer  
**åˆ†ç±»**ï¼šcs.LG, cs.CL, cs.IR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Standard reranking evaluations study how a reranker orders candidates returned by an upstream retriever. This setup couples ranking behavior with retrieval quality, so differences in output cannot be attributed to the ranking policy alone. We introduce a controlled diagnostic that isolates reranking by using Multi-News clusters as fixed evidence pools. We limit each pool to exactly eight documents and pass identical inputs to all rankers. Within this setup, BM25 and MMR serve as interpretable reference points for lexical matching and diversity optimization. Across 345 clusters, we find that redundancy patterns vary by model: one LLM implicitly diversifies at larger selection budgets, while another increases redundancy. In contrast, LLMs underperform on lexical coverage at small selection budgets. As a result, LLM rankings diverge substantially from both baselines rather than consistently approximating either strategy. By eliminating retrieval variance, we can attribute these differences directly to the ranking policy. This diagnostic is model-agnostic and applicable to any ranker, including open source systems and proprietary APIs.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§åœ¨å›ºå®šè¯æ®æ± ä¸‹è¯Šæ–­LLMé‡æ’åºå™¨è¡Œä¸ºçš„è¯„æµ‹æ¡†æ¶ï¼Œå‘ç°ä¸åŒLLMçš„å†—ä½™/å¤šæ ·æ€§ä¸è¯æ³•è¦†ç›–ç­–ç•¥å·®å¼‚æ˜¾è‘—ä¸”ä¸ç¨³å®šåœ°åç¦»BM25ä¸MMRåŸºçº¿ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿé‡æ’åºè¯„æµ‹ä¾èµ–ä¸Šæ¸¸æ£€ç´¢ç»“æœï¼Œæ’åºè¡¨ç°ä¸æ£€ç´¢è´¨é‡è€¦åˆï¼Œéš¾ä»¥å°†å·®å¼‚å½’å› åˆ°â€œæ’åºç­–ç•¥â€æœ¬èº«ã€‚éœ€è¦ä¸€ç§å¯æ§è®¾ç½®æ¥éš”ç¦»æ£€ç´¢æ–¹å·®ï¼Œç›´æ¥è§‚å¯Ÿé‡æ’åºå™¨çš„è¡Œä¸ºç‰¹å¾ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥Multi-Newsçš„æ¯ä¸ªèšç±»ä½œä¸ºå›ºå®šè¯æ®æ± ï¼Œå°†å€™é€‰æ–‡æ¡£æ•°ä¸¥æ ¼é™åˆ¶ä¸º8ç¯‡ï¼Œå¹¶å¯¹æ‰€æœ‰rankerè¾“å…¥å®Œå…¨ä¸€è‡´çš„å€™é€‰é›†åˆã€‚ç”¨BM25ï¼ˆè¯æ³•åŒ¹é…ï¼‰ä¸MMRï¼ˆå¤šæ ·æ€§ä¼˜åŒ–ï¼‰ä½œä¸ºå¯è§£é‡Šå‚ç…§ï¼Œåœ¨345ä¸ªèšç±»ä¸Šæ¯”è¾ƒä¸åŒé€‰æ‹©é¢„ç®—ä¸‹çš„å†—ä½™æ¨¡å¼ä¸è¯æ³•è¦†ç›–ç­‰æŒ‡æ ‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å›ºå®šè¯æ®æ± ä¸­ï¼Œä¸åŒLLMéšé€‰æ‹©é¢„ç®—å˜åŒ–å‘ˆç°ä¸åŒå†—ä½™è¶‹åŠ¿ï¼šæœ‰çš„åœ¨å¤§é¢„ç®—ä¸‹éšå¼å¤šæ ·åŒ–ï¼Œæœ‰çš„åè€Œæ›´å†—ä½™ï¼›ä¸”åœ¨å°é¢„ç®—ä¸‹LLMçš„è¯æ³•è¦†ç›–æ™®éå¼±äºåŸºçº¿ã€‚æ€»ä½“ä¸ŠLLMæ’åºç»“æœå¹¶éç¨³å®šé€¼è¿‘BM25æˆ–MMRï¼Œè€Œæ˜¯æ˜¾è‘—åˆ†æ­§ï¼Œè¯´æ˜å·®å¼‚å¯ç›´æ¥å½’å› äºæ’åºç­–ç•¥è€Œéæ£€ç´¢æ³¢åŠ¨ã€‚

**å…³é”®è¯**ï¼šLLM é‡æ’åº, é‡æ’åºè¯„æµ‹, å›ºå®šè¯æ®æ± , å¯æ§è¯Šæ–­è¯„æµ‹, æ£€ç´¢-æ’åºè§£è€¦, BM25 åŸºçº¿, æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼ˆMMRï¼‰, å¤šæ ·æ€§ä¼˜åŒ–, å†—ä½™åº¦åˆ†æ, è¯æ±‡è¦†ç›–ç‡, é€‰æ‹©é¢„ç®—ï¼ˆtop-kï¼‰

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18613v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18613v1.pdf)

---

## [15. MapTab: Can MLLMs Master Constrained Route Planning?](https://arxiv.org/abs/2602.18600v1)

**ä½œè€…**ï¼šZiqiao Shang, Lingyue Ge, Yang Chen ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-20

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Systematic evaluation of Multimodal Large Language Models (MLLMs) is crucial for advancing Artificial General Intelligence (AGI). However, existing benchmarks remain insufficient for rigorously assessing their constrained reasoning capabilities. To bridge this gap, we introduce MapTab, a multimodal benchmark specifically designed to evaluate constrained reasoning in MLLMs via route planning tasks. MapTab requires MLLMs to perceive and ground visual cues from map images alongside route attributes (e.g., Time, Price) from structured tabular data. The benchmark encompasses two scenarios: Metromap, covering metro networks in 160 cities across 52 countries, and Travelmap, depicting 168 representative tourist attractions from 19 countries. In total, MapTab comprises 328 images, 196,800 route planning queries, and 3,936 QA queries, all incorporating 4 key constraints: Time, Price, Comfort, and Reliability. Extensive evaluations across 15 representative MLLMs reveal that current models face substantial challenges in constrained multimodal reasoning. Notably, under conditions of limited visual perception, multimodal collaboration often underperforms compared to unimodal approaches. We believe MapTab provides a challenging and realistic testbed to advance the systematic evaluation of MLLMs.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºå¹¶å‘å¸ƒMapTabåŸºå‡†ï¼Œç”¨å¤šæ¨¡æ€åœ°å›¾+è¡¨æ ¼çº¦æŸçš„è·¯å¾„è§„åˆ’ä»»åŠ¡ç³»ç»Ÿè¯„æµ‹MLLMçš„å—çº¦æŸæ¨ç†èƒ½åŠ›ï¼Œå‘ç°ç°æœ‰æ¨¡å‹æ•´ä½“è¡¨ç°ä»æ˜¾è‘—ä¸è¶³ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†éš¾ä»¥ä¸¥æ ¼è¡¡é‡æ¨¡å‹åœ¨â€œå¤šçº¦æŸæ¡ä»¶ä¸‹çš„æ¨ç†ä¸å†³ç­–â€èƒ½åŠ›ï¼Œå°¤å…¶ç¼ºå°‘è´´è¿‘çœŸå®åœºæ™¯çš„è·¯å¾„è§„åˆ’ç±»ä»»åŠ¡ã€‚ä¸ºæ­¤éœ€è¦ä¸€ä¸ªåŒæ—¶è¦æ±‚è§†è§‰å®šä½ã€è¡¨æ ¼å±æ€§å¯¹é½ä¸çº¦æŸä¼˜åŒ–çš„ç»Ÿä¸€æµ‹è¯•å¹³å°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºMapTabï¼šåŒ…å«Metromapï¼ˆ52å›½160åŸåœ°é“ç½‘ç»œï¼‰ä¸Travelmapï¼ˆ19å›½168æ™¯ç‚¹ï¼‰ä¸¤ç±»åœ°å›¾å›¾åƒï¼Œå¹¶é…å¥—ç»“æ„åŒ–è¡¨æ ¼è·¯ç”±å±æ€§ä¸æŸ¥è¯¢ï¼Œè¦†ç›–æ—¶é—´/ä»·æ ¼/èˆ’é€‚/å¯é æ€§å››å¤§çº¦æŸï¼›åœ¨15ä¸ªä»£è¡¨æ€§MLLMä¸Šè¿›è¡Œå¤§è§„æ¨¡è¯„æµ‹ï¼ˆ196,800æ¡è§„åˆ’æŸ¥è¯¢ä¸3,936æ¡QAï¼‰ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜å½“å‰MLLMåœ¨å¤šæ¨¡æ€å—çº¦æŸè·¯å¾„è§„åˆ’ä¸Šå­˜åœ¨æ˜¾è‘—å›°éš¾ï¼›åœ¨è§†è§‰æ„ŸçŸ¥å—é™æ—¶ï¼Œå¤šæ¨¡æ€åä½œæ–¹æ¡ˆç”šè‡³å¯èƒ½ä¸å¦‚çº¯å•æ¨¡æ€æ–¹æ³•ï¼Œè¯´æ˜å¤šæ¨¡æ€èåˆä¸çº¦æŸæ¨ç†ä»æ˜¯å…³é”®ç“¶é¢ˆã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€LLMè¯„æµ‹, å¤šæ¨¡æ€åŸºå‡†, çº¦æŸæ¨ç†, çº¦æŸè·¯å¾„è§„åˆ’, è§†è§‰è¯­ä¹‰å¯¹é½, è§†è§‰-è¡¨æ ¼èåˆ, å¤šçº¦æŸä¼˜åŒ–, åœ°é“ç½‘ç»œå¯¼èˆª, å¤šæ¨¡æ€åä½œ

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18600v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18600v1.pdf)

---

