# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-25

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.LG](#csLG) (8 ç¯‡)
- [cs.CV](#csCV) (7 ç¯‡)

---

## cs.CV

## [1. Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking](https://arxiv.org/abs/2602.21435v1)

**ä½œè€…**ï¼šShengqiong Wu, Bobo Li, Xinkai Wang ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº¤é”™çš„â€œåˆ†æ-èµ·è‰â€æ€è€ƒå¾ªç¯ï¼ˆAD-Loopï¼‰ï¼Œè®©ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£ä¸ç”Ÿæˆé—´åŠ¨æ€è¿­ä»£ï¼Œä»è€ŒåŒæ—¶æå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆæ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰UVLMå¤šå¼ºè°ƒæ¶æ„å±‚é¢çš„ç»Ÿä¸€ï¼Œä½†åœ¨è§£é¢˜è¿‡ç¨‹ä¸­ç¼ºå°‘ç†è§£ä¸ç”Ÿæˆçš„æ˜¾å¼äº¤äº’ï¼Œå¯¼è‡´ä¸¤ç§èƒ½åŠ›æ›´åƒå¹¶è¡ŒæŠ€èƒ½è€Œéç›¸äº’ä¿ƒè¿›çš„ååŒè¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šAD-Loopåœ¨æ¨ç†æ—¶äº¤æ›¿è¿›è¡Œåˆ†æï¼ˆç†è§£ã€æ¨æ–­ï¼‰ä¸èµ·è‰ï¼ˆç”Ÿæˆã€æ”¹å†™ï¼‰æ­¥éª¤ï¼Œå¹¶å°†æ–‡æœ¬æ€è€ƒä¸éšå¼è§†è§‰æ€è€ƒäº¤é”™èåˆä»¥é€æ­¥ä¿®æ­£è®¤çŸ¥ä¸è¾“å‡ºï¼›è®­ç»ƒé‡‡ç”¨ä¸¤é˜¶æ®µï¼šå…ˆç”¨äº¤é”™æ€ç»´æ•°æ®åšç›‘ç£å­¦ä¹ åˆå§‹åŒ–äº¤æ›¿æœºåˆ¶ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ å­¦ä¹ æ›´è‡ªé€‚åº”çš„å¾ªç¯æ§åˆ¶ç­–ç•¥ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§ç†è§£ä¸ç”ŸæˆåŸºå‡†ä¸Šï¼ŒAD-Loopå¸¦æ¥ç¨³å®šä¸€è‡´çš„æå‡ï¼Œå¹¶èƒ½è¾ƒå¥½è¿ç§»åˆ°ä¸åŒUVLMæ¶æ„ï¼›å¯è§†åŒ–åˆ†æä¹Ÿæ”¯æŒâ€œéšå¼è§†è§‰æ€è€ƒâ€ç¡®å®æœ‰åŠ©äºæ”¹è¿›æ¨ç†ä¸ç”Ÿæˆè´¨é‡ã€‚

**å…³é”®è¯**ï¼šç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹, äº¤é”™åˆ†æ-èµ·è‰å¾ªç¯, äº¤é”™æ€ç»´é“¾, è§†è§‰æ€ç»´, æ–‡æœ¬-è§†è§‰äº¤é”™æ¨ç†, ä¸¤é˜¶æ®µè®­ç»ƒ, ç›‘ç£å¾®è°ƒ, å¼ºåŒ–å­¦ä¹ æ§åˆ¶, å¤šæ¨¡æ€åŸºå‡†è¯„æµ‹, è·¨æ¶æ„è¿ç§»

**è¯„åˆ†**ï¼š38

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21435v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21435v1.pdf)

---

## [2. PSF-Med: Measuring and Explaining Paraphrase Sensitivity in Medical Vision Language Models](https://arxiv.org/abs/2602.21428v1)

**ä½œè€…**ï¼šBinesh Sadanandan, Vahid Behzadan  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Medical Vision Language Models (VLMs) can change their answers when clinicians rephrase the same question, which raises deployment risks. We introduce Paraphrase Sensitivity Failure (PSF)-Med, a benchmark of 19,748 chest Xray questions paired with about 92,000 meaningpreserving paraphrases across MIMIC-CXR and PadChest. Across six medical VLMs, we measure yes/no flips for the same image and find flip rates from 8% to 58%. However, low flip rate does not imply visual grounding: text-only baselines show that some models stay consistent even when the image is removed, suggesting they rely on language priors. To study mechanisms in one model, we apply GemmaScope 2 Sparse Autoencoders (SAEs) to MedGemma 4B and analyze FlipBank, a curated set of 158 flip cases. We identify a sparse feature at layer 17 that correlates with prompt framing and predicts decision margin shifts. In causal patching, removing this feature's contribution recovers 45% of the yesminus-no logit margin on average and fully reverses 15% of flips. Acting on this finding, we show that clamping the identified feature at inference reduces flip rates by 31% relative with only a 1.3 percentage-point accuracy cost, while also decreasing text-prior reliance. These results suggest that flip rate alone is not enough; robustness evaluations should test both paraphrase stability and image reliance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPSF-Medæå‡ºä¸€ä¸ªåŒ»ç–—VLMå¤è¿°æ•æ„Ÿæ€§åŸºå‡†ï¼Œå‘ç°æ¨¡å‹å¯¹åŒä¹‰æ”¹å†™ä¼šå‡ºç°æ˜¾è‘—â€œæ˜¯/å¦â€ç¿»è½¬ï¼Œå¹¶é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨å®šä½å¯å¹²é¢„çš„å†…éƒ¨ç‰¹å¾ä»¥é™ä½ç¿»è½¬ä¸”ä»£ä»·å¾ˆå°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¸´åºŠéƒ¨ç½²ä¸­åŒä¸€é—®é¢˜çš„ä¸åŒè¡¨è¿°å¯èƒ½å¯¼è‡´åŒ»ç–—VLMç»™å‡ºä¸åŒç­”æ¡ˆï¼Œå¸¦æ¥å®‰å…¨ä¸å¯ä¿¡é£é™©ï¼›ä¸”ä»…çœ‹ç¿»è½¬ç‡å¯èƒ½æ©ç›–æ¨¡å‹å…¶å®ä¸»è¦ä¾èµ–è¯­è¨€å…ˆéªŒè€Œéå›¾åƒè¯æ®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºPSF-Medï¼ˆ19,748ä¸ªèƒ¸ç‰‡é—®é¢˜+çº¦92,000æ¡ä¿ä¹‰å¤è¿°ï¼‰è¯„æµ‹6ä¸ªåŒ»å­¦VLMçš„åŒå›¾åŒä¹‰é—®æ³•â€œyes/noç¿»è½¬ç‡â€ï¼Œå¹¶ç”¨å»å›¾æ–‡æœ¬åŸºçº¿æ£€éªŒå›¾åƒä¾èµ–ï¼›è¿›ä¸€æ­¥å¯¹MedGemma 4Bç”¨GemmaScope2çš„SAEåˆ†æFlipBankç¿»è½¬æ ·ä¾‹ï¼Œåšå› æœpatchingå¹¶åœ¨æ¨ç†æ—¶å¯¹å…³é”®ç¨€ç–ç‰¹å¾è¿›è¡Œclampä»¥å‡å°‘ç¿»è½¬ã€‚

**ä¸»è¦ç»“è®º**ï¼šä¸åŒåŒ»å­¦VLMçš„ç¿»è½¬ç‡é«˜è¾¾8%â€“58%ï¼Œä¸”ä½ç¿»è½¬å¹¶ä¸ä»£è¡¨è§†è§‰æ‰æ ¹ï¼ˆå»å›¾ä»å¯èƒ½ä¿æŒä¸€è‡´ï¼‰ï¼›åœ¨MedGemmaä¸­å®šä½åˆ°ä¸æç¤ºæ¡†æ¶ç›¸å…³çš„å±‚17ç¨€ç–ç‰¹å¾ï¼Œå¹²é¢„è¯¥ç‰¹å¾å¯æ¢å¤éƒ¨åˆ†logit marginå¹¶å°†ç¿»è½¬ç‡ç›¸å¯¹é™ä½31%ä¸”ä»…æŸå¤±çº¦1.3ä¸ªç™¾åˆ†ç‚¹å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘å¯¹æ–‡æœ¬å…ˆéªŒçš„ä¾èµ–ã€‚

**å…³é”®è¯**ï¼šåŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹, é‡Šä¹‰é²æ£’æ€§, é‡Šä¹‰æ•æ„Ÿæ€§å¤±è´¥, èƒ¸éƒ¨Xå…‰é—®ç­”, é²æ£’æ€§åŸºå‡†, æ˜¯éç¿»è½¬ç‡, è§†è§‰åŸºç¡€è¯„æµ‹, æ–‡æœ¬å…ˆéªŒä¾èµ–, ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰, å¯è§£é‡Šç‰¹å¾åˆ†æ, å› æœè¡¥ä¸, æ¨ç†æ—¶ç‰¹å¾é’³åˆ¶

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21428v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21428v1.pdf)

---

## [3. Automating Timed Up and Go Phase Segmentation and Gait Analysis via the tugturn Markerless 3D Pipeline](https://arxiv.org/abs/2602.21425v1)

**ä½œè€…**ï¼šAbel GonÃ§alves Chinaglia, Guilherme Manna Cesar, Paulo Roberto Pereira Santiago  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Instrumented Timed Up and Go (TUG) analysis can support clinical and research decision-making, but robust and reproducible markerless pipelines are still limited. We present \textit{tugturn.py}, a Python-based workflow for 3D markerless TUG processing that combines phase segmentation, gait-event detection, spatiotemporal metrics, intersegmental coordination, and dynamic stability analysis. The pipeline uses spatial thresholds to segment each trial into stand, first gait, turning, second gait, and sit phases, and applies a relative-distance strategy to detect heel-strike and toe-off events within valid gait windows. In addition to conventional kinematics, \textit{tugturn} provides Vector Coding outputs and Extrapolated Center of Mass (XCoM)-based metrics. The software is configured through TOML files and produces reproducible artifacts, including HTML reports, CSV tables, and quality-assurance visual outputs. A complete runnable example is provided with test data and command-line instructions. This manuscript describes the implementation, outputs, and reproducibility workflow of \textit{tugturn} as a focused software contribution for markerless biomechanical TUG analysis.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºå¹¶å¼€æºäº†tugturn.pyï¼šä¸€ä¸ªå¯å¤ç°çš„æ— æ ‡è®°3D TUGè‡ªåŠ¨å¤„ç†æµæ°´çº¿ï¼Œè¦†ç›–ç›¸ä½åˆ†å‰²ã€æ­¥æ€äº‹ä»¶æ£€æµ‹ä¸å¤šç±»æ­¥æ€/ç¨³å®šæ€§æŒ‡æ ‡è¾“å‡ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¸´åºŠä¸ç ”ç©¶ä¸­çš„TUGé‡åŒ–åˆ†æéœ€è¦ç¨³å¥ã€å¯å¤ç°çš„æ— æ ‡è®°æµç¨‹ï¼Œä½†ç°æœ‰æ–¹æ¡ˆåœ¨è‡ªåŠ¨åˆ†ç›¸ã€äº‹ä»¶æ£€æµ‹ä¸æ ‡å‡†åŒ–äº§ç‰©è¾“å‡ºä¸Šä»è¾ƒä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç”¨ç©ºé—´é˜ˆå€¼å°†TUGåˆ†ä¸ºèµ·ç«‹ã€ç¬¬ä¸€æ®µè¡Œèµ°ã€è½¬èº«ã€ç¬¬äºŒæ®µè¡Œèµ°ã€åä¸‹äº”ç›¸ï¼Œå¹¶åœ¨æœ‰æ•ˆæ­¥æ€çª—å£å†…ç”¨ç›¸å¯¹è·ç¦»ç­–ç•¥æ£€æµ‹è¶³è·Ÿç€åœ°/ç¦»åœ°ï¼›åŒæ—¶è®¡ç®—æ—¶ç©ºå‚æ•°ã€Vector Codingæ®µé—´åè°ƒä¸åŸºäºXCoMçš„åŠ¨æ€ç¨³å®šæ€§æŒ‡æ ‡ï¼Œé‡‡ç”¨TOMLé…ç½®å¹¶è¾“å‡ºHTML/CSVä¸è´¨æ£€å¯è§†åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼štugturnæä¾›äº†ç«¯åˆ°ç«¯ã€å¯é…ç½®ä¸”å¯å¤ç°çš„æ— æ ‡è®°3D TUGåˆ†æå®ç°ä¸ç¤ºä¾‹æ•°æ®/å‘½ä»¤è¡Œæµç¨‹ï¼Œèƒ½å¤Ÿç¨³å®šäº§å‡ºåˆ†ç›¸ã€æ­¥æ€äº‹ä»¶åŠä¼ ç»Ÿä¸é«˜çº§ç”Ÿç‰©åŠ›å­¦æŒ‡æ ‡ï¼Œä½œä¸ºèšç„¦è½¯ä»¶è´¡çŒ®æ”¯æ’‘æ ‡å‡†åŒ–TUGç ”ç©¶ä¸åº”ç”¨ã€‚

**å…³é”®è¯**ï¼šæ— æ ‡è®°3DåŠ¨ä½œæ•æ‰, æ­¥æ€é˜¶æ®µåˆ†å‰², æ­¥æ€äº‹ä»¶æ£€æµ‹, è¶³è·Ÿç€åœ°/è¶³å°–ç¦»åœ°, æ­¥æ€æ—¶ç©ºå‚æ•°, åŠ¨æ€ç¨³å®šæ€§åˆ†æ, å¤–æ¨è´¨å¿ƒï¼ˆXCoMï¼‰, å¯å¤ç°ç”Ÿç‰©åŠ›å­¦å·¥ä½œæµ

**è¯„åˆ†**ï¼š32

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21425v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21425v1.pdf)

---

## [4. ECHOSAT: Estimating Canopy Height Over Space And Time](https://arxiv.org/abs/2602.21421v1)

**ä½œè€…**ï¼šJan Pauls, Karsten SchrÃ¶dter, Sven Ligensa ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Forest monitoring is critical for climate change mitigation. However, existing global tree height maps provide only static snapshots and do not capture temporal forest dynamics, which are essential for accurate carbon accounting. We introduce ECHOSAT, a global and temporally consistent tree height map at 10 m resolution spanning multiple years. To this end, we resort to multi-sensor satellite data to train a specialized vision transformer model, which performs pixel-level temporal regression. A self-supervised growth loss regularizes the predictions to follow growth curves that are in line with natural tree development, including gradual height increases over time, but also abrupt declines due to forest loss events such as fires. Our experimental evaluation shows that our model improves state-of-the-art accuracies in the context of single-year predictions. We also provide the first global-scale height map that accurately quantifies tree growth and disturbances over time. We expect ECHOSAT to advance global efforts in carbon monitoring and disturbance assessment. The maps can be accessed at https://github.com/ai4forest/echosat.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šECHOSAT åˆ©ç”¨å¤šæºå«æ˜Ÿæ•°æ®ä¸ä¸“é—¨çš„è§†è§‰Transformerï¼Œåœ¨10ç±³åˆ†è¾¨ç‡ä¸Šç”Ÿæˆå…¨çƒå¤šå¹´ä¸€è‡´çš„æ ‘é«˜æ—¶åºå›¾ï¼Œå®ç°å¯¹æ£®æ—ç”Ÿé•¿ä¸æ‰°åŠ¨çš„åŠ¨æ€ç›‘æµ‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å…¨çƒæ ‘é«˜äº§å“å¤šä¸ºå•ä¸€å¹´ä»½çš„é™æ€å¿«ç…§ï¼Œéš¾ä»¥åæ˜ æ£®æ—éšæ—¶é—´çš„ç”Ÿé•¿ä¸æŸå¤±ï¼Œè¿›è€Œå½±å“ç¢³æ ¸ç®—ä¸æ‰°åŠ¨è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šèåˆå¤šä¼ æ„Ÿå™¨å«æ˜Ÿè§‚æµ‹è®­ç»ƒè§†è§‰Transformeråšåƒç´ çº§æ—¶é—´å›å½’ï¼Œå¹¶å¼•å…¥è‡ªç›‘ç£â€œç”Ÿé•¿æŸå¤±â€çº¦æŸé¢„æµ‹ç¬¦åˆè‡ªç„¶ç”Ÿé•¿æ›²çº¿ï¼ˆç¼“æ…¢å¢é«˜ï¼‰ä¸”èƒ½åˆ»ç”»ç«ç¾ç­‰å¯¼è‡´çš„çªå‘ä¸‹é™ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å•å¹´æ ‘é«˜é¢„æµ‹ä¸Šä¼˜äºç°æœ‰SOTAï¼ŒåŒæ—¶æä¾›é¦–ä¸ªå…¨çƒå°ºåº¦ã€å¯é‡åŒ–å¤šå¹´ç”Ÿé•¿ä¸æ‰°åŠ¨çš„10ç±³æ ‘é«˜æ—¶åºåœ°å›¾ï¼Œæ”¯æŒæ›´å¯é çš„ç¢³ç›‘æµ‹ä¸æ£®æ—æ‰°åŠ¨è¯„ä¼°ã€‚

**å…³é”®è¯**ï¼šå† å±‚é«˜åº¦ä¼°è®¡, å…¨çƒæ ‘é«˜åˆ¶å›¾, æ—¶åºä¸€è‡´æ€§åˆ¶å›¾, å¤šä¼ æ„Ÿå™¨é¥æ„Ÿæ•°æ®, åƒç´ çº§æ—¶é—´å›å½’, è‡ªç›‘ç£å­¦ä¹ , ç”Ÿé•¿æ›²çº¿æ­£åˆ™åŒ–, æ£®æ—æ‰°åŠ¨ç›‘æµ‹, æ£®æ—ç«ç¾æŸå¤±æ£€æµ‹, ç¢³æ ¸ç®—ç›‘æµ‹

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21421v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21421v1.pdf)

---

## [5. WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions](https://arxiv.org/abs/2602.21416v1)

**ä½œè€…**ï¼šMarco Terral, Haotian Zhang, Tianyang Zhang ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºå¹¶æ„å»º WildSVG åŸºå‡†ä»¥è¯„æµ‹â€œä»çœŸå®ä¸–ç•Œå›¾åƒä¸­æå–å¹¶ç”ŸæˆSVGâ€çš„èƒ½åŠ›ï¼Œå‘ç°ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨çœŸå®å™ªå£°åœºæ™¯ä¸‹æ˜¾è‘—ä¸è¶³ï¼Œä½†è¿­ä»£å¼ç²¾ç‚¼æ˜¾ç¤ºå‡ºæ”¹è¿›æ½œåŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰SVGç”Ÿæˆæ–¹æ³•å¤šåœ¨å¹²å‡€æ¸²æŸ“æˆ–çº¯æ–‡æœ¬æ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†é¢å¯¹çœŸå®ç…§ç‰‡ä¸­çš„å™ªå£°ã€é®æŒ¡ä¸åŸŸåç§»æ—¶å¯é æ€§ä¸è¶³ï¼›åŒæ—¶ç¼ºå°‘å¯ç³»ç»Ÿè¯„æµ‹çœŸå®åœºæ™¯SVGæå–çš„åŸºå‡†æ•°æ®é›†ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºâ€œSVG extractionâ€ä»»åŠ¡ï¼Œå¹¶å‘å¸ƒ WildSVG Benchmarkï¼šNatural WildSVGï¼ˆçœŸå®å›¾åƒä¸­çš„å…¬å¸logoä¸å¯¹åº”SVGæ ‡æ³¨ï¼‰ä¸ Synthetic WildSVGï¼ˆå°†å¤æ‚SVGæ¸²æŸ“åˆæˆåˆ°çœŸå®åœºæ™¯ä»¥æ¨¡æ‹Ÿå›°éš¾æ¡ä»¶ï¼‰ï¼›åœ¨è¯¥åŸºå‡†ä¸Šç³»ç»Ÿè¯„æµ‹å¤šç§SOTAå¤šæ¨¡æ€æ¨¡å‹ï¼Œå¹¶è€ƒå¯Ÿè¿­ä»£å¼ç”Ÿæˆ/ç²¾ç‚¼ç­–ç•¥ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜å½“å‰SOTAæ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒSVGæå–ä¸Šè·ç¦»å¯ç”¨ä»æœ‰æ˜æ˜¾å·®è·ï¼›ä¸è¿‡é€šè¿‡è¿­ä»£ç²¾ç‚¼ç­‰æ–¹æ³•å¯æ˜¾è‘—æå‡æ•ˆæœï¼Œä¸”æ•´ä½“èƒ½åŠ›å‘ˆæŒç»­è¿›æ­¥è¶‹åŠ¿ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€æ¨¡å‹, çœŸå®åœºæ™¯, è‡ªç„¶å›¾åƒ, åˆæˆå›¾åƒ, è¿­ä»£ä¼˜åŒ–, åŸºå‡†è¯„æµ‹, WildSVG, Towards

**è¯„åˆ†**ï¼š26

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21416v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21416v1.pdf)

---

## [6. Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation](https://arxiv.org/abs/2602.21406v1)

**ä½œè€…**ï¼šAsim Unmesh, Kaki Ramesh, Mayank Patel ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºå¼€æ”¾è¯æ±‡é›¶æ ·æœ¬æ—¶åºåŠ¨ä½œåˆ†å‰²ï¼ˆOVTASï¼‰ï¼Œç”¨è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å°†é€å¸§åŒ¹é…ä¸æ—¶åºä¸€è‡´æ€§åˆ†å‰²ç»“åˆï¼Œå®ç°å¯¹ä»»æ„å€™é€‰åŠ¨ä½œæ ‡ç­¾çš„åˆ†å‰²ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šTAS é¢ä¸´åŠ¨ä½œç±»åˆ«ç©ºé—´å·¨å¤§ä¸”æ ‡æ³¨ä»£ä»·é«˜çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¤šä¾èµ–å°é—­æ ‡ç­¾é›†ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æœªè§åŠ¨ä½œã€‚VLM å…·å¤‡å¼ºé›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼Œå› æ­¤æ¢ç´¢å…¶ç”¨äºå¼€æ”¾è¯æ±‡çš„ç»“æ„åŒ–æ—¶åºç†è§£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®­ç»ƒè‡ªç”±çš„â€œå…ˆåˆ†ç±»å†åˆ†å‰²â€æµæ°´çº¿ï¼šç”¨ FAES è®¡ç®—å¸§ç‰¹å¾ä¸æ–‡æœ¬åŠ¨ä½œæ ‡ç­¾çš„åµŒå…¥ç›¸ä¼¼åº¦å¾—åˆ°é€å¸§ç±»åˆ«å“åº”ï¼›å†ç”¨ SMTS åŸºäºç›¸ä¼¼åº¦çŸ©é˜µæ–½åŠ æ—¶åºä¸€è‡´æ€§çº¦æŸï¼Œç”Ÿæˆè¿ç»­åŠ¨ä½œç‰‡æ®µè¾¹ç•Œä¸æ ‡ç­¾ã€‚å¹¶ç³»ç»Ÿè¯„æµ‹ 14 ç§ VLM åœ¨ OVTAS ä¸Šçš„é€‚ç”¨æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ ‡å‡†åŸºå‡†ä¸Šæ— éœ€ä»»åŠ¡ç›‘ç£å³å¯å–å¾—æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ•ˆæœï¼Œè¡¨æ˜ VLM èƒ½æ”¯æŒå¼€æ”¾è¯æ±‡çš„æ—¶åºåŠ¨ä½œç†è§£ã€‚ä¸åŒ VLM åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç³»ç»Ÿè¯„æµ‹ä¸ºé€‰æ‹©/æ”¹è¿› VLM ç”¨äºåŠ¨ä½œåˆ†å‰²æä¾›äº†ä¾æ®ã€‚

**å…³é”®è¯**ï¼šæ—¶åºåŠ¨ä½œåˆ†å‰², å¼€æ”¾è¯è¡¨, é›¶æ ·æœ¬å­¦ä¹ , è§†è§‰-è¯­è¨€æ¨¡å‹, è®­ç»ƒ-free æ¨ç†, åˆ†å‰²-åˆ†ç±»æ¡†æ¶, å¸§-åŠ¨ä½œåµŒå…¥ç›¸ä¼¼åº¦, ç›¸ä¼¼åº¦çŸ©é˜µåˆ†å‰², æ—¶åºä¸€è‡´æ€§çº¦æŸ, å¤šæ¨¡å‹ç³»ç»Ÿè¯„æµ‹, å¼€æ”¾è¯è¡¨åŠ¨ä½œè¯†åˆ«

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21406v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21406v1.pdf)

---

## [7. FlowFixer: Towards Detail-Preserving Subject-Driven Generation](https://arxiv.org/abs/2602.21402v1)

**ä½œè€…**ï¼šJinyoung Jun, Won-Dong Jang, Wenbin Ouyang ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present FlowFixer, a refinement framework for subject-driven generation (SDG) that restores fine details lost during generation caused by changes in scale and perspective of a subject. FlowFixer proposes direct image-to-image translation from visual references, avoiding ambiguities in language prompts. To enable image-to-image training, we introduce a one-step denoising scheme to generate self-supervised training data, which automatically removes high-frequency details while preserving global structure, effectively simulating real-world SDG errors. We further propose a keypoint matching-based metric to properly assess fidelity in details beyond semantic similarities usually measured by CLIP or DINO. Experimental results demonstrate that FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFlowFixeræ˜¯ä¸€ç§ç»†èŠ‚ä¿ç•™çš„ä¸»é¢˜é©±åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ¢å¤åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å› ä¸»é¢˜å°ºåº¦å’Œè§†è§’å˜åŒ–è€Œä¸¢å¤±çš„ç»†èŠ‚ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€ä¸»é¢˜é©±åŠ¨ç”ŸæˆæŠ€æœ¯çš„å‘å±•ï¼Œå¦‚ä½•æœ‰æ•ˆæ¢å¤ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸¢å¤±çš„ç»†èŠ‚æˆä¸ºé‡è¦ç ”ç©¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šFlowFixeré‡‡ç”¨ç›´æ¥çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œç»“åˆä¸€æ­¥å»å™ªæ–¹æ¡ˆç”Ÿæˆè‡ªç›‘ç£è®­ç»ƒæ•°æ®ï¼Œå¹¶å¼•å…¥åŸºäºå…³é”®ç‚¹åŒ¹é…çš„åº¦é‡æ ‡å‡†ä»¥è¯„ä¼°ç»†èŠ‚ä¿çœŸåº¦ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒFlowFixeråœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­è¶…è¶Šäº†ç°æœ‰çš„ä¸»é¢˜é©±åŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºé«˜ä¿çœŸç”Ÿæˆè®¾å®šäº†æ–°åŸºå‡†ã€‚

**å…³é”®è¯**ï¼šä¸»ä½“é©±åŠ¨ç”Ÿæˆ, ç»†èŠ‚ä¿çœŸç”Ÿæˆ, å›¾åƒåˆ°å›¾åƒç¿»è¯‘, è§†è§‰å‚è€ƒæ¡ä»¶ç”Ÿæˆ, å°ºåº¦ä¸è§†è§’å˜åŒ–é²æ£’æ€§, é«˜é¢‘ç»†èŠ‚æ¢å¤, è‡ªç›‘ç£è®­ç»ƒæ•°æ®ç”Ÿæˆ, ä¸€æ­¥å»å™ªæ–¹æ¡ˆ, é€€åŒ–æ¨¡æ‹Ÿ, å…³é”®ç‚¹åŒ¹é…è¯„æµ‹, ç»†èŠ‚ä¿çœŸåº¦è¯„ä¼°

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21402v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21402v1.pdf)

---

## cs.LG

## [8. MINAR: Mechanistic Interpretability for Neural Algorithmic Reasoning](https://arxiv.org/abs/2602.21442v1)

**ä½œè€…**ï¼šJesse He, Helen Jenne, Max Vargas ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The recent field of neural algorithmic reasoning (NAR) studies the ability of graph neural networks (GNNs) to emulate classical algorithms like Bellman-Ford, a phenomenon known as algorithmic alignment. At the same time, recent advances in large language models (LLMs) have spawned the study of mechanistic interpretability, which aims to identify granular model components like circuits that perform specific computations. In this work, we introduce Mechanistic Interpretability for Neural Algorithmic Reasoning (MINAR), an efficient circuit discovery toolbox that adapts attribution patching methods from mechanistic interpretability to the GNN setting. We show through two case studies that MINAR recovers faithful neuron-level circuits from GNNs trained on algorithmic tasks. Our study sheds new light on the process of circuit formation and pruning during training, as well as giving new insight into how GNNs trained to perform multiple tasks in parallel reuse circuit components for related tasks. Our code is available at https://github.com/pnnl/MINAR.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMINAR å°†æœºåˆ¶å¯è§£é‡Šæ€§çš„å½’å› /patching ç”µè·¯å‘ç°æ–¹æ³•è¿ç§»åˆ°GNNçš„ç¥ç»ç®—æ³•æ¨ç†ä»»åŠ¡ä¸Šï¼Œèƒ½åœ¨ç¥ç»å…ƒçº§åˆ«æ‰¾å‡ºä¸ç»å…¸ç®—æ³•å¯¹é½çš„å¯éªŒè¯ç”µè·¯ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šNAR å…³æ³¨GNNä¸ºä½•èƒ½â€œå­¦ä¼šâ€æ‰§è¡Œå¦‚ Bellman-Ford ç­‰ç®—æ³•ï¼Œä½†ç¼ºå°‘ç»†ç²’åº¦æœºåˆ¶è§£é‡Šï¼›æœºåˆ¶å¯è§£é‡Šæ€§åœ¨LLMä¸Šå·²å‘å±•å‡ºç”µè·¯åˆ†æå·¥å…·ï¼Œä½œè€…å¸Œæœ›å°†å…¶å¼•å…¥GNNä»¥æ­ç¤ºç®—æ³•å¯¹é½çš„å†…éƒ¨è®¡ç®—ç»“æ„ä¸è®­ç»ƒè¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º MINAR å·¥å…·ç®±ï¼Œæ”¹é€  attribution patching ç­‰æ–¹æ³•ä»¥é€‚é…å›¾ç»“æ„ä¸GNNè®¡ç®—æµç¨‹ï¼Œé€šè¿‡å¹²é¢„/æ›¿æ¢æ¿€æ´»å¹¶åº¦é‡è¾“å‡ºå½±å“æ¥é«˜æ•ˆå®šä½å¯¹ä»»åŠ¡å…³é”®çš„ç¥ç»å…ƒçº§ç”µè·¯ï¼›å¹¶ç”¨ä¸¤ä¸ªç®—æ³•ä»»åŠ¡æ¡ˆä¾‹ç ”ç©¶éªŒè¯ç”µè·¯å‘ç°çš„å¿ å®æ€§ä¸å¯è§£é‡Šæ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šMINAR èƒ½ä»è®­ç»ƒå¥½çš„GNNä¸­æ¢å¤å‡ºä¸ç®—æ³•æ‰§è¡Œç›¸å…³çš„å¿ å®ç”µè·¯ï¼Œå¹¶æ­ç¤ºç”µè·¯åœ¨è®­ç»ƒä¸­çš„å½¢æˆä¸å‰ªæè§„å¾‹ï¼›åœ¨å¤šä»»åŠ¡å¹¶è¡Œè®­ç»ƒæ—¶ï¼Œæ¨¡å‹ä¼šåœ¨ç›¸å…³ä»»åŠ¡é—´å¤ç”¨éƒ¨åˆ†ç”µè·¯ç»„ä»¶ï¼Œè¯´æ˜å­˜åœ¨å¯ç»„åˆçš„å…±äº«è®¡ç®—å­ç»“æ„ã€‚

**å…³é”®è¯**ï¼šç¥ç»ç®—æ³•æ¨ç†, å›¾ç¥ç»ç½‘ç»œ, ç®—æ³•å¯¹é½, æœºåˆ¶å¯è§£é‡Šæ€§, å½’å› è¡¥ä¸, ç¥ç»å…ƒçº§ç”µè·¯, å¤šä»»åŠ¡å­¦ä¹ , è®­ç»ƒå‰ªæ

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21442v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21442v1.pdf)

---

## [9. Provably Safe Generative Sampling with Constricting Barrier Functions](https://arxiv.org/abs/2602.21429v1)

**ä½œè€…**ï¼šDarshan Gadginmath, Ahmed Allibhoy, Fabio Pasqualetti  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, eess.SY, math.OC  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Flow-based generative models, such as diffusion models and flow matching models, have achieved remarkable success in learning complex data distributions. However, a critical gap remains for their deployment in safety-critical domains: the lack of formal guarantees that generated samples will satisfy hard constraints. We address this by proposing a safety filtering framework that acts as an online shield for any pre-trained generative model. Our key insight is to cooperate with the generative process rather than override it. We define a constricting safety tube that is relaxed at the initial noise distribution and progressively tightens to the target safe set at the final data distribution, mirroring the coarse-to-fine structure of the generative process itself. By characterizing this tube via Control Barrier Functions (CBFs), we synthesize a feedback control input through a convex Quadratic Program (QP) at each sampling step. As the tube is loosest when noise is high and intervention is cheapest in terms of control energy, most constraint enforcement occurs when it least disrupts the model's learned structure. We prove that this mechanism guarantees safe sampling while minimizing the distributional shift from the original model at each sampling step, as quantified by the KL divergence. Our framework applies to any pre-trained flow-based generative scheme requiring no retraining or architectural modifications. We validate the approach across constrained image generation, physically-consistent trajectory sampling, and safe robotic manipulation policies, achieving 100% constraint satisfaction while preserving semantic fidelity.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§æ— éœ€é‡è®­çš„â€œå®‰å…¨è¿‡æ»¤/æŠ¤ç›¾â€æ¡†æ¶ï¼Œç”¨æ§åˆ¶å±éšœå‡½æ•°åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­é€æ­¥æ”¶ç´§çº¦æŸï¼Œè¯æ˜å¯åœ¨æœ€å°åˆ†å¸ƒåç§»ä¸‹å®ç°å¯è¯æ˜å®‰å…¨çš„ç”Ÿæˆé‡‡æ ·ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ‰©æ•£/æµå¼ç”Ÿæˆæ¨¡å‹è™½å¼ºï¼Œä½†åœ¨å®‰å…¨å…³é”®åœºæ™¯ç¼ºä¹ç”Ÿæˆæ ·æœ¬æ»¡è¶³ç¡¬çº¦æŸçš„å½¢å¼åŒ–ä¿è¯ï¼›ç°æœ‰åšæ³•å¾€å¾€å¼ºè¡Œæ”¹åŠ¨æ¨¡å‹æˆ–ä»£ä»·é«˜ã€ç ´åè¯­ä¹‰ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„é€ ä»åˆå§‹å™ªå£°åˆ†å¸ƒåˆ°æœ€ç»ˆå®‰å…¨é›†é€æ­¥æ”¶ç´§çš„â€œå®‰å…¨ç®¡é“ï¼ˆtubeï¼‰â€ï¼Œç”¨æ§åˆ¶å±éšœå‡½æ•°ï¼ˆCBFï¼‰åˆ»ç”»ï¼Œå¹¶åœ¨æ¯ä¸ªé‡‡æ ·æ­¥é€šè¿‡å‡¸äºŒæ¬¡è§„åˆ’ï¼ˆQPï¼‰æ±‚è§£æœ€å°æ§åˆ¶èƒ½é‡çš„åé¦ˆä¿®æ­£ï¼Œä½¿çº¦æŸä¸»è¦åœ¨æ—©æœŸé«˜å™ªå£°é˜¶æ®µè¢«ä½å¹²æ‰°åœ°æ–½åŠ ï¼ŒåŒæ—¶ç»™å‡ºé€æ­¥æœ€å°åŒ–KLåç§»çš„ç†è®ºä¿è¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šç†è®ºä¸Šè¯æ˜è¯¥åœ¨çº¿æŠ¤ç›¾èƒ½ä¿è¯é‡‡æ ·å…¨ç¨‹å®‰å…¨å¹¶åœ¨æ¯æ­¥å°½é‡è´´è¿‘åŸç”Ÿæˆåˆ†å¸ƒï¼›å®éªŒåœ¨å—çº¦æŸå›¾åƒç”Ÿæˆã€ç‰©ç†ä¸€è‡´è½¨è¿¹ä¸å®‰å…¨æœºå™¨äººæ“ä½œä¸­å®ç°100%çº¦æŸæ»¡è¶³ä¸”ä¿æŒè¯­ä¹‰/è´¨é‡ã€‚

**å…³é”®è¯**ï¼šå®‰å…¨ç”Ÿæˆé‡‡æ ·, å®‰å…¨è¿‡æ»¤å™¨, çº¦æŸæ‰©æ•£æ¨¡å‹, æµåŒ¹é…æ¨¡å‹, æ§åˆ¶éšœç¢å‡½æ•°ï¼ˆCBFï¼‰, äºŒæ¬¡è§„åˆ’ï¼ˆQPï¼‰, ç¡¬çº¦æŸæ»¡è¶³, KLæ•£åº¦æœ€å°åŒ–, çº¦æŸå›¾åƒç”Ÿæˆ, å®‰å…¨æœºå™¨äººæ“æ§

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21429v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21429v1.pdf)

---

## [10. Proximal-IMH: Proximal Posterior Proposals for Independent Metropolis-Hastings with Approximate Operators](https://arxiv.org/abs/2602.21426v1)

**ä½œè€…**ï¼šYouguang Chen, George Biros  
**åˆ†ç±»**ï¼šcs.LG, stat.CO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We consider the problem of sampling from a posterior distribution arising in Bayesian inverse problems in science, engineering, and imaging. Our method belongs to the family of independence Metropolis-Hastings (IMH) sampling algorithms, which are common in Bayesian inference. Relying on the existence of an approximate posterior distribution that is cheaper to sample from but may have significant bias, we introduce Proximal-IMH, a scheme that removes this bias by correcting samples from the approximate posterior through an auxiliary optimization problem. This yields a local adjustment that trades off adherence to the exact model against stability around the approximate reference point. For idealized settings, we prove that the proximal correction tightens the match between approximate and exact posteriors, thereby improving acceptance rates and mixing. The method applies to both linear and nonlinear input-output operators and is particularly suitable for inverse problems where exact posterior sampling is too expensive. We present numerical experiments including multimodal and data-driven priors with nonlinear input-output operators. The results show that Proximal-IMH reliably outperforms existing IMH variants.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šProximal-IMH é€šè¿‡å¯¹å»‰ä»·çš„è¿‘ä¼¼åéªŒæ ·æœ¬åšä¸€æ¬¡â€œè¿‘ç«¯ä¼˜åŒ–æ ¡æ­£â€ï¼Œåœ¨ä¿æŒç‹¬ç«‹MHæ¡†æ¶çš„åŒæ—¶æ˜¾è‘—é™ä½è¿‘ä¼¼åå·®å¹¶æå‡é‡‡æ ·æ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè´å¶æ–¯åé—®é¢˜ä¸­çš„ç²¾ç¡®åéªŒé‡‡æ ·ä»£ä»·é«˜ï¼Œè€Œå¯å¿«é€Ÿé‡‡æ ·çš„è¿‘ä¼¼åéªŒå¾€å¾€åå·®å¤§ã€å¯¼è‡´IMHæ¥å—ç‡å’Œæ··åˆæ€§å˜å·®ï¼›éœ€è¦ä¸€ç§æ—¢åˆ©ç”¨è¿‘ä¼¼åéªŒçš„æ•ˆç‡åˆèƒ½çº åçš„é€šç”¨æœºåˆ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥è¿‘ä¼¼åéªŒä½œä¸ºç‹¬ç«‹æè®®åˆ†å¸ƒï¼Œå¹¶å¯¹æ¯ä¸ªæè®®æ ·æœ¬æ±‚è§£ä¸€ä¸ªè¾…åŠ©ä¼˜åŒ–ï¼ˆè¿‘ç«¯/Proximalæ ¡æ­£ï¼‰æ¥åšå±€éƒ¨è°ƒæ•´ï¼Œåœ¨â€œè´´è¿‘ç²¾ç¡®æ¨¡å‹â€ä¸â€œå›´ç»•è¿‘ä¼¼å‚è€ƒç‚¹ç¨³å®šâ€ä¹‹é—´æƒè¡¡ï¼›ç†è®ºä¸Šè¯æ˜è¯¥æ ¡æ­£å¯æ‹‰è¿‘è¿‘ä¼¼ä¸ç²¾ç¡®åéªŒã€æé«˜æ¥å—ç‡ä¸æ··åˆï¼Œå¹¶é€‚ç”¨äºçº¿æ€§/éçº¿æ€§ç®—å­ä¸å¤šå³°ã€æ•°æ®é©±åŠ¨å…ˆéªŒç­‰åœºæ™¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ç†æƒ³åŒ–è®¾å®šä¸‹ï¼Œè¿‘ç«¯æ ¡æ­£èƒ½ç³»ç»Ÿæ€§æ”¹å–„æè®®è´¨é‡å¹¶æå‡IMHçš„æ¥å—ç‡å’Œæ··åˆæ€§ï¼›æ•°å€¼å®éªŒè¡¨æ˜åœ¨å¤šç§åé—®é¢˜è®¾ç½®ä¸­ Proximal-IMH ç›¸æ¯”ç°æœ‰IMHå˜ä½“æ›´ç¨³å¥ã€æ€§èƒ½æ›´ä¼˜ã€‚

**å…³é”®è¯**ï¼šè´å¶æ–¯é€†é—®é¢˜, åéªŒé‡‡æ ·, è¿‘ä¼¼åéªŒæè®®åˆ†å¸ƒ, åå·®æ ¡æ­£, è¾…åŠ©ä¼˜åŒ–é—®é¢˜, é©¬å°”å¯å¤«é“¾æ··åˆ, æ¥å—ç‡æå‡, éçº¿æ€§å‰å‘ç®—å­, å¤šå³°åéªŒ, æ•°æ®é©±åŠ¨å…ˆéªŒ

**è¯„åˆ†**ï¼š19

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21426v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21426v1.pdf)

---

## [11. On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation](https://arxiv.org/abs/2602.21424v1)

**ä½œè€…**ï¼šAlexander Galozy  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect to internal information under fixed observations. This induces a probe-relative notion of $Îµ$-behavioural equivalence and a within-policy behavioural distance that quantifies probe sensitivity. We establish three structural results. First, the set of policies exhibiting non-trivial behavioural dependency is not closed under convex aggregation. Second, behavioural distance contracts under convex combination. Third, we prove a sufficient local condition under which gradient ascent on a skewed mixture objective decreases behavioural distance when a dominant-mode gradient aligns with the direction of steepest contraction. Minimal bandit and partially observable gridworld experiments provide controlled witnesses of these mechanisms. In the examined settings, behavioural distance decreases under convex aggregation and under continued optimisation with skewed latent priors, and in these experiments it precedes degradation under latent prior shift. These results identify structural conditions under which probe-conditioned behavioural separation is not preserved under common policy transformations.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºâ€œè¡Œä¸ºä¾èµ–æ€§/è¡Œä¸ºè·ç¦»â€æ¥åˆ»ç”»éƒ¨åˆ†å¯è§‚æµ‹RLä¸­ç­–ç•¥å¯¹å†…éƒ¨ä¿¡æ¯ï¼ˆè®°å¿†/æ½œå˜é‡æ¨æ–­ï¼‰çš„ä¾èµ–ï¼Œå¹¶è¯æ˜è¿™ç§â€œè®¤çŸ¥å¼è¡Œä¸ºåˆ†ç¦»â€åœ¨å¸¸è§çš„ç­–ç•¥å˜æ¢ï¼ˆå¦‚å‡¸ç»„åˆä¸ç»§ç»­ä¼˜åŒ–ï¼‰ä¸‹å¾€å¾€ä¸è¢«ç»“æ„æ€§ä¿ç•™ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒé‡Œï¼Œæ™ºèƒ½ä½“å¾€å¾€ä¾èµ–å†…éƒ¨çŠ¶æ€è€Œéä»…ä¾èµ–è§‚æµ‹ï¼Œä½†è¿™ç§ä¾èµ–åœ¨ç­–ç•¥æ··åˆã€é›†æˆæˆ–è®­ç»ƒç›®æ ‡æ”¹å˜ï¼ˆä¾‹å¦‚æ½œå˜é‡å…ˆéªŒåç½®ï¼‰åæ˜¯å¦è¿˜èƒ½ç»´æŒç¼ºä¹æ¸…æ™°ç†è®ºåˆ»ç”»ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†â€œå›ºå®šè§‚æµ‹ä¸‹åŠ¨ä½œéšå†…éƒ¨ä¿¡æ¯å˜åŒ–â€çš„ç°è±¡å½¢å¼åŒ–ä¸ºè¡Œä¸ºä¾èµ–æ€§ï¼Œå¹¶å®šä¹‰æ¢é’ˆï¼ˆprobeï¼‰ç›¸å¯¹çš„Îµ-è¡Œä¸ºç­‰ä»·ä¸ç­–ç•¥å†…è¡Œä¸ºè·ç¦»ï¼Œç”¨äºé‡åŒ–ç­–ç•¥å¯¹æ¢é’ˆ/å†…éƒ¨ä¿¡æ¯çš„æ•æ„Ÿæ€§ï¼›éšåç»™å‡ºä¸‰ä¸ªç»“æ„æ€§å®šç†å¹¶ç”¨æœ€å°banditä¸éƒ¨åˆ†å¯è§‚æµ‹gridworldå®éªŒä½œæœºåˆ¶è§è¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»“æœè¡¨æ˜ï¼šéå¹³å‡¡è¡Œä¸ºä¾èµ–æ€§çš„ç­–ç•¥é›†åˆå¯¹å‡¸èšåˆä¸å°é—­ï¼Œä¸”è¡Œä¸ºè·ç¦»åœ¨å‡¸ç»„åˆä¸‹æ”¶ç¼©ï¼›åœ¨å¸¦åç½®çš„æ··åˆç›®æ ‡åšæ¢¯åº¦ä¸Šå‡æ—¶ï¼Œæ»¡è¶³å±€éƒ¨å¯¹é½æ¡ä»¶ä¼šè¿›ä¸€æ­¥é™ä½è¡Œä¸ºè·ç¦»ï¼Œå®éªŒä¸­è¿™ç§è·ç¦»ä¸‹é™å¸¸å…ˆäºæ½œåœ¨å…ˆéªŒæ¼‚ç§»ä¸‹çš„æ€§èƒ½é€€åŒ–ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , è¡Œä¸ºä¾èµ–, éƒ¨åˆ†å¯è§‚æµ‹æ€§, è¡Œä¸ºè·ç¦», ç­–ç•¥å˜æ¢, å‡¸èšåˆ, æ¢¯åº¦ä¸Šå‡, æ½œåœ¨å…ˆéªŒ

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21424v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21424v1.pdf)

---

## [12. Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning](https://arxiv.org/abs/2602.21420v1)

**ä½œè€…**ï¼šYuanda Xu, Hejian Sang, Zhengze Zhou ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸å¯¹ç§°ç½®ä¿¡ç½šï¼ˆACEï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€è°ƒèŠ‚é”™è¯¯æƒ©ç½šæ¥æ”¹è¿›å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯è¿›è¡Œæ›´å¼ºçš„ä¿®æ­£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ç®—æ³•åœ¨æ”¹å–„å‡†ç¡®æ€§å’Œæ¨ç†è¾¹ç•Œæ—¶ï¼Œå¿½è§†äº†å¯¹é”™è¯¯çš„å‡åŒ€æƒ©ç½šï¼Œå¯¼è‡´è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯æŒç»­å­˜åœ¨å¹¶æŠ‘åˆ¶äº†æ¢ç´¢å¤šæ ·æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šACEæ–¹æ³•å¼•å…¥äº†ä¸€ç§æŒ‰å›æŠ¥åŠ¨æ€è°ƒæ•´çš„ç½®ä¿¡åº¦åç§»åº¦é‡ï¼Œç»“åˆé€‰æ‹©æ€§æ­£åˆ™åŒ–æ¥æœ‰é’ˆå¯¹æ€§åœ°ä¿®æ­£è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šé€šè¿‡åœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå®éªŒï¼ŒACEæ–¹æ³•åœ¨æå‡å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ”¹å–„äº†ç°æœ‰æ–¹æ³•åœ¨å…¨èŒƒå›´å†…çš„è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ , LLMæ¨ç†å¼ºåŒ–å­¦ä¹ , è¿‡åº¦è‡ªä¿¡é”™è¯¯, éå¯¹ç§°é”™è¯¯æƒ©ç½š, ç½®ä¿¡åº¦åç§», è´Ÿä¼˜åŠ¿åŠ æƒ, é€‰æ‹©æ€§æ­£åˆ™åŒ–, ç­–ç•¥å‚è€ƒæ¯”ç‡, ç”Ÿæˆå¤šæ ·æ€§

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21420v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21420v1.pdf)

---

## [13. Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting](https://arxiv.org/abs/2602.21415v1)

**ä½œè€…**ï¼šSunki Hong, Jisoo Lee, Yuanyuan Shi  
**åˆ†ç±»**ï¼šcs.LG, eess.SY  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Selecting the right deep learning model for power grid forecasting is challenging, as performance heavily depends on the data available to the operator. This paper presents a comprehensive benchmark of five modern neural architectures: two state space models (PowerMamba, S-Mamba), two Transformers (iTransformer, PatchTST), and a traditional LSTM. We evaluate these models on hourly electricity demand across six diverse US power grids for forecast windows between 24 and 168 hours. To ensure a fair comparison, we adapt each model with specialized temporal processing and a modular layer that cleanly integrates weather covariates. Our results reveal that there is no single best model for all situations. When forecasting using only historical load, PatchTST and the state space models provide the highest accuracy. However, when explicit weather data is added to the inputs, the rankings reverse: iTransformer improves its accuracy three times more efficiently than PatchTST. By controlling for model size, we confirm that this advantage stems from the architecture's inherent ability to mix information across different variables. Extending our evaluation to solar generation, wind power, and wholesale prices further demonstrates that model rankings depend on the forecast task: PatchTST excels on highly rhythmic signals like solar, while state space models are better suited for the chaotic fluctuations of wind and price. Ultimately, this benchmark provides grid operators with actionable guidelines for selecting the optimal forecasting architecture based on their specific data environments.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç³»ç»ŸåŸºå‡†å¯¹æ¯”äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ã€Transformerä¸LSTMåœ¨ç¾å›½å¤šç”µç½‘è´Ÿè·åŠå¤šç§èƒ½æº/ä»·æ ¼é¢„æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°æœ€ä¼˜æ¶æ„å¼ºä¾èµ–è¾“å…¥ç‰¹å¾ï¼ˆæ˜¯å¦å«å¤©æ°”ï¼‰ä¸ä»»åŠ¡ä¿¡å·ç‰¹æ€§ï¼ˆèŠ‚å¾‹æ€§ vs æ··æ²Œæ€§ï¼‰ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç”µç½‘é¢„æµ‹ä¸­æ¨¡å‹æ•ˆæœé«˜åº¦ä¾èµ–è¿è¥æ–¹å¯ç”¨æ•°æ®ï¼ˆä»…å†å²è´Ÿè·æˆ–å«å¤©æ°”ç­‰åå˜é‡ï¼‰ï¼Œå¯¼è‡´â€œé€‰å“ªç§æ·±åº¦æ¨¡å‹â€ç¼ºä¹é€šç”¨ç»“è®ºä¸å¯æ“ä½œæŒ‡å—ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨6ä¸ªç¾å›½ç”µç½‘çš„å°æ—¶çº§è´Ÿè·æ•°æ®ä¸Šï¼Œé’ˆå¯¹24â€“168å°æ—¶é¢„æµ‹çª—å£ï¼Œå…¬å¹³æ¯”è¾ƒPowerMambaã€S-Mambaã€iTransformerã€PatchTSTä¸LSTMï¼Œå¹¶ä¸ºå„æ¨¡å‹ç»Ÿä¸€åŠ å…¥ä¸“é—¨çš„æ—¶é—´å¤„ç†ä¸å¯æ¨¡å—åŒ–èåˆå¤©æ°”åå˜é‡çš„å±‚ï¼›åŒæ—¶æ§åˆ¶æ¨¡å‹è§„æ¨¡å¹¶æ‰©å±•åˆ°å…‰ä¼ã€é£ç”µä¸æ‰¹å‘ç”µä»·ä»»åŠ¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šä»…ç”¨å†å²è´Ÿè·æ—¶ï¼ŒPatchTSTä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ç²¾åº¦æœ€é«˜ï¼›åŠ å…¥æ˜¾å¼å¤©æ°”åï¼ŒiTransformerçš„æ”¹è¿›æ•ˆç‡æ˜¾è‘—é«˜äºPatchTSTï¼ˆçº¦3å€ï¼‰ï¼Œä¼˜åŠ¿æ¥è‡ªå…¶è·¨å˜é‡ä¿¡æ¯æ··åˆèƒ½åŠ›è€Œéæ¨¡å‹å¤§å°ã€‚ä¸åŒä»»åŠ¡æ’åä¸åŒï¼šPatchTSTæ›´é€‚åˆå¼ºèŠ‚å¾‹ä¿¡å·ï¼ˆå¦‚å…‰ä¼ï¼‰ï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹æ›´é€‚åˆæ³¢åŠ¨æ›´æ··æ²Œçš„é£ç”µä¸ç”µä»·ã€‚

**å…³é”®è¯**ï¼šç”µç½‘è´Ÿè·é¢„æµ‹, ç”µåŠ›æ—¶é—´åºåˆ—é¢„æµ‹, å¤šå˜é‡æ—¶é—´åºåˆ—, å¤–ç”Ÿå˜é‡èåˆ, æ°”è±¡åå˜é‡, æ¨¡å‹åŸºå‡†è¯„æµ‹, çŠ¶æ€ç©ºé—´æ¨¡å‹, å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆLSTMï¼‰, å¯å†ç”Ÿèƒ½æºé¢„æµ‹, ç”µåŠ›ç°è´§ä»·æ ¼é¢„æµ‹, å¤šæ­¥é¢„æµ‹ï¼ˆ24-168hï¼‰

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21415v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21415v1.pdf)

---

## [14. Generative Bayesian Computation as a Scalable Alternative to Gaussian Process Surrogates](https://arxiv.org/abs/2602.21408v1)

**ä½œè€…**ï¼šNick Polson, Vadim Sokolov  
**åˆ†ç±»**ï¼šcs.LG, stat.AP, stat.CO, stat.ME, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Gaussian process (GP) surrogates are the default tool for emulating expensive computer experiments, but cubic cost, stationarity assumptions, and Gaussian predictive distributions limit their reach. We propose Generative Bayesian Computation (GBC) via Implicit Quantile Networks (IQNs) as a surrogate framework that targets all three limitations. GBC learns the full conditional quantile function from input--output pairs; at test time, a single forward pass per quantile level produces draws from the predictive distribution.   Across fourteen benchmarks we compare GBC to four GP-based methods. GBC improves CRPS by 11--26\% on piecewise jump-process benchmarks, by 14\% on a ten-dimensional Friedman function, and scales linearly to 90,000 training points where dense-covariance GPs are infeasible. A boundary-augmented variant matches or outperforms Modular Jump GPs on two-dimensional jump datasets (up to 46\% CRPS improvement). In active learning, a randomized-prior IQN ensemble achieves nearly three times lower RMSE than deep GP active learning on Rocket LGBB. Overall, GBC records a favorable point estimate in 12 of 14 comparisons. GPs retain an edge on smooth surfaces where their smoothness prior provides effective regularization.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºGenerative Bayesian Computationï¼ˆGBCï¼‰åŸºäºImplicit Quantile Networksï¼ˆIQNï¼‰ä½œä¸ºæ›¿ä»£é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰çš„å¯æ‰©å±•ä»£ç†æ¨¡å‹ï¼Œåœ¨éå¹³ç¨³/è·³å˜ä»»åŠ¡ä¸å¤§æ•°æ®è§„æ¨¡ä¸Šå–å¾—æ›´å¥½é¢„æµ‹åˆ†å¸ƒè´¨é‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šGPä»£ç†è™½å¸¸ç”¨ä½†å­˜åœ¨è®­ç»ƒç«‹æ–¹å¤æ‚åº¦ã€å¹³ç¨³æ€§ç­‰å…ˆéªŒå‡è®¾é™åˆ¶ï¼Œä»¥åŠé¢„æµ‹åˆ†å¸ƒå—é«˜æ–¯å½¢çŠ¶çº¦æŸï¼Œéš¾ä»¥å¤„ç†è·³å˜ç­‰å¤æ‚è¾“å‡ºå¹¶æ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®ã€‚ä½œè€…å¸Œæœ›ç”¨èƒ½ç›´æ¥å­¦ä¹ æ›´çµæ´»é¢„æµ‹åˆ†å¸ƒä¸”è®¡ç®—å¯çº¿æ€§æ‰©å±•çš„æ–¹æ³•æ›¿ä»£GPã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šGBCç”¨IQNä»è¾“å…¥-è¾“å‡ºå¯¹å­¦ä¹ æ¡ä»¶åˆ†ä½æ•°å‡½æ•°ï¼ˆconditional quantile functionï¼‰ï¼Œæµ‹è¯•æ—¶å¯¹ä¸åŒåˆ†ä½æ•°æ°´å¹³ä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯é‡‡æ ·å¾—åˆ°é¢„æµ‹åˆ†å¸ƒï¼›å¹¶æå‡ºè¾¹ç•Œå¢å¼ºå˜ä½“ä¸éšæœºå…ˆéªŒIQNé›†æˆç”¨äºä¸»åŠ¨å­¦ä¹ æå‡ä¸ç¡®å®šæ€§ä¸é‡‡æ ·æ•ˆç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨14ä¸ªåŸºå‡†ä¸Šï¼ŒGBCåœ¨å¤šæ•°ä»»åŠ¡ï¼ˆ12/14ï¼‰ç»™å‡ºæ›´ä¼˜ç‚¹ä¼°è®¡æˆ–åˆ†å¸ƒè¯„åˆ†ï¼Œå°¤å…¶åœ¨è·³å˜/åˆ†æ®µè¿‡ç¨‹ä¸é«˜ç»´å‡½æ•°ä¸ŠCRPSæ˜¾è‘—æå‡ï¼Œå¹¶å¯çº¿æ€§æ‰©å±•åˆ°9ä¸‡è®­ç»ƒç‚¹ï¼›ä½†åœ¨å…‰æ»‘æ›²é¢ä¸ŠGPçš„å¹³æ»‘å…ˆéªŒä»æä¾›æ›´å¼ºæ­£åˆ™åŒ–è€Œå ä¼˜ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆè´å¶æ–¯è®¡ç®—, éšå¼åˆ†ä½ç½‘ç»œ, é«˜æ–¯è¿‡ç¨‹, ä»£ç†æ¡†æ¶, æ¡ä»¶åˆ†ä½å‡½æ•°, ä¸»åŠ¨å­¦ä¹ , å‡æ–¹æ ¹è¯¯å·®, è®­ç»ƒç‚¹, æ¨¡å—åŒ–è·³è·ƒGP

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21408v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21408v1.pdf)

---

## [15. FedVG: Gradient-Guided Aggregation for Enhanced Federated Learning](https://arxiv.org/abs/2602.21399v1)

**ä½œè€…**ï¼šAlina Devkota, Jacob Thrasher, Donald Adjeroh ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Federated Learning (FL) enables collaborative model training across multiple clients without sharing their private data. However, data heterogeneity across clients leads to client drift, which degrades the overall generalization performance of the model. This effect is further compounded by overemphasis on poorly performing clients. To address this problem, we propose FedVG, a novel gradient-based federated aggregation framework that leverages a global validation set to guide the optimization process. Such a global validation set can be established using readily available public datasets, ensuring accessibility and consistency across clients without compromising privacy. In contrast to conventional approaches that prioritize client dataset volume, FedVG assesses the generalization ability of client models by measuring the magnitude of validation gradients across layers. Specifically, we compute layerwise gradient norms to derive a client-specific score that reflects how much each client needs to adjust for improved generalization on the global validation set, thereby enabling more informed and adaptive federated aggregation. Extensive experiments on both natural and medical image benchmarking datasets, across diverse model architectures, demonstrate that FedVG consistently improves performance, particularly in highly heterogeneous settings. Moreover, FedVG is modular and can be seamlessly integrated with various state-of-the-art FL algorithms, often further improving their results. Our code is available at https://github.com/alinadevkota/FedVG.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFedVGåˆ©ç”¨å…¨å±€éªŒè¯é›†çš„åˆ†å±‚éªŒè¯æ¢¯åº¦èŒƒæ•°ä¸ºå®¢æˆ·ç«¯æ‰“åˆ†å¹¶è‡ªé€‚åº”èšåˆï¼Œä»è€Œåœ¨æ•°æ®é«˜åº¦å¼‚è´¨çš„è”é‚¦å­¦ä¹ ä¸­æå‡æ³›åŒ–æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè”é‚¦å­¦ä¹ ä¸­å®¢æˆ·ç«¯æ•°æ®å¼‚è´¨æ€§ä¼šå¯¼è‡´client driftå¹¶æŸå®³å…¨å±€æ¨¡å‹æ³›åŒ–ï¼Œä¸”æŒ‰æ•°æ®é‡åŠ æƒå¯èƒ½è¿‡åº¦å¼ºè°ƒè¡¨ç°å·®çš„å®¢æˆ·ç«¯ã€‚éœ€è¦ä¸€ç§èƒ½ç›´æ¥åæ˜ â€œå¯¹æ³›åŒ–æœ‰å¸®åŠ©ç¨‹åº¦â€çš„èšåˆä¾æ®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¼•å…¥å¯ç”±å…¬å…±æ•°æ®æ„å»ºçš„å…¨å±€éªŒè¯é›†ï¼Œç”¨å…¶è®¡ç®—å„å®¢æˆ·ç«¯æ¨¡å‹åœ¨å„å±‚çš„éªŒè¯æ¢¯åº¦èŒƒæ•°ï¼Œå¹¶æ®æ­¤å½¢æˆå®¢æˆ·ç«¯å¾—åˆ†ï¼ˆè¡¨ç¤ºä¸ºæå‡å…¨å±€éªŒè¯æ³›åŒ–éœ€è°ƒæ•´çš„å¹…åº¦ï¼‰ã€‚èšåˆæ—¶ä¸å†ä»…æŒ‰æ ·æœ¬é‡ï¼Œè€Œæ˜¯ä¾æ®è¯¥å¾—åˆ†è¿›è¡Œæ›´æœ‰ä¿¡æ¯çš„åŠ æƒ/é€‰æ‹©ï¼Œå¹¶å¯æ¨¡å—åŒ–æ¥å…¥å¤šç§ç°æœ‰FLç®—æ³•ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨è‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒç­‰å¤šæ•°æ®é›†ã€å¤šæ¶æ„å®éªŒä¸­ï¼ŒFedVGåœ¨å¼‚è´¨æ€§æ›´å¼ºçš„åœºæ™¯ä¸‹ç¨³å®šæå‡æ€§èƒ½ï¼›åŒæ—¶ä½œä¸ºé€šç”¨èšåˆæ¨¡å—ï¼Œå¸¸èƒ½è¿›ä¸€æ­¥å¢å¼ºç°æœ‰è”é‚¦å­¦ä¹ æ–¹æ³•çš„æ•ˆæœã€‚

**å…³é”®è¯**ï¼šè”é‚¦å­¦ä¹ , å®¢æˆ·ç«¯æ¼‚ç§», æ¢¯åº¦å¼•å¯¼èšåˆ, éªŒè¯æ¢¯åº¦, å…¨å±€éªŒè¯é›†, å…¬å…±æ•°æ®é›†, å±‚çº§æ¢¯åº¦èŒƒæ•°, å®¢æˆ·ç«¯åŠ æƒ, è‡ªé€‚åº”èšåˆ, è·¨å®¢æˆ·ç«¯æ³›åŒ–, åŒ»å­¦å½±åƒè”é‚¦å­¦ä¹ 

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21399v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21399v1.pdf)

---

