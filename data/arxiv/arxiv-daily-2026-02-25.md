# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-25

> å…± 2 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (2 ç¯‡)

---

## cs.CV

## [1. Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences](https://arxiv.org/abs/2602.22212v1)

**ä½œè€…**ï¼šJulian Kaltheuner, Hannah DrÃ¶ge, Markus Plack ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šNeu-PiG é€šè¿‡â€œæ³•çº¿æ¡ä»¶åŒ–â€çš„å¤šåˆ†è¾¨ç‡æ½œå˜é‡ç½‘æ ¼ç¼–ç ä¸ Sobolev é¢„æ¡ä»¶ä¼˜åŒ–ï¼Œå®ç°é•¿åºåˆ—åŠ¨æ€ç‰©ä½“çš„å¿«é€Ÿã€æ— æ¼‚ç§»é«˜è´¨é‡è¡¨é¢é‡å»ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šé•¿åºåˆ—åŠ¨æ€ç‚¹äº‘çš„æ—¶åºä¸€è‡´é‡å»ºéš¾ç‚¹åœ¨äºï¼šå¢é‡å½¢å˜ä¼˜åŒ–æ˜“æ¼‚ç§»ä¸”è€—æ—¶ï¼Œè€Œä¾èµ–å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•å¸¸éœ€ç±»åˆ«ç‰¹å®šè®­ç»ƒã€ç»“æ„å¤æ‚ã€‚ä½œè€…å¸Œæœ›åœ¨æ— éœ€æ˜¾å¼å¯¹åº”ä¸å¼ºå…ˆéªŒçš„å‰æä¸‹ï¼Œå®ç°ç§’çº§ã€å¯æ‰©å±•çš„é«˜ä¿çœŸé‡å»ºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥å•å¸§å…³é”®å¸§è¡¨é¢ä¸ºå‚è€ƒï¼Œå°†ç©ºé—´ç‰¹å¾æŒ‰â€œä½ç½®+æ³•çº¿æ–¹å‘â€å‚æ•°åŒ–å¹¶ç¼–ç è¿›è·¨å°ºåº¦å¤šåˆ†è¾¨ç‡æ½œå˜é‡ç½‘æ ¼ï¼Œç»“åˆæ—¶é—´è°ƒåˆ¶åç”±è½»é‡ MLP è§£ç ä¸ºæ¯å¸§ 6-DoF å½¢å˜ã€‚è®­ç»ƒæ—¶åœ¨æ½œç©ºé—´ä½¿ç”¨ Sobolev é¢„æ¡ä»¶çš„æ¢¯åº¦ä¼˜åŒ–ä»¥åŠ é€Ÿæ”¶æ•›å¹¶æŠ‘åˆ¶æ¼‚ç§»ï¼Œå…¨ç¨‹ä¸ä¾èµ–æ˜¾å¼å¯¹åº”å…³ç³»ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç±»äºº/åŠ¨ç‰©æ•°æ®ä¸Šï¼ŒNeu-PiG åœ¨ç²¾åº¦ä¸é•¿åºåˆ—å¯æ‰©å±•æ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ç›¸è¾ƒè®­ç»ƒ-free æ–¹æ³•è‡³å°‘å¿« 60Ã—ï¼Œå¹¶è¾¾åˆ°æ¥è¿‘é‡å‹é¢„è®­ç»ƒæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚æ•´ä½“è¯æ˜äº†é¢„æ¡ä»¶åŒ–æ½œç½‘æ ¼è¡¨ç¤ºå¯åœ¨ä¸å¼•å…¥å¤æ‚å­¦ä¹ å…ˆéªŒçš„æƒ…å†µä¸‹å®ç°å¿«é€Ÿä¸”ç¨³å®šçš„åŠ¨æ€é‡å»ºã€‚

**å…³é”®è¯**ï¼šåŠ¨æ€è¡¨é¢é‡å»º, æ—¶åºä¸€è‡´æ€§é‡å»º, é•¿åºåˆ—ä¸‰ç»´é‡å»º, æ— ç»“æ„ç‚¹äº‘, å½¢å˜ä¼˜åŒ–, å¤šåˆ†è¾¨ç‡éšå¼ç½‘æ ¼ç¼–ç , æ½œç©ºé—´é¢„æ¡ä»¶åŒ–, å…³é”®å¸§å‚è€ƒè¡¨é¢, æ— å¯¹åº”å…³ç³»é‡å»º, 6-DoFå½¢å˜, è½»é‡çº§MLPè§£ç å™¨

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22212v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22212v1.pdf)

---

## [2. WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos](https://arxiv.org/abs/2602.22209v1)

**ä½œè€…**ï¼šYufei Ye, Jiaman Li, Ryan Rong ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šWHOLEé€šè¿‡å­¦ä¹ æ‰‹-ç‰©äº¤äº’çš„ç”Ÿæˆå¼å…ˆéªŒï¼Œåœ¨ç¬¬ä¸€è§†è§’è§†é¢‘ä¸­è”åˆé‡å»ºæ‰‹ä¸ç‰©ä½“åœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„è¿åŠ¨è½¨è¿¹ï¼Œå¹¶æ˜¾è‘—æå‡äº¤äº’ä¸€è‡´æ€§ä¸é®æŒ¡/å‡ºè§†é‡é²æ£’æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç¬¬ä¸€è§†è§’æ“ä½œè§†é¢‘ä¸­æ‰‹ä¸ç‰©ä½“å¼ºé®æŒ¡ä¸”ç‰©ä½“é¢‘ç¹å‡ºå…¥è§†é‡ï¼Œåˆ†åˆ«ä¼°è®¡æ‰‹æˆ–ç‰©ä½“å§¿æ€çš„æ–¹æ³•åœ¨äº¤äº’ä¸ç¼ºå¤±è§‚æµ‹æ—¶å®¹æ˜“å¤±è´¥ã€‚ç‹¬ç«‹é¢„æµ‹è¿˜ä¼šå¯¼è‡´æ‰‹-ç‰©å…³ç³»ä¸ä¸€è‡´ï¼Œéš¾ä»¥å¯é é‡å»ºçœŸå®äº¤äº’è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ–¹æ³•å­¦ä¹ ä¸€ä¸ªç”Ÿæˆå¼çš„æ‰‹-ç‰©è¿åŠ¨å…ˆéªŒï¼ˆè”åˆå»ºæ¨¡äº¤äº’è½¨è¿¹ï¼‰ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶ç”¨è§†é¢‘è§‚æµ‹å¯¹è¯¥å…ˆéªŒè¿›è¡Œå¼•å¯¼/çº¦æŸï¼Œä½¿ç”Ÿæˆçš„æ‰‹ä¸ç‰©ä½“è½¨è¿¹åŒæ—¶åŒ¹é…å›¾åƒè¯æ®ä¸ç‰©ç†äº¤äº’å…³ç³»ã€‚è¾“å…¥åŒ…å«ç‰©ä½“æ¨¡æ¿ï¼Œä»è€Œåœ¨ä¸–ç•Œç©ºé—´ä¸­åŒæ­¥æ¢å¤æ‰‹è¿åŠ¨ä¸ç‰©ä½“6Dä½å§¿åŠç›¸å¯¹å…³ç³»ã€‚

**ä¸»è¦ç»“è®º**ï¼šè”åˆç”Ÿæˆå¼é‡å»ºç›¸æ¯”â€œæ‰‹/ç‰©åˆ†åˆ«ä¼°è®¡+åå¤„ç†â€åœ¨é®æŒ¡ä¸å‡ºè§†é‡åœºæ™¯æ›´ç¨³å¥ï¼Œæ˜¾è‘—æå‡æ‰‹è¿åŠ¨ã€ç‰©ä½“6Dä½å§¿ä»¥åŠæ‰‹-ç‰©ç›¸å¯¹äº¤äº’é‡å»ºè´¨é‡ã€‚WHOLEåœ¨ç›¸å…³åŸºå‡†ä¸Šè¾¾åˆ°æˆ–åˆ·æ–°SOTAè¡¨ç°ã€‚

**å…³é”®è¯**ï¼šWHOLE, World-Grounded, Hand-Object, Lifted, Egocentric, Videos, manipulation, highly

**è¯„åˆ†**ï¼š0

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22209v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22209v1.pdf)

---

