# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-03

> å…± 30 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (11 ç¯‡)
- [cs.LG](#csLG) (14 ç¯‡)
- [cs.CL](#csCL) (2 ç¯‡)
- [cs.AI](#csAI) (3 ç¯‡)

---

## cs.AI

## [1. AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828v1)

**ä½œè€…**ï¼šMinjun Zhu, Zhen Lin, Yixuan Weng ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL, cs.CV, cs.DL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šAutoFigureæ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ç§‘å­¦æ’å›¾çš„æ¡†æ¶ï¼ŒåŸºäºé•¿æ–‡æœ¬è¾“å…¥ï¼Œå¹¶é€šè¿‡FigureBenchè¿›è¡Œè¯„ä¼°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç§‘å­¦æ’å›¾åœ¨æœ‰æ•ˆä¼ è¾¾å¤æ‚æ¦‚å¿µæ–¹é¢è‡³å…³é‡è¦ï¼Œä½†æ‰‹åŠ¨åˆ¶ä½œè¿‡ç¨‹æ•ˆç‡ä½ä¸‹ï¼ŒäºŸéœ€è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šAutoFigureæ¡†æ¶é€šè¿‡æ€è€ƒã€é‡ç»„å’ŒéªŒè¯ï¼Œç”Ÿæˆç»“æ„åˆç†ä¸”ç¾è§‚çš„ç§‘å­¦æ’å›¾ï¼ŒåŒæ—¶ä¾æ‰˜FigureBenchæ•°æ®é›†è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒAutoFigureåœ¨ç”Ÿæˆç¬¦åˆå‡ºç‰ˆæ ‡å‡†çš„ç§‘å­¦æ’å›¾æ–¹é¢æ€§èƒ½ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆ, ç§‘å­¦æ’å›¾, è‡ªåŠ¨ç”Ÿæˆ, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, ä»£ç†æ¡†æ¶, æ–‡æœ¬åˆ°æ’å›¾, FigureBench, è®ºæ–‡æ’å›¾, agent

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03828v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03828v1.pdf)

---

## [2. Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794v1)

**ä½œè€…**ï¼šYingxuan Yang, Chengrui Qu, Muning Wen ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šå¼‚æ„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ€§èƒ½æ‰©å±•ä¸Šä¼˜äºåŒè´¨æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå› ä¸ºå¤šæ ·æ€§æ˜¾è‘—æå‡äº†ä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç ”ç©¶è€…å¸Œæœ›ç†è§£åœ¨åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œæ™ºèƒ½ä½“æ•°é‡å¢åŠ æ—¶ä¸ºä½•å­˜åœ¨è¾¹é™…æ•ˆç›Šé€’å‡ç°è±¡ï¼Œä»¥åŠå¤šæ ·æ€§å¦‚ä½•æå‡æ€§èƒ½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡ä¿¡æ¯è®ºæ¡†æ¶ï¼Œæå‡ºäº†æœ‰æ•ˆé€šé“æ•°K*çš„æ¦‚å¿µï¼Œä»¥é‡åŒ–ä¸åŒé…ç½®çš„è´¡çŒ®ï¼Œå¹¶åˆ†æä»»åŠ¡ä¸ç¡®å®šæ€§å¯¹æ€§èƒ½çš„é™åˆ¶ã€‚

**ä¸»è¦ç»“è®º**ï¼šå¼‚æ„æ™ºèƒ½ä½“é…ç½®çš„æ€§èƒ½ä¸€è‡´è¶…è¶ŠåŒè´¨æ™ºèƒ½ä½“ï¼Œæä¾›äº†é€šè¿‡å¤šæ ·æ€§è®¾è®¡æ„å»ºé«˜æ•ˆã€ç¨³å¥çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æŒ‡å¯¼ã€‚

**å…³é”®è¯**ï¼šå¤šä»£ç†ç³»ç»Ÿ, LLM, ä»£ç†, å¤šæ ·æ€§, ä»»åŠ¡ä¸ç¡®å®šæ€§, ä¿¡æ¯è®ºæ¡†æ¶, æ•ˆæœé€šé“, ååŒå·¥ä½œ, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ 

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03794v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03794v1.pdf)

---

## [3. AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786v1)

**ä½œè€…**ï¼šJianhao Ruan, Zhihao Xu, Yiran Peng ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šAOrchestraæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å­ä»£ç†åˆ›å»ºçš„ç³»ç»Ÿï¼Œé€šè¿‡åŠ¨æ€æŠ½è±¡æ¨¡å‹æå‡å¤šè½®ä»»åŠ¡è§£å†³çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„å­ä»£ç†è®¾è®¡ç¼ºä¹åŠ¨æ€æŠ½è±¡è§†å›¾ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨åˆ›å»ºå’Œç®¡ç†å­ä»£ç†çš„ç³»ç»Ÿã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šAOrchestraé‡‡ç”¨ç»Ÿä¸€çš„ä»£ç†æŠ½è±¡æ¨¡å‹ï¼Œå°†ä»£ç†è¡¨ç¤ºä¸ºæŒ‡ä»¤ã€ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œæ¨¡å‹çš„å…ƒç»„ï¼Œä»¥ä¾¿åŠ¨æ€ç”Ÿæˆä¸“ç”¨æ‰§è¡Œå™¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨GAIAã€SWE-Benchå’ŒTerminal-Benchç­‰ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAOrchestraç›¸è¾ƒäºæœ€å¼ºåŸºå‡†å®ç°äº†16.28%çš„ç›¸å¯¹æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ä»»åŠ¡æ‰§è¡Œä¸­çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šå­ä»£ç†, ä»»åŠ¡è‡ªåŠ¨åŒ–, å¤šè½®ä»»åŠ¡è§£å†³, ä»£ç†æŠ½è±¡, ç»Ÿä¸€æ¡†æ¶, è‡ªé€‚åº”èƒ½åŠ›, AOrchestra, ä»»åŠ¡æ‰§è¡Œå™¨, ä»£ç†ç³»ç»Ÿ, ç»©æ•ˆæˆæœ¬æƒè¡¡, agent

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03786v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03786v1.pdf)

---

## cs.CL

## [4. Accelerating Scientific Research with Gemini: Case Studies and Common Techniques](https://arxiv.org/abs/2602.03837v1)

**ä½œè€…**ï¼šDavid P. Woodruff, Vincent Cohen-Addad, Lalit Jain ç­‰ 34 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨Geminiæ¨¡å‹åŠ é€Ÿç§‘å­¦ç ”ç©¶ï¼Œå¹¶æç‚¼å‡ºæœ‰æ•ˆçš„äººæœºåä½œæŠ€æœ¯ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œæ¢ç´¢å…¶åœ¨é«˜æ°´å¹³æ•°å­¦å‘ç°ä¸­çš„åº”ç”¨æ½œåŠ›æˆä¸ºç ”ç©¶çš„åŠ¨æœºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†Geminiæ¨¡å‹åœ¨è§£å†³å¼€æ”¾é—®é¢˜å’Œç”Ÿæˆæ–°è¯æ˜ä¸­çš„åº”ç”¨ï¼Œå¹¶æ€»ç»“äº†è¿­ä»£ä¼˜åŒ–ã€é—®é¢˜åˆ†è§£ç­‰åä½œæŠ€æœ¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šAIä¸ä»…å¯ä»¥ä½œä¸ºè‡ªåŠ¨åŒ–å·¥å…·ï¼Œè¿˜èƒ½ä½œä¸ºç§‘å­¦å‘ç°è¿‡ç¨‹ä¸­çš„åˆ›æ–°åˆä½œä¼™ä¼´ï¼Œæ¨åŠ¨ç ”ç©¶è¿›å±•ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, å¤§è¯­è¨€æ¨¡å‹, äººæœºåä½œ, è¿­ä»£ä¼˜åŒ–, è·¨å­¦ç§‘çŸ¥è¯†è½¬ç§», ç”Ÿæˆæ¨¡å‹, è‡ªä¸»ä»£ç æ‰§è¡Œ, llm

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03837v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03837v1.pdf)

---

## [5. They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References](https://arxiv.org/abs/2602.03822v1)

**ä½œè€…**ï¼šSahil Tripathi, Gautam Siddharth Kashyap, Mehwish Nasim ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCROSS-ALIGN+æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºè¡¨æƒ…åŒ…çš„ç¤¾ä¼šè™å¾…æ£€æµ‹çš„æ•ˆæœï¼Œå…‹æœæ–‡åŒ–ç›²ç‚¹ã€è¾¹ç•Œæ¨¡ç³Šå’Œç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè¡¨æƒ…åŒ…ä¸­çš„ç¤¾ä¼šè™å¾…æ£€æµ‹é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºæœ‰å®³æ„å›¾å¸¸å¸¸ä¾èµ–äºéšå«çš„æ–‡åŒ–ç¬¦å·å’Œå¾®å¦™çš„è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šCROSS-ALIGN+é€šè¿‡ä¸‰ä¸ªé˜¶æ®µä¾æ¬¡è§£å†³æ–‡åŒ–ç›²ç‚¹ã€è¾¹ç•Œæ¨¡ç³Šå’Œå¯è§£é‡Šæ€§é—®é¢˜ï¼Œåˆ©ç”¨çŸ¥è¯†åº“ä¸°å¯Œå¤šæ¨¡æ€è¡¨ç¤ºï¼Œä¼˜åŒ–å†³ç­–è¾¹ç•Œï¼Œå¹¶ç”Ÿæˆçº§è”è§£é‡Šã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒCROSS-ALIGN+åœ¨äº”ä¸ªåŸºå‡†å’Œå…«ä¸ªå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ€é«˜å¯å®ç°17%çš„ç›¸å¯¹F1æå‡ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„å†³ç­–ä¾æ®ã€‚

**å…³é”®è¯**ï¼šå…³é”®è¯ï¼šæ·±åº¦å­¦ä¹ , å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹, æ–‡åŒ–ç¬¦å·, å¤šæ¨¡æ€è¡¨ç¤º, è§£é‡Šæ€§, CROSS-ALIGN+, å‚æ•°é«˜æ•ˆ, å†³ç­–è¾¹ç•Œ, è¯­ä¹‰æœç´¢, ml

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03822v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03822v1.pdf)

---

## cs.CV

## [6. EventNeuS: 3D Mesh Reconstruction from a Single Event Camera](https://arxiv.org/abs/2602.03847v1)

**ä½œè€…**ï¼šShreyas Sachan, Viktor Rudnev, Mohamed Elgharib ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šEventNeuSæ˜¯ä¸€ç§è‡ªç›‘ç£ç¥ç»æ¨¡å‹ï¼Œé€šè¿‡å•ä¸€äº‹ä»¶ç›¸æœºçš„å½©è‰²äº‹ä»¶æµå­¦ä¹ 3Dè¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜äº†3Dç½‘æ ¼é‡å»ºçš„å‡†ç¡®æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå°½ç®¡è¿‘æœŸåœ¨åŸºäºäº‹ä»¶çš„è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†å¯†é›†çš„3Dç½‘æ ¼é‡å»ºä»ç„¶ç¼ºä¹æ·±å…¥ç ”ç©¶ï¼Œç°æœ‰æŠ€æœ¯åœ¨3Dé‡å»ºç²¾åº¦ä¸Šå­˜åœ¨ä¸¥é‡å±€é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šEventNeuSé¦–æ¬¡ç»“åˆäº†3Dç¬¦å·è·ç¦»å‡½æ•°å’Œå¯†åº¦åœºå­¦ä¹ ï¼Œå¹¶å¼•å…¥çƒè°ç¼–ç ä»¥å¢å¼ºå¯¹è§†è§’ä¾èµ–æ•ˆåº”çš„å¤„ç†èƒ½åŠ›ã€‚

**ä¸»è¦ç»“è®º**ï¼šEventNeuSåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡å®ç°äº†34%çš„Chamferè·ç¦»é™ä½å’Œ31%çš„å¹³å‡ç»å¯¹è¯¯å·®é™ä½ã€‚

**å…³é”®è¯**ï¼š3Dé‡å»º, äº‹ä»¶ç›¸æœº, è‡ªç›‘ç£, ç¥ç»ç½‘ç»œ, è§†å›¾ä¾èµ–æ•ˆæœ, äº‹ä»¶é©±åŠ¨, ç”Ÿæˆæ¨¡å‹, æ¨¡å‹ä¼˜åŒ–, è¯­ä¹‰æœç´¢, rag

**è¯„åˆ†**ï¼š71

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03847v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03847v1.pdf)

---

## [7. Continuous Control of Editing Models via Adaptive-Origin Guidance](https://arxiv.org/abs/2602.03826v1)

**ä½œè€…**ï¼šAlon Wolf, Chen Katzir, Kfir Aberman ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.GR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¼•å¯¼æ–¹æ³•AdaOrï¼Œæ—¨åœ¨å®ç°å¯¹ç¼–è¾‘æ¨¡å‹çš„å¹³æ»‘æ§åˆ¶ï¼Œä»è€Œæ”¹å–„æ–‡æœ¬å¼•å¯¼ç¼–è¾‘çš„å¼ºåº¦è°ƒèŠ‚ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„æ‰©æ•£ç¼–è¾‘æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼ç¼–è¾‘çš„å¼ºåº¦æ§åˆ¶ä¸Šå­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å®ç°è¾“å…¥ä¸ç¼–è¾‘ç»“æœä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºçš„AdaOræ–¹æ³•é€šè¿‡å°†æ ‡å‡†æ— æ¡ä»¶é¢„æµ‹ä¸èº«ä»½æ¡ä»¶è‡ªé€‚åº”é¢„æµ‹è¿›è¡Œæ’å€¼ï¼Œæ ¹æ®ç¼–è¾‘å¼ºåº¦è°ƒæ•´å¼•å¯¼åŸç‚¹ï¼Œå®ç°è¿ç»­æ§åˆ¶ã€‚

**ä¸»è¦ç»“è®º**ï¼šä¸ç°æœ‰åŸºäºæ»‘å—çš„ç¼–è¾‘æ–¹æ³•ç›¸æ¯”ï¼ŒAdaOråœ¨å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­æä¾›äº†æ›´å¹³æ»‘ã€æ›´ä¸€è‡´çš„æ§åˆ¶ï¼Œä¸”æ— éœ€ä¾èµ–ç‰¹å®šæ•°æ®é›†æˆ–é€ä¸ªç¼–è¾‘è¿‡ç¨‹ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, ç¼–è¾‘æ¨¡å‹, è¯­ä¹‰å›¾åƒ, è§†é¢‘æ“æ§, è‡ªé€‚åº”å¼•å¯¼, ç”Ÿæˆæ¨¡å‹, æ§åˆ¶å¼ºåº¦, ç»†ç²’åº¦æ§åˆ¶, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , diffusion

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03826v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03826v1.pdf)

---

## [8. From Pre- to Intra-operative MRI: Predicting Brain Shift in Temporal Lobe Resection for Epilepsy Surgery](https://arxiv.org/abs/2602.03785v1)

**ä½œè€…**ï¼šJingjing Peng, Giorgio Fiore, Yang Liu ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Introduction: In neurosurgery, image-guided Neurosurgery Systems (IGNS) highly rely on preoperative brain magnetic resonance images (MRI) to assist surgeons in locating surgical targets and determining surgical paths. However, brain shift invalidates the preoperative MRI after dural opening. Updated intraoperative brain MRI with brain shift compensation is crucial for enhancing the precision of neuronavigation systems and ensuring the optimal outcome of surgical interventions. Methodology: We propose NeuralShift, a U-Net-based model that predicts brain shift entirely from pre-operative MRI for patients undergoing temporal lobe resection. We evaluated our results using Target Registration Errors (TREs) computed on anatomical landmarks located on the resection side and along the midline, and DICE scores comparing predicted intraoperative masks with masks derived from intraoperative MRI. Results: Our experimental results show that our model can predict the global deformation of the brain (DICE of 0.97) with accurate local displacements (achieve landmark TRE as low as 1.12 mm), compensating for large brain shifts during temporal lobe removal neurosurgery. Conclusion: Our proposed model is capable of predicting the global deformation of the brain during temporal lobe resection using only preoperative images, providing potential opportunities to the surgical team to increase safety and efficiency of neurosurgery and better outcomes to patients. Our contributions will be publicly available after acceptance in https://github.com/SurgicalDataScienceKCL/NeuralShift.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºU-Netçš„æ¨¡å‹NeuralShiftï¼Œèƒ½å¤Ÿä»…é€šè¿‡æœ¯å‰MRIé¢„æµ‹ç™«ç—«æ‰‹æœ¯ä¸­çš„è„‘ä½ç§»ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨ç¥ç»å¤–ç§‘ä¸­ï¼Œæœ¯å‰MRIå—åˆ°è„‘ä½ç§»çš„å½±å“ï¼Œå¯¼è‡´å®šä½ä¸å‡†ç¡®ï¼Œå› æ­¤éœ€è¦æ›´æ–°çš„æœ¯ä¸­MRIæ¥è¡¥å¿è„‘ä½ç§»ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæˆ‘ä»¬æå‡ºçš„NeuralShiftæ¨¡å‹åˆ©ç”¨æœ¯å‰MRIæ•°æ®ï¼Œé¢„æµ‹ç™«ç—«æ‰‹æœ¯ä¸­è„‘çš„å…¨çƒå˜å½¢ï¼Œå¹¶é€šè¿‡ç›®æ ‡æ³¨å†Œè¯¯å·®å’ŒDICEåˆ†æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹è„‘ä½ç§»ï¼Œä»è€Œæé«˜ç¥ç»å¤–ç§‘æ‰‹æœ¯çš„å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œæ”¹å–„æ‚£è€…çš„æ‰‹æœ¯ç»“æœã€‚

**å…³é”®è¯**ï¼šç¥ç»ç½‘ç»œ, æ·±åº¦å­¦ä¹ , é¢„æµ‹æ¨¡å‹, U-Net, ç¥ç»å¤–ç§‘, è„‘ç§»ä½, å›¾åƒå¼•å¯¼, æ‰‹æœ¯å¯¼èˆª, DICEè¯„åˆ†, ç›®æ ‡æ³¨å†Œè¯¯å·®, agent

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03785v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03785v1.pdf)

---

## [9. QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization](https://arxiv.org/abs/2602.03782v1)

**ä½œè€…**ï¼šYuhao Xu, Yantai Yang, Zhenyang Fan ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.RO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šQVLAæ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é‡åŒ–çš„æ–°æ¡†æ¶ï¼Œé‡‡ç”¨é€šé“çº§æ¯”ç‰¹åˆ†é…ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å‹ç¼©æ•ˆæœå’Œæ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„ç»Ÿä¸€æ¯”ç‰¹é‡åŒ–æ–¹æ³•åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†åŠ¨ä½œåå·®å¯¹ä»»åŠ¡å¤±è´¥çš„å½±å“ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªæ›´ä¸ºç²¾ç»†çš„é‡åŒ–ç­–ç•¥ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šQVLAæ¡†æ¶é€šè¿‡ç›´æ¥æµ‹é‡æ¯ä¸ªé€šé“åœ¨ä¸åŒæ¯”ç‰¹å®½åº¦ä¸‹çš„æœ€ç»ˆåŠ¨ä½œç©ºé—´æ•æ„Ÿæ€§ï¼Œæä¾›äº†é€šé“é‡è¦æ€§åº¦é‡ï¼Œå¹¶å°†é‡åŒ–ä¸å‰ªæç»Ÿä¸€ä¸ºä¸€ä¸ªä¼˜åŒ–æ¡†æ¶ã€‚

**ä¸»è¦ç»“è®º**ï¼šQVLAåœ¨LIBEROæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶é‡åŒ–ç‰ˆæœ¬ä»…éœ€29.2%çš„åŸå§‹æ¨¡å‹æ˜¾å­˜ï¼ŒåŒæ—¶ä¿æŒ98.9%çš„åŸå§‹æ€§èƒ½ï¼Œå®ç°äº†1.49å€çš„åŠ é€Ÿï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

**å…³é”®è¯**ï¼šé‡å­åŒ–, è§†è§‰-è¯­è¨€-åŠ¨ä½œ, embodied intelligence, ä½æ¯”ç‰¹é‡å­åŒ–, æ¨¡å‹å‹ç¼©, QVLA, æœºå™¨äººæ§åˆ¶, action-centric quantization, channel-wise bit allocation, æ€§èƒ½æå‡, llm

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03782v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03782v1.pdf)

---

## [10. FOVI: A biologically-inspired foveated interface for deep vision models](https://arxiv.org/abs/2602.03766v1)

**ä½œè€…**ï¼šNicholas M. Blauch, George A. Alvarez, Talia Konkle  
**åˆ†ç±»**ï¼šcs.CV, cs.NE, q-bio.NC  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Human vision is foveated, with variable resolution peaking at the center of a large field of view; this reflects an efficient trade-off for active sensing, allowing eye-movements to bring different parts of the world into focus with other parts of the world in context. In contrast, most computer vision systems encode the visual world at a uniform resolution, raising challenges for processing full-field high-resolution images efficiently. We propose a foveated vision interface (FOVI) based on the human retina and primary visual cortex, that reformats a variable-resolution retina-like sensor array into a uniformly dense, V1-like sensor manifold. Receptive fields are defined as k-nearest-neighborhoods (kNNs) on the sensor manifold, enabling kNN-convolution via a novel kernel mapping technique. We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, leveraging low-rank adaptation (LoRA). These models provide competitive performance at a fraction of the computational cost of non-foveated baselines, opening pathways for efficient and scalable active sensing for high-resolution egocentric vision. Code and pre-trained models are available at https://github.com/nblauch/fovi and https://huggingface.co/fovi-pytorch.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFOVIæ˜¯ä¸€ç§å—ç”Ÿç‰©å¯å‘çš„å‡¹è§†ç•Œé¢ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»è§†ç½‘è†œæé«˜æ·±åº¦è§†è§‰æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šäººç±»çš„è§†åŠ›å…·æœ‰å¯å˜åˆ†è¾¨ç‡çš„ç‰¹æ€§ï¼Œè€Œå¤§å¤šæ•°è®¡ç®—æœºè§†è§‰ç³»ç»Ÿå´ä½¿ç”¨å‡åŒ€åˆ†è¾¨ç‡ï¼Œè¿™å¯¼è‡´å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶çš„æ•ˆç‡é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šFOVIé€šè¿‡å°†å¯å˜åˆ†è¾¨ç‡çš„ä¼ æ„Ÿå™¨é˜µåˆ—é‡å¡‘ä¸ºå‡åŒ€å¯†é›†çš„ä¼ æ„Ÿå™¨æµå½¢ï¼Œå¹¶å®šä¹‰æ¥æ”¶åœºä¸ºkè¿‘é‚»ï¼Œåˆ©ç”¨æ–°é¢–çš„æ ¸æ˜ å°„æŠ€æœ¯å®ç°kNNå·ç§¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šFOVIåœ¨è®¡ç®—æˆæœ¬ä¸Šæ˜¾è‘—ä¼˜äºéå‡¹è§†æ¨¡å‹ï¼Œå±•ç¤ºäº†é«˜æ•ˆã€å¯æ‰©å±•çš„ä¸»åŠ¨æ„ŸçŸ¥åœ¨é«˜åˆ†è¾¨ç‡è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šç”Ÿç‰©å¯å‘, foveated interface, æ·±åº¦è§†è§‰æ¨¡å‹, è§†è§‰å¤„ç†, kNNå·ç§¯, DINOv3, ä½ç§©é€‚åº”, ä¸»åŠ¨æ„ŸçŸ¥, è®¡ç®—æ•ˆç‡, ml

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03766v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03766v1.pdf)

---

## [11. RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images](https://arxiv.org/abs/2602.03760v1)

**ä½œè€…**ï¼šMishal Fatima, Shashank Agnihotri, Kanchana Vaishnavi Gandikota ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Most vision models are trained on RGB images processed through ISP pipelines optimized for human perception, which can discard sensor-level information useful for machine reasoning. RAW images preserve unprocessed scene data, enabling models to leverage richer cues for both object detection and object description, capturing fine-grained details, spatial relationships, and contextual information often lost in processed images. To support research in this domain, we introduce RAWDet-7, a large-scale dataset of ~25k training and 7.6k test RAW images collected across diverse cameras, lighting conditions, and environments, densely annotated for seven object categories following MS-COCO and LVIS conventions. In addition, we provide object-level descriptions derived from the corresponding high-resolution sRGB images, facilitating the study of object-level information preservation under RAW image processing and low-bit quantization. The dataset allows evaluation under simulated 4-bit, 6-bit, and 8-bit quantization, reflecting realistic sensor constraints, and provides a benchmark for studying detection performance, description quality & detail, and generalization in low-bit RAW image processing. Dataset & code upon acceptance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šRAWDet-7æ˜¯ä¸€ä¸ªç”¨äºé‡åŒ–RAWå›¾åƒä¸‹ç‰©ä½“æ£€æµ‹å’Œæè¿°çš„å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«25kè®­ç»ƒå’Œ7.6kæµ‹è¯•å›¾åƒã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è§†è§‰æ¨¡å‹å¤šåŸºäºRGBå›¾åƒï¼Œå¿½è§†äº†RAWå›¾åƒä¸­ä¿ç•™çš„ä¼ æ„Ÿå™¨çº§ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯¹æœºå™¨æ¨ç†æœ‰é‡è¦ä»·å€¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§ç›¸æœºå’Œç¯å¢ƒçš„RAWå›¾åƒæ•°æ®é›†ï¼Œå¹¶æä¾›äº†å¤šç§é‡åŒ–æ¨¡æ‹Ÿä»¥è¯„ä¼°ç‰©ä½“æ£€æµ‹å’Œæè¿°æ€§èƒ½ã€‚

**ä¸»è¦ç»“è®º**ï¼šRAWDet-7ä¸ºç ”ç©¶é‡åŒ–RAWå›¾åƒå¤„ç†ä¸‹çš„ç‰©ä½“æ£€æµ‹å’Œæè¿°æä¾›äº†åŸºå‡†ï¼Œæ˜¾ç¤ºäº†åœ¨ä½ä½æ•°æƒ…å†µä¸‹çš„ä¿¡æ¯ä¿ç•™èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šç›®æ ‡æ£€æµ‹, RAWå›¾åƒ, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , æ•°æ®é›†, è®¡ç®—æœºè§†è§‰, è¯­ä¹‰æœç´¢, å¤šåœºæ™¯åŸºå‡†, ä½ä½é‡åŒ–, ç‰©ä½“æè¿°, rag

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03760v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03760v1.pdf)

---

## [12. Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives](https://arxiv.org/abs/2602.03750v1)

**ä½œè€…**ï¼šOwen Dong, Lily Gao, Manish Kota ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é›¶-shotæç¤ºç­–ç•¥ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è‡ªåŠ¨è¯†åˆ«å¤äººç±»æ”¾å°„å­¦Xå…‰å›¾åƒä¸­çš„ä¸»è¦éª¨éª¼ã€æŠ•å½±è§†å›¾å’Œä¾§å‘æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¤äººç±»æ”¾å°„å­¦ä¸­çš„Xå…‰å›¾åƒæ•°æ®å¼‚è´¨æ€§ä½¿å¾—ä¸“å®¶åˆ†ææ•ˆç‡ä½ä¸‹ï¼Œå› æ­¤éœ€è¦ä¸€ç§è‡ªåŠ¨åŒ–çš„æ–¹æ³•æ¥åŠ é€Ÿå›¾åƒå†…å®¹çš„å¯¼èˆªå’Œåˆ†ç±»ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç ”ç©¶ä¸­ä½¿ç”¨äº†å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå°†åŸå§‹DICOMæ–‡ä»¶è½¬æ¢ä¸ºéª¨çª—PNGæ ¼å¼ï¼Œå¹¶è¾“å‡ºç»“æ„åŒ–çš„JSONæ•°æ®ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨éª¨éª¼è¯†åˆ«ã€æŠ•å½±è§†å›¾å’Œä¾§å‘æ€§è¯†åˆ«ä¸Šå–å¾—äº†é«˜å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºäº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤äººç±»æ”¾å°„å­¦æ•°æ®é›†ä¸­çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚

**å…³é”®è¯**ï¼šå¤§æ¨¡å‹, è§†è§‰è¯­è¨€æ¨¡å‹, è‡ªåŠ¨åŒ–è¯†åˆ«, éª¨éª¼è¯†åˆ«, å½±åƒåˆ†æ, DICOMå¤„ç†, äººæœºåä½œ, ä¸“å®¶éªŒè¯, å†…å®¹å¯¼èˆª, rag

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03750v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03750v1.pdf)

---

## [13. See-through: Single-image Layer Decomposition for Anime Characters](https://arxiv.org/abs/2602.03749v1)

**ä½œè€…**ï¼šJian Lin, Chengze Li, Haoyun Qin ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.GR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination'' of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œå°†é™æ€åŠ¨æ¼«æ’å›¾è‡ªåŠ¨è½¬æ¢ä¸ºå¯æ“ä½œçš„2.5Dæ¨¡å‹ï¼Œé€šè¿‡å•å›¾å±‚åˆ†è§£å®ç°é«˜è´¨é‡åŠ¨ç”»æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰ä¸“ä¸šå·¥ä½œæµç¨‹éœ€è¦ç¹ççš„æ‰‹åŠ¨åˆ†å‰²å’Œè‰ºæœ¯æ€§è¡¥å…¨ï¼Œé™åˆ¶äº†åŠ¨æ¼«è§’è‰²çš„åŠ¨æ€è¡¨ç°èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦ä¸€ç§è‡ªåŠ¨åŒ–çš„æ–¹æ³•æ¥æå‡æ•ˆç‡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæœ¬æ–¹æ³•é€šè¿‡å°†å•å¹…å›¾åƒåˆ†è§£ä¸ºè¯­ä¹‰æ˜ç¡®çš„å±‚ï¼Œå¹¶ä½¿ç”¨åŸºäºæ‰©æ•£çš„èº«ä½“éƒ¨ä½ä¸€è‡´æ€§æ¨¡å—å’Œä¼ªæ·±åº¦æ¨æ–­æœºåˆ¶ï¼Œå®ç°äº†åŠ¨æ¼«è§’è‰²çš„åŠ¨æ€å±‚é‡æ„ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€å¯æ“ä½œçš„æ¨¡å‹ï¼Œé€‚ç”¨äºä¸“ä¸šå®æ—¶åŠ¨ç”»åº”ç”¨ã€‚

**å…³é”®è¯**ï¼šå•å¹…å›¾åƒ, å±‚åˆ†è§£, åŠ¨æ¼«è§’è‰², 2.5Dæ¨¡å‹, è¯­ä¹‰åˆ†å±‚, åƒç´ çº§æ¨æ–­, ç”Ÿæˆæ¨¡å‹, æ·±åº¦å­¦ä¹ , è¯­ä¹‰ä¸€è‡´æ€§, diffusion

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03749v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03749v1.pdf)

---

## [14. LIVE: Long-horizon Interactive Video World Modeling](https://arxiv.org/abs/2602.03747v1)

**ä½œè€…**ï¼šJunchao Huang, Ziyang Ye, Xinting Hu ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘ä¸–ç•Œå»ºæ¨¡æ–¹æ³•LIVEï¼Œèƒ½å¤Ÿåœ¨é•¿æ—¶é—´èŒƒå›´å†…æœ‰æ•ˆé¢„æµ‹è§†é¢‘ï¼Œå‡å°‘é¢„æµ‹è¯¯å·®çš„ç´¯ç§¯ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿçš„è‡ªå›å½’è§†é¢‘æ¨¡å‹åœ¨é•¿æ—¶é—´é¢„æµ‹ä¸­è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´è¯¯å·®ç´¯ç§¯å’Œç”Ÿæˆè´¨é‡ä¸‹é™ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ”¹å–„è¿™ä¸€é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šLIVEé€šè¿‡å¼•å…¥å¾ªç¯ä¸€è‡´æ€§ç›®æ ‡æ¥é™åˆ¶è¯¯å·®ç´¯ç§¯ï¼Œé‡‡ç”¨å‰å‘ç”Ÿæˆå’Œåå‘é‡å»ºçš„è¿‡ç¨‹æ¥æé«˜é•¿æ—¶é—´é¢„æµ‹çš„ç¨³å®šæ€§ä¸è´¨é‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒLIVEåœ¨é•¿æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆçš„è§†é¢‘è´¨é‡é«˜ä¸”ç¨³å®šï¼Œè¶…å‡ºè®­ç»ƒèŒƒå›´çš„ç”Ÿæˆèƒ½åŠ›æ˜¾è‘—æå‡ã€‚

**å…³é”®è¯**ï¼šé•¿è§†è·, äº¤äº’å¼è§†é¢‘, ä¸–ç•Œå»ºæ¨¡, è‡ªå›å½’æ¨¡å‹, å¾ªç¯ä¸€è‡´æ€§, ç”Ÿæˆæ¨¡å‹, è®­ç»ƒè¯¾ç¨‹, ç¨³å®šæ€§, é¢„æµ‹é”™è¯¯, è§†è§‰è§‚å¯Ÿ, diffusion

**è¯„åˆ†**ï¼š62

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03747v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03747v1.pdf)

---

## [15. Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment](https://arxiv.org/abs/2602.03742v1)

**ä½œè€…**ï¼šJohny J. Lopez, Md Meftahul Ferdaus, Mahdi Abdelguerfi  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¾¹ç¼˜ä¼˜åŒ–çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºåœ°ä¸‹åŸºç¡€è®¾æ–½çš„è‡ªåŠ¨æ£€æµ‹å’Œæ€»ç»“ï¼Œæå‡äº†æ£€æµ‹æ•ˆç‡å’Œå®æ—¶æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ°ä¸‹åŸºç¡€è®¾æ–½çš„è‡ªåŠ¨æ£€æµ‹å¯¹å…¬å…±å®‰å…¨å’ŒåŸå¸‚å¯æŒç»­å‘å±•è‡³å…³é‡è¦ï¼Œä½†åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šç”Ÿæˆå¯è¯»çš„æ£€æµ‹æ‘˜è¦ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç®¡é“ï¼Œç»“åˆäº†è½»é‡çº§çš„RAPID-SCANåˆ†å‰²æ¨¡å‹å’Œç²¾è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨è¾¹ç¼˜è®¡ç®—å¹³å°ä¸Šå®ç°äº†é«˜æ•ˆçš„ç¼ºé™·åˆ†å‰²å’Œæ‘˜è¦ç”Ÿæˆã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥ç³»ç»Ÿåœ¨ç§»åŠ¨æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå±•ç¤ºäº†è¾¹ç¼˜å¯éƒ¨ç½²çš„é›†æˆAIç³»ç»Ÿåœ¨è‡ªåŠ¨ç¼ºé™·æ£€æµ‹ä¸åŸºç¡€è®¾æ–½ç»´æŠ¤æ´å¯Ÿä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ï¼Œä¸ºæ›´å¯æ‰©å±•çš„è‡ªåŠ¨æ£€æµ‹è§£å†³æ–¹æ¡ˆé“ºå¹³äº†é“è·¯ã€‚

**å…³é”®è¯**ï¼šè§†è§‰è¯­è¨€æ¨¡å‹, æ·±åº¦å­¦ä¹ , æœºå™¨äººå¹³å°, è‡ªåŠ¨åŒ–æ£€æµ‹, è¾¹ç¼˜è®¡ç®—, èµ„æºæ„ŸçŸ¥, ç¼ºé™·åˆ†å‰², å®æ—¶æ€§èƒ½, æ¨¡å‹ä¼˜åŒ–, è‡ªä¸»æ£€æŸ¥, autonomous

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03742v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03742v1.pdf)

---

## [16. RegionReasoner: Region-Grounded Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.03733v1)

**ä½œè€…**ï¼šWenfang Sun, Hao Chen, Yingjun Du ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šRegionReasoneræ˜¯ä¸€ç§é€šè¿‡å¤šè½®æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶æé«˜è§†è§‰æ¨ç†å‡†ç¡®æ€§çš„æ¨¡å‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè½®æ¨ç†æ–¹é¢èƒ½åŠ›æœ‰é™ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–°çš„åŸºå‡†å’Œæ–¹æ³•æ¥æå‡å…¶åœ¨æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šRegionReasoneré€šè¿‡è¦æ±‚æ¯ä¸ªæ¨ç†è¿‡ç¨‹æ˜ç¡®å¼•ç”¨å¯¹åº”çš„è¾¹ç•Œæ¡†ï¼Œå¹¶ç»“åˆå…¨å±€-å±€éƒ¨ä¸€è‡´æ€§å¥–åŠ±ï¼Œä¼˜åŒ–æ¨ç†çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒRegionReasoner-7Bæ˜¾è‘—æå‡äº†å¤šè½®æ¨ç†çš„å‡†ç¡®æ€§å’Œç©ºé—´å®šä½çš„ç²¾ç¡®åº¦ï¼Œä¸ºè¿™ä¸€æ–°å…´ç ”ç©¶æ–¹å‘å¥ å®šäº†åšå®çš„åŸºçº¿ã€‚

**å…³é”®è¯**ï¼šè§†è§‰æ¨ç†, å¤šè½®æ¨ç†, å¼ºåŒ–å­¦ä¹ , è¯­ä¹‰ä¸€è‡´æ€§, RegionReasoner, è§†è§‰-è¯­è¨€æ¨¡å‹, æ£€æµ‹ä¸åˆ†å‰², å¥–åŠ±æ¨¡å‹, è¿­ä»£æ¨ç†, context

**è¯„åˆ†**ï¼š73

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03733v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03733v1.pdf)

---

## cs.LG

## [17. Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL](https://arxiv.org/abs/2602.03839v1)

**ä½œè€…**ï¼šErfan Miahi, Eugene Belilovsky  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPULSEçš„é«˜æ•ˆç¨€ç–æƒé‡åŒæ­¥æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†å¸¦å®½éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒæ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨å¸¦å®½å—é™çš„åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç­–ç•¥æƒé‡çš„åŒæ­¥å¸¸æˆä¸ºæ‰©å±•æ€§çš„ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨å•†å“ç½‘ç»œæˆ–å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸­ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šPULSEæ–¹æ³•é€šè¿‡ä¼ è¾“ä¿®æ”¹å‚æ•°çš„ç´¢å¼•å’Œå€¼ï¼Œåˆ©ç”¨æƒé‡æ›´æ–°çš„ç¨€ç–æ€§ï¼Œé¿å…äº†æµ®ç‚¹æ•°æ¼‚ç§»å’Œä¼ è¾“é”™è¯¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šPULSEåœ¨å¸¦å®½é™åˆ¶çš„å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸­å®ç°äº†è¶…è¿‡100å€çš„é€šä¿¡å‡å°‘ï¼Œä¿æŒäº†ä¸å…¨æƒé‡åŒæ­¥ç›¸åŒçš„è®­ç»ƒåŠ¨æ€å’Œæ€§èƒ½ã€‚

**å…³é”®è¯**ï¼šæƒé‡æ›´æ–°ç¨€ç–æ€§, å¼ºåŒ–å­¦ä¹ , åˆ†å¸ƒå¼RL, å¤§è¯­è¨€æ¨¡å‹, PULSE, é€šä¿¡æ•ˆç‡, å‚æ•°åŒæ­¥, è®­ç»ƒåŠ¨æ€, å¸¦å®½çº¦æŸ, llm

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03839v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03839v1.pdf)

---

## [18. Robust Intervention Learning from Emergency Stop Interventions](https://arxiv.org/abs/2602.03825v1)

**ä½œè€…**ï¼šEthan Pronovost, Khimya Khetarpal, Siddhartha Srinivasa  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal. In the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance. We study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy. By framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task. We provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail. Our experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§ç¨³å¥å¹²é¢„å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä»å™ªå£°å’Œä¸å®Œæ•´çš„å¹²é¢„æ•°æ®ä¸­å­¦ä¹ ï¼Œä»¥æ”¹è¿›è‡ªä¸»ç³»ç»Ÿçš„ç­–ç•¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šäººç±»å¹²é¢„åœ¨è‡ªä¸»ç³»ç»Ÿæµ‹è¯•ä¸­æä¾›äº†é‡è¦ä¿¡å·ï¼Œä½†å¾€å¾€å™ªå£°å¤§ä¸”ä¸å®Œæ•´ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–¹æ³•æ¥æœ‰æ•ˆåˆ©ç”¨è¿™äº›å¹²é¢„æ•°æ®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºäº†æ®‹å·®å¹²é¢„å¾®è°ƒ(RIFT)ç®—æ³•ï¼Œå°†å¹²é¢„åé¦ˆè§†ä¸ºä¸å®Œæ•´çš„å­¦ä¹ ä¿¡å·ï¼Œå¹¶ä¸å…ˆéªŒç­–ç•¥æ˜¾å¼ç»“åˆï¼Œä»¥æé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæ®‹å·®å¾®è°ƒèƒ½å¤Ÿåœ¨å¤šç§å¹²é¢„ç­–ç•¥å’Œå…ˆéªŒç­–ç•¥è´¨é‡ä¸‹å®ç°ç¨³å¥ä¸”ä¸€è‡´çš„ç­–ç•¥æ”¹è¿›ï¼Œå±•ç¤ºäº†ç¨³å¥å¹²é¢„å­¦ä¹ çš„æœªæ¥åº”ç”¨æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šå¹²é¢„å­¦ä¹ , å¼ºå¥å­¦ä¹ , æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, ç´§æ€¥åœæ­¢å¹²é¢„, æ®‹å·®å¾®è°ƒ, åé¦ˆä¿¡å·, ç­–ç•¥æ”¹è¿›, autonomous

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03825v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03825v1.pdf)

---

## [19. SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving](https://arxiv.org/abs/2602.03816v1)

**ä½œè€…**ï¼šYesom Park, Annie C. Lu, Shao-Ching Huang ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSymPlexæ˜¯ä¸€ä¸ªç”¨äºç¬¦å·åå¾®åˆ†æ–¹ç¨‹æ±‚è§£çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®è¡¨è¾¾å¼çš„æƒ…å†µä¸‹å‘ç°è§£æç¬¦å·è§£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„æ•°å€¼å’Œç¥ç»æ–¹æ³•é€šå¸¸åœ¨ç¦»æ•£æˆ–éšå¼å‡½æ•°ç©ºé—´ä¸­è¿‘ä¼¼æ±‚è§£ï¼Œè€ŒSymPlexå¸Œæœ›ç›´æ¥åœ¨ç¬¦å·è¡¨è¾¾ç©ºé—´ä¸­æ‰¾åˆ°å¯è§£é‡Šçš„ç¬¦å·è§£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šSymPlexå°†ç¬¦å·PDEæ±‚è§£å½¢å¼åŒ–ä¸ºæ ‘ç»“æ„å†³ç­–è¿‡ç¨‹ï¼Œä½¿ç”¨ç»“æ„æ„ŸçŸ¥çš„Transformerï¼ˆSymFormerï¼‰é€šè¿‡æ ‘ç›¸å¯¹è‡ªæ³¨æ„åŠ›å»ºæ¨¡å±‚æ¬¡ç¬¦å·ä¾èµ–å…³ç³»ï¼Œå¹¶é€šè¿‡è¯­æ³•çº¦æŸçš„è‡ªå›å½’è§£ç ç¡®ä¿è¯­æ³•æœ‰æ•ˆæ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¯æ˜ï¼ŒSymPlexèƒ½å¤Ÿå‡†ç¡®æ¢å¤éå…‰æ»‘å’Œå‚æ•°åŒ–çš„PDEè§£ï¼Œå±•ç¤ºäº†æ·±åº¦å­¦ä¹ åŸºç¡€çš„ç¬¦å·æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šç»“æ„æ„ŸçŸ¥, Transformer, å¼ºåŒ–å­¦ä¹ , ç¬¦å·è§£æ³•, éƒ¨åˆ†å¾®åˆ†æ–¹ç¨‹, è§£æè§£, æ ‘ç»“æ„å†³ç­–, è¯­æ³•çº¦æŸ, è‡ªå›å½’è§£ç , æ·±åº¦å­¦ä¹ 

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03816v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03816v1.pdf)

---

## [20. Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network](https://arxiv.org/abs/2602.03808v1)

**ä½œè€…**ï¼šAbdul Joseph Fofanah, Lian Wen, David Chen ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model's step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§åä¸ºCL3AN-GNNçš„ä¸‰é˜¶æ®µæ³¨æ„åŠ›ç½‘ç»œï¼Œä»¥è§£å†³å›¾ç¥ç»ç½‘ç»œä¸­çš„ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¸å¹³è¡¡çš„èŠ‚ç‚¹åˆ†ç±»ä½¿å¾—æ¨¡å‹åœ¨å°‘æ•°ç±»ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–°çš„å­¦ä¹ ç­–ç•¥æ¥æå‡æ¨¡å‹çš„å…¬å¹³æ€§å’Œå‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šCL3AN-GNNé€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆEngage, Enact, Embedï¼‰é€æ­¥å­¦ä¹ ä¸åŒå¤æ‚åº¦çš„ç‰¹å¾ï¼Œæ”¯æŒåœ¨æ ‡ç­¾ä¸å¹³è¡¡æƒ…å†µä¸‹çš„ç¨³å®šå­¦ä¹ ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒCL3AN-GNNåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œè‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œå¯¹ä¸å¹³è¡¡é—®é¢˜å…·æœ‰æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**å…³é”®è¯**ï¼šèŠ‚ç‚¹åˆ†ç±», å›¾ç¥ç»ç½‘ç»œ, ç‰¹å¾å­¦ä¹ , æ³¨æ„åŠ›æœºåˆ¶, è¯¾ç¨‹å­¦ä¹ , ä¸å¹³è¡¡æ•°æ®, ç›‘ç£å­¦ä¹ , GNN, attention network, feature learning, neural network

**è¯„åˆ†**ï¼š64

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03808v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03808v1.pdf)

---

## [21. Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation](https://arxiv.org/abs/2602.03806v1)

**ä½œè€…**ï¼šZiru Chen, Dongdong Chen, Ruinan Jin ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CL, cs.SE  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCobaltæ˜¯ä¸€ç§ç»“åˆåœ¨çº¿å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šè½®ä»£ç ç”Ÿæˆçš„æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…ä»»åŠ¡ä¸­çš„åº”ç”¨å¢å¤šï¼Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„é«˜æˆæœ¬å’Œä¸ç¨³å®šæ€§é™åˆ¶äº†å…¶å¹¿æ³›é‡‡ç”¨ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šCobalté€šè¿‡ä½¿ç”¨å‚è€ƒLLMæ”¶é›†ä»£ç ç”Ÿæˆè½¨è¿¹ï¼Œå¹¶å°†å…¶åˆ†å‰²ä¸ºä¸Šä¸‹æ–‡æç¤ºï¼Œåœ¨åœ¨çº¿èµŒåšå­¦ä¹ ä¸­è®­ç»ƒLLMå®Œæˆæ¯ä¸ªéƒ¨åˆ†è½¨è¿¹çš„å•æ­¥ä»£ç ç”Ÿæˆã€‚

**ä¸»è¦ç»“è®º**ï¼šCobaltåœ¨å¤šè½®ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¸”é€šè¿‡å¯¹æŠ—æ€§è½¨è¿¹å¢å¼ºè®­ç»ƒï¼Œç¼“è§£äº†LLMçš„å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚

**å…³é”®è¯**ï¼šå…³é”®è¯ï¼šæ·±åº¦å­¦ä¹ , æœºå™¨å­¦ä¹ , å¼ºåŒ–å­¦ä¹ , å¤šè½®ä»£ç ç”Ÿæˆ, ä¸Šä¸‹æ–‡èµŒåšå­¦ä¹ , LLM, Markovå†³ç­–è¿‡ç¨‹, è¿­ä»£å†³ç­–ä»»åŠ¡, ä»£ç ç”Ÿæˆè½¨è¿¹

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03806v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03806v1.pdf)

---

## [22. Prediction of Critical Heat Flux in Rod Bundles Using Tube-Based Hybrid Machine Learning Models in CTF](https://arxiv.org/abs/2602.03805v1)

**ä½œè€…**ï¼šAidan Furlong, Robert Salko, Xingang Zhao ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The prediction of critical heat flux (CHF) using machine learning (ML) approaches has become a highly active research activity in recent years, the goal of which is to build models more accurate than current conventional approaches such as empirical correlations or lookup tables (LUTs). Previous work developed and deployed tube-based pure and hybrid ML models in the CTF subchannel code, however, full-scale reactor core simulations require the use of rod bundle geometries. Unlike isolated subchannels, rod bundles experience complex thermal hydraulic phenomena such as channel crossflow, spacer grid losses, and effects from unheated conductors. This study investigates the generalization of ML-based CHF prediction models in rod bundles after being trained on tube-based CHF data. A purely data-driven DNN and two hybrid bias-correction models were implemented in the CTF subchannel code and used to predict CHF location and magnitude in the Combustion Engineering 5-by-5 bundle CHF test series. The W-3 correlation, Bowring correlation, and Groeneveld LUT were used as baseline comparators. On average, all three ML-based approaches produced magnitude and location predictions more accurate than the baseline models, with the hybrid LUT model exhibiting the most favorable performance metrics.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶åˆ©ç”¨åŸºäºç®¡é“çš„æ··åˆæœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹æ£’æŸä¸­çš„ä¸´ç•Œçƒ­æµå¯†åº¦ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€æœºå™¨å­¦ä¹ åœ¨ä¸´ç•Œçƒ­æµå¯†åº¦é¢„æµ‹ä¸­çš„åº”ç”¨æ—¥ç›Šå¢åŠ ï¼Œç ”ç©¶è€…å¸Œæœ›å»ºç«‹æ¯”ä¼ ç»Ÿç»éªŒæ¨¡å‹æ›´å‡†ç¡®çš„é¢„æµ‹æ¨¡å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç ”ç©¶ä¸­å®ç°äº†çº¯æ•°æ®é©±åŠ¨çš„æ·±åº¦ç¥ç»ç½‘ç»œå’Œä¸¤ç§æ··åˆåå·®æ ¡æ­£æ¨¡å‹ï¼Œå¹¶åœ¨CTFå­é€šé“ä»£ç ä¸­è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ‰€æœ‰ä¸‰ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•åœ¨é¢„æµ‹çƒ­æµå¯†åº¦çš„å¤§å°å’Œä½ç½®ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå…¶ä¸­æ··åˆLUTæ¨¡å‹è¡¨ç°æœ€ä½³ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, é¢„æµ‹æ¨¡å‹, æ•°æ®é©±åŠ¨, å¤åˆæ¨¡å‹, çƒ­æµå¯†åº¦, rod bundle, CTF, machine learning

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03805v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03805v1.pdf)

---

## [23. Manifold Random Features](https://arxiv.org/abs/2602.03797v1)

**ä½œè€…**ï¼šAnanya Parashar, Derek Long, Dwaipayan Saha ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present a new paradigm for creating random features to approximate bi-variate functions (in particular, kernels) defined on general manifolds. This new mechanism of Manifold Random Features (MRFs) leverages discretization of the manifold and the recently introduced technique of Graph Random Features (GRFs) to learn continuous fields on manifolds. Those fields are used to find continuous approximation mechanisms that otherwise, in general scenarios, cannot be derived analytically. MRFs provide positive and bounded features, a key property for accurate, low-variance approximation. We show deep asymptotic connection between GRFs, defined on discrete graph objects, and continuous random features used for regular kernels. As a by-product of our method, we re-discover recently introduced mechanism of Gaussian kernel approximation applied in particular to improve linear-attention Transformers, considering simple random walks on graphs and by-passing original complex mathematical computations. We complement our algorithm with a rigorous theoretical analysis and verify in thorough experimental studies.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„éšæœºç‰¹å¾æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸€èˆ¬æµå½¢ä¸Šè¿‘ä¼¼åŒå˜é‡å‡½æ•°ï¼Œç‰¹åˆ«æ˜¯æ ¸å‡½æ•°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç ”ç©¶å¦‚ä½•åœ¨å¤æ‚æµå½¢ä¸Šæœ‰æ•ˆåœ°è¿‘ä¼¼å‡½æ•°ï¼Œä»¥è§£å†³æ— æ³•è§£ææ¨å¯¼çš„è¿ç»­è¿‘ä¼¼æœºåˆ¶é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¼•å…¥æµå½¢éšæœºç‰¹å¾ï¼ˆMRFsï¼‰ï¼Œç»“åˆæµå½¢çš„ç¦»æ•£åŒ–å’Œå›¾éšæœºç‰¹å¾ï¼ˆGRFsï¼‰æŠ€æœ¯ï¼Œå­¦ä¹ æµå½¢ä¸Šçš„è¿ç»­åœºï¼Œä»è€Œå®ç°å‡†ç¡®ä¸”ä½æ–¹å·®çš„å‡½æ•°è¿‘ä¼¼ã€‚

**ä¸»è¦ç»“è®º**ï¼šé€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼ŒMRFsèƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„çº¿æ€§æ³¨æ„åŠ›Transformerçš„æ€§èƒ½ï¼Œå¹¶ç®€åŒ–é«˜å¤æ‚åº¦çš„æ•°å­¦è®¡ç®—ã€‚

**å…³é”®è¯**ï¼šéšæœºç‰¹å¾, åŒå˜é‡å‡½æ•°, æµå½¢, æ·±åº¦å­¦ä¹ , å›¾éšæœºç‰¹å¾, çº¿æ€§æ³¨æ„åŠ›, å˜æ¢å™¨, è¿ç»­è¿‘ä¼¼, ä½æ–¹å·®, ç‰¹å¾å­¦ä¹ , transformer

**è¯„åˆ†**ï¼š62

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03797v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03797v1.pdf)

---

## [24. Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation](https://arxiv.org/abs/2602.03791v1)

**ä½œè€…**ï¼šBogdan Kulynych, Theresa Stadler, Jean Louis Raisaro ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.CY  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent advances in generative modelling have led many to see synthetic data as the go-to solution for a range of problems around data access, scarcity, and under-representation. In this paper, we study three prominent use cases: (1) Sharing synthetic data as a proxy for proprietary datasets to enable statistical analyses while protecting privacy, (2) Augmenting machine learning training sets with synthetic data to improve model performance, and (3) Augmenting datasets with synthetic data to reduce variance in statistical estimation. For each use case, we formalise the problem setting and study, through formal analysis and case studies, under which conditions synthetic data can achieve its intended objectives. We identify fundamental and practical limits that constrain when synthetic data can serve as an effective solution for a particular problem. Our analysis reveals that due to these limits many existing or envisioned use cases of synthetic data are a poor problem fit. Our formalisations and classification of synthetic data use cases enable decision makers to assess whether synthetic data is a suitable approach for their specific data availability problem.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬è®ºæ–‡åˆ†æäº†åˆæˆæ•°æ®åœ¨æ•°æ®å…±äº«å’Œå¢å¼ºä¸­çš„é€‚ç”¨æ€§ï¼Œæå‡ºäº†å…¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„å±€é™æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€ç”Ÿæˆå»ºæ¨¡çš„è¿›æ­¥ï¼Œåˆæˆæ•°æ®è¢«è§†ä¸ºè§£å†³æ•°æ®è®¿é—®å’Œç¨€ç¼ºé—®é¢˜çš„ä¸€ç§ç†æƒ³æ–¹æ¡ˆï¼Œæœ¬æ–‡æ—¨åœ¨è¯„ä¼°å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡å½¢å¼åŒ–åˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œè¯†åˆ«åˆæˆæ•°æ®åœ¨ä¸‰ç§ä¸»è¦ä½¿ç”¨åœºæ™¯ä¸‹çš„é€‚ç”¨æ¡ä»¶åŠå…¶å±€é™æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šç°æœ‰æˆ–è®¾æƒ³çš„åˆæˆæ•°æ®åº”ç”¨åœºæ™¯å¹¶ä¸é€‚åˆï¼Œè¿™ä¸ºå†³ç­–è€…æä¾›äº†è¯„ä¼°åˆæˆæ•°æ®é€‚ç”¨æ€§çš„æ¡†æ¶ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆæ•°æ®, ç”Ÿæˆæ¨¡å‹, æœºå™¨å­¦ä¹ , æ•°æ®å…±äº«, æ•°æ®å¢å¼º, ç»Ÿè®¡åˆ†æ, æ¨¡å‹æ€§èƒ½, å˜å¼‚æ€§é™ä½, synthetic data, æ•°æ®éšç§, machine learning

**è¯„åˆ†**ï¼š60

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03791v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03791v1.pdf)

---

## [25. Efficient Estimation of Kernel Surrogate Models for Task Attribution](https://arxiv.org/abs/2602.03783v1)

**ä½œè€…**ï¼šZhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ ¸ä»£ç†æ¨¡å‹ï¼Œç”¨äºåˆ†æä»»åŠ¡å½’å±ï¼Œå…‹æœäº†çº¿æ€§ä»£ç†æ¨¡å‹åœ¨æ•æ‰éçº¿æ€§äº¤äº’æ–¹é¢çš„å±€é™ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°ä»£AIä»£ç†åŒæ—¶åœ¨å¤šç§ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç†è§£æ¯ä¸ªè®­ç»ƒä»»åŠ¡å¯¹ç›®æ ‡ä»»åŠ¡æ€§èƒ½çš„å½±å“æ˜¯è‡³å…³é‡è¦çš„ï¼Œä½†ä¼ ç»Ÿçš„æ–¹æ³•åœ¨å¤§è§„æ¨¡ä¸Šè®¡ç®—ä¸å¯è¡Œã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ–‡ç« æå‡ºäº†åŸºäºæ¢¯åº¦çš„æ ¸ä»£ç†æ¨¡å‹ä¼°è®¡ç¨‹åºï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºä»»åŠ¡é—´çš„äºŒé˜¶äº¤äº’ï¼ŒåŒæ—¶é€šè¿‡ä¸€é˜¶è¿‘ä¼¼åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæ ¸ä»£ç†æ¨¡å‹åœ¨å¤šç§é¢†åŸŸä¸­çš„æ€§èƒ½è¯„ä¼°ä¸Šæ¯”çº¿æ€§ä»£ç†æ›´ä¸ºå‡†ç¡®ï¼Œä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡é€‰æ‹©ä¸­æ˜¾è‘—æé«˜äº†è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šä»»åŠ¡å½’å› , æ ¸å¿ƒä»£ç†æ¨¡å‹, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ä»£ç†, é¢„è®­ç»ƒæ¨¡å‹, ä»»åŠ¡åŠ æƒæ¡†æ¶, å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ , ä¸Šä¸‹æ–‡å­¦ä¹ , æ€§èƒ½é¢„æµ‹, agent

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03783v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03783v1.pdf)

---

## [26. Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity](https://arxiv.org/abs/2602.03778v1)

**ä½œè€…**ï¼šAneri Muni, Vincent Taboga, Esther Derman ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¢å¼ºçš„é™æ€æ¡ä»¶ä»·å€¼-at-risk (CVaR)ç›®æ ‡çš„æ–°å…¬å¼ï¼Œé€šè¿‡è´å°”æ›¼ç®—å­å®ç°é£é™©åŒæ¶çš„ä»·å€¼è¿­ä»£å’Œæ— æ¨¡å‹Qå­¦ä¹ ç®—æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­ï¼Œä¼ ç»Ÿçš„é£é™©ä¸­æ€§ç›®æ ‡æ— æ³•æœ‰æ•ˆå¤„ç†å°¾éƒ¨é£é™©ï¼Œå› æ­¤éœ€è¦æ–°çš„æ–¹æ³•æ¥æ›´å¥½åœ°ç®¡ç†ç¨€æœ‰ä½†ç¾éš¾æ€§çš„äº‹ä»¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡çŠ¶æ€å¢å¼ºçš„æ–¹æ³•æå‡ºé™æ€CVaRç›®æ ‡çš„æ–°å…¬å¼ï¼Œä»è€Œè·å¾—ç¨ å¯†çš„æ¯æ­¥å¥–åŠ±å’Œæ”¶æ•›æ€§è´¨ï¼Œå¹¶å¼€å‘ç›¸åº”çš„ç®—æ³•ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•èƒ½å¤ŸæˆåŠŸå­¦ä¹ å¯¹CVaRæ•æ„Ÿçš„ç­–ç•¥ï¼Œå¹¶å®ç°æœ‰æ•ˆçš„æ€§èƒ½ä¸å®‰å…¨æƒè¡¡ã€‚

**å…³é”®è¯**ï¼šå¥–åŠ±å†åˆ†é…, CVaR, é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹, é£é™©è§„é¿, å€¼è¿­ä»£, å¼ºåŒ–å­¦ä¹ , Bellmanç®—å­, ç¨ å¯†å¥–åŠ±, è¿‘ä¼¼è¯¯å·®ç•Œé™

**è¯„åˆ†**ï¼š54

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03778v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03778v1.pdf)

---

## [27. Decision-oriented benchmarking to transform AI weather forecast access: Application to the Indian monsoon](https://arxiv.org/abs/2602.03767v1)

**ä½œè€…**ï¼šRajat Masiwal, Colin Aitken, Adam Marchakitus ç­‰ 12 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, econ.GN, physics.ao-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Artificial intelligence weather prediction (AIWP) models now often outperform traditional physics-based models on common metrics while requiring orders-of-magnitude less computing resources and time. Open-access AIWP models thus hold promise as transformational tools for helping low- and middle-income populations make decisions in the face of high-impact weather shocks. Yet, current approaches to evaluating AIWP models focus mainly on aggregated meteorological metrics without considering local stakeholders' needs in decision-oriented, operational frameworks. Here, we introduce such a framework that connects meteorology, AI, and social sciences. As an example, we apply it to the 150-year-old problem of Indian monsoon forecasting, focusing on benefits to rain-fed agriculture, which is highly susceptible to climate change. AIWP models skillfully predict an agriculturally relevant onset index at regional scales weeks in advance when evaluated out-of-sample using deterministic and probabilistic metrics. This framework informed a government-led effort in 2025 to send 38 million Indian farmers AI-based monsoon onset forecasts, which captured an unusual weeks-long pause in monsoon progression. This decision-oriented benchmarking framework provides a key component of a blueprint for harnessing the power of AIWP models to help large vulnerable populations adapt to weather shocks in the face of climate variability and change.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å†³ç­–å¯¼å‘çš„åŸºå‡†è¯„ä¼°æ¡†æ¶ï¼Œä»¥æå‡AIå¤©æ°”é¢„æŠ¥åœ¨å°åº¦å­£é£é¢„æµ‹ä¸­çš„åº”ç”¨ï¼Œå¸®åŠ©å†œæ°‘åº”å¯¹æ°”å€™å˜åŒ–å¸¦æ¥çš„å½±å“ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰çš„AIå¤©æ°”é¢„æŠ¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œä½†è¯„ä¼°æ–¹æ³•æœªèƒ½æ»¡è¶³å½“åœ°åˆ©ç›Šç›¸å…³è€…çš„å†³ç­–éœ€æ±‚ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç»“åˆæ°”è±¡å­¦ã€äººå·¥æ™ºèƒ½å’Œç¤¾ä¼šç§‘å­¦çš„å†³ç­–å¯¼å‘æ¡†æ¶ï¼Œå¹¶åº”ç”¨äºå°åº¦å­£é£çš„é¢„æµ‹ï¼Œç‰¹åˆ«å…³æ³¨å¯¹é›¨å…»å†œä¸šçš„å½±å“ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ¡†æ¶ä¸ºåˆ©ç”¨AIå¤©æ°”é¢„æŠ¥æ¨¡å‹å¸®åŠ©è„†å¼±äººç¾¤é€‚åº”æ°”å€™å˜åŒ–æä¾›äº†é‡è¦çš„å‚è€ƒï¼ŒæˆåŠŸåœ°ä¸º3800ä¸‡å†œæ°‘æä¾›äº†å­£é£é¢„æµ‹ä¿¡æ¯ã€‚

**å…³é”®è¯**ï¼šæ°”è±¡é¢„æµ‹, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, å†³ç­–å¯¼å‘, å†œä¸šé€‚åº”, AIå¤©æ°”é¢„æµ‹, é¢„æµ‹æ¨¡å‹, æ°”å€™å˜åŒ–, å†œæ°‘åŠ©æ‰‹

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03767v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03767v1.pdf)

---

## [28. Soft Sensor for Bottom-Hole Pressure Estimation in Petroleum Wells Using Long Short-Term Memory and Transfer Learning](https://arxiv.org/abs/2602.03737v1)

**ä½œè€…**ï¼šM. A. Fernandes, E. Gildin, M. A. Sampaio  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Monitoring bottom-hole variables in petroleum wells is essential for production optimization, safety, and emissions reduction. Permanent Downhole Gauges (PDGs) provide real-time pressure data but face reliability and cost issues. We propose a machine learning-based soft sensor to estimate flowing Bottom-Hole Pressure (BHP) using wellhead and topside measurements. A Long Short-Term Memory (LSTM) model is introduced and compared with Multi-Layer Perceptron (MLP) and Ridge Regression. We also pioneer Transfer Learning for adapting models across operational environments. Tested on real offshore datasets from Brazil's Pre-salt basin, the methodology achieved Mean Absolute Percentage Error (MAPE) consistently below 2\%, outperforming benchmarks. This work offers a cost-effective, accurate alternative to physical sensors, with broad applicability across diverse reservoir and flow conditions.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLSTMå’Œè¿ç§»å­¦ä¹ çš„è½¯ä¼ æ„Ÿå™¨ï¼Œç”¨äºå‡†ç¡®ä¼°è®¡çŸ³æ²¹äº•çš„åº•éƒ¨å‹åŠ›ï¼Œä¸”åœ¨å®é™…æ•°æ®æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç›‘æµ‹çŸ³æ²¹äº•åº•éƒ¨å˜é‡å¯¹äºä¼˜åŒ–ç”Ÿäº§ã€å®‰å…¨å’Œå‡å°‘æ’æ”¾è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ°¸ä¹…äº•ä¸‹ä¼ æ„Ÿå™¨å­˜åœ¨å¯é æ€§å’Œæˆæœ¬é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¼•å…¥é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰æ¨¡å‹ï¼Œå¹¶ä¸å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å’Œå²­å›å½’è¿›è¡Œæ¯”è¾ƒï¼ŒåŒæ—¶åº”ç”¨è¿ç§»å­¦ä¹ ä»¥é€‚åº”ä¸åŒçš„æ“ä½œç¯å¢ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•åœ¨å·´è¥¿é¢„ç›ç›†åœ°çš„å®é™…æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆMAPEï¼‰å§‹ç»ˆä½äº2%ï¼Œä¸ºç‰©ç†ä¼ æ„Ÿå™¨æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜ä¸”å‡†ç¡®çš„æ›¿ä»£æ–¹æ¡ˆã€‚

**å…³é”®è¯**ï¼šåº•éƒ¨å‹åŠ›, è½¯ä¼ æ„Ÿå™¨, æœºå™¨å­¦ä¹ , LSTM, è½¬ç§»å­¦ä¹ , å¤šå±‚æ„ŸçŸ¥å™¨, å‡†ç¡®æ€§, å®æ—¶ç›‘æµ‹, æ•°æ®é€‚åº”, machine learning

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03737v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03737v1.pdf)

---

## [29. Fast-MWEM: Private Data Release in Sublinear Time](https://arxiv.org/abs/2602.03732v1)

**ä½œè€…**ï¼šThemistoklis Haris, Steve Choi, Mutiraj Laksanawisit  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The Multiplicative Weights Exponential Mechanism (MWEM) is a fundamental iterative framework for private data analysis, with broad applications such as answering $m$ linear queries, or privately solving systems of $m$ linear constraints. However, a critical bottleneck hindering its scalability is the $Î˜(m)$ time complexity required to execute the exponential mechanism in each iteration. We introduce a modification to the MWEM framework that improves the per-iteration runtime dependency to $Î˜(\sqrt{m})$ in expectation. This is done via a lazy sampling approach to the Report-Noisy-Max mechanism, which we implement efficiently using Gumbel noise and a $k$-Nearest Neighbor data structure. This allows for the rapid selection of the approximate score in the exponential mechanism without an exhaustive linear scan. We apply our accelerated framework to the problems of private linear query release and solving Linear Programs (LPs) under neighboring constraint conditions and low-sensitivity assumptions. Experimental evaluation confirms that our method provides a substantial runtime improvement over classic MWEM.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFast-MWEMé€šè¿‡æ‡’é‡‡æ ·æ–¹æ³•å°†MWEMæ¡†æ¶çš„æ¯æ¬¡è¿­ä»£æ—¶é—´å¤æ‚åº¦é™ä½è‡³Î˜(âˆšm)ï¼Œæ˜¾è‘—æé«˜äº†ç§æœ‰æ•°æ®å‘å¸ƒçš„æ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šMWEMæ¡†æ¶åœ¨æ‰§è¡Œæ¯æ¬¡è¿­ä»£æ—¶éœ€è¦Î˜(m)çš„æ—¶é—´å¤æ‚åº¦ï¼Œå½±å“äº†å…¶å¯æ‰©å±•æ€§ï¼Œå› æ­¤éœ€è¦å¯»æ‰¾æ›´é«˜æ•ˆçš„å®ç°æ–¹å¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé‡‡ç”¨æ‡’é‡‡æ ·çš„Report-Noisy-Maxæœºåˆ¶ï¼Œç»“åˆGumbelå™ªå£°å’Œk-è¿‘é‚»æ•°æ®ç»“æ„ï¼Œä¼˜åŒ–æ¯æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´è‡³Î˜(âˆšm)ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒFast-MWEMåœ¨ç§æœ‰çº¿æ€§æŸ¥è¯¢å‘å¸ƒå’Œè§£å†³çº¿æ€§è§„åˆ’é—®é¢˜ä¸Šï¼Œç›¸è¾ƒäºç»å…¸MWEMæ–¹æ³•æ˜¾è‘—æå‡äº†è¿è¡Œæ•ˆç‡ã€‚

**å…³é”®è¯**ï¼šç§æœ‰æ•°æ®å‘å¸ƒ, å¤šé‡æƒé‡æŒ‡æ•°æœºåˆ¶, çº¿æ€§æŸ¥è¯¢, çº¿æ€§çº¦æŸ, Gumbelå™ªå£°, k-è¿‘é‚»æ•°æ®ç»“æ„, è¿‘ä¼¼è¯„åˆ†, æ•°æ®åˆ†æ, è¿­ä»£æ¡†æ¶, agent

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03732v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03732v1.pdf)

---

## [30. Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization](https://arxiv.org/abs/2602.03729v1)

**ä½œè€…**ï¼šHenrik Schopmans, Christopher von Klitzing, Pascal Friederich  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-03

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¦»çº¿æ”¿ç­–å¯¹æ•°æ•£å¸ƒæ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥æé«˜Boltzmannç”Ÿæˆå™¨çš„è®­ç»ƒæ•ˆç‡å’Œæ•°æ®åˆ©ç”¨ç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨è®¡ç®—ç§‘å­¦ä¸­ï¼Œä»æœªå½’ä¸€åŒ–æ¦‚ç‡å¯†åº¦ä¸­é‡‡æ ·æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ï¼Œè€ŒBoltzmannç”Ÿæˆå™¨åœ¨æ­¤è¿‡ç¨‹ä¸­ä¾èµ–äºé«˜æ•ˆçš„æ•°æ®è®­ç»ƒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºçš„ç¦»çº¿æ”¿ç­–å¯¹æ•°æ•£å¸ƒæ­£åˆ™åŒ–ï¼ˆLDRï¼‰åœ¨ä¸éœ€è¦é¢å¤–æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œç»“åˆæ ‡å‡†æ•°æ®è®­ç»ƒç›®æ ‡ï¼Œä»¥æ”¹å–„èƒ½é‡æ™¯è§‚çš„å½¢çŠ¶ã€‚

**ä¸»è¦ç»“è®º**ï¼šLDRåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½æé«˜äº†æœ€ç»ˆæ€§èƒ½å’Œæ•°æ®æ•ˆç‡ï¼Œæ ·æœ¬æ•ˆç‡æå‡å¯è¾¾ä¸€ä¸ªæ•°é‡çº§ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆæ¨¡å‹, é‡‡æ ·, è®­ç»ƒ, æ­£åˆ™åŒ–, Boltzmannç”Ÿæˆå™¨, æ•°æ®æ•ˆç‡, èƒ½é‡æ ‡ç­¾, ç¦»çº¿å­¦ä¹ , ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–, ç‰©ç†ç³»ç»Ÿ, generative

**è¯„åˆ†**ï¼š69

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.03729v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.03729v1.pdf)

---

