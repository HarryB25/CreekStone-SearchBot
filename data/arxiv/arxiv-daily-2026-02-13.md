# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-13

> å…± 30 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (7 ç¯‡)
- [cs.LG](#csLG) (14 ç¯‡)
- [cs.AI](#csAI) (6 ç¯‡)
- [cs.CL](#csCL) (3 ç¯‡)

---

## cs.AI

## [1. CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use](https://arxiv.org/abs/2602.12268v1)

**ä½œè€…**ï¼šZhen Zhang, Kaiqiang Song, Xun Wang ç­‰ 14 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCM2 æå‡ºç”¨â€œæ£€æŸ¥å•å¼å¥–åŠ±â€æ›¿ä»£å¯éªŒè¯ç»“æœå¥–åŠ±ï¼Œåœ¨æ¨¡æ‹Ÿå·¥å…·ç¯å¢ƒä¸­å¯¹å¤šè½®å¤šæ­¥å·¥å…·å‹æ™ºèƒ½ä½“è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¹¶æ˜¾è‘—ä¼˜äºçº¯ç›‘ç£å¾®è°ƒã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°å®å¤šè½®å·¥å…·ä½¿ç”¨ä»»åŠ¡ç›®æ ‡å¼€æ”¾ã€ç¼ºä¹å¯éªŒè¯å¥–åŠ±ï¼Œç°æœ‰ RL éš¾ä»¥ç¨³å®šè®­ç»ƒå¤šæ­¥ agentï¼Œä¸”çœŸå®å¯æ‰§è¡Œå·¥å…·ç¯å¢ƒæ˜‚è´µéš¾æ‰©å±•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†æ¯è½®æœŸæœ›è¡Œä¸ºæ‹†è§£ä¸ºç»†ç²’åº¦äºŒå…ƒâ€œæ£€æŸ¥é¡¹â€ï¼Œé…æœ‰è¯æ®å’Œç»“æ„åŒ–å…ƒæ•°æ®ï¼Œç”¨ç¨€ç–å¥–åŠ±+å¯†é›†è¯„ä¼°æ ‡å‡†çš„ç­–ç•¥åœ¨ LLM æ¨¡æ‹Ÿçš„å·¥å…·ç¯å¢ƒä¸­å¯¹ 8B æ¨¡å‹è¿›è¡Œ RL è®­ç»ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ tau^-Benchã€BFCL-V4 å’Œ ToolSandbox ç­‰åŸºå‡†ä¸Šï¼ŒCM2 ç›¸æ¯” SFT åˆ†åˆ«æå‡ 8/10/12 åˆ†ï¼Œè¾¾åˆ°æˆ–è¶…è¶ŠåŒè§„æ¨¡å¼€æºåŸºçº¿å’Œè¯„åˆ¤æ¨¡å‹ï¼Œå±•ç¤ºäº†æ— éœ€å¯éªŒè¯ç»“æœå¥–åŠ±å³å¯å¯æ‰©å±•åœ°ä¼˜åŒ–å¤šè½®å¤šæ­¥å·¥å…·æ™ºèƒ½ä½“çš„å¯è¡Œè·¯å¾„ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , LLM, agenticworkflow, å¤šè½®å¯¹è¯å·¥å…·è°ƒç”¨, æ¸…å•å¼å¥–åŠ±è®¾è®¡, LLMæ¨¡æ‹Ÿç¯å¢ƒ, å·¥å…·ä½¿ç”¨ç­–ç•¥ä¼˜åŒ–, RLCM2æ¡†æ¶

**è¯„åˆ†**ï¼š65

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12268v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12268v1.pdf)

---

## [2. Think like a Scientist: Physics-guided LLM Agent for Equation Discovery](https://arxiv.org/abs/2602.12259v1)

**ä½œè€…**ï¼šJianke Yang, Ohm Venkatachalam, Mohammad Kianezhad ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºKeplerAgentï¼Œä¸€ä¸ªè®©LLMåƒç‰©ç†å­¦å®¶ä¸€æ ·å…ˆæ¨ç†ç‰©ç†æ€§è´¨å†åšç¬¦å·å›å½’çš„ä»£ç†æ¡†æ¶ï¼Œåœ¨ç‰©ç†æ–¹ç¨‹å‘ç°ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡å‡†ç¡®ç‡å’ŒæŠ—å™ªæ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰LLMæ–¹ç¨‹å‘ç°æ–¹æ³•å¤šç›´æ¥ä»æ•°æ®â€œçŒœå…¬å¼â€ï¼Œå¿½ç•¥ç§‘å­¦å®¶å¸¸ç”¨çš„åˆ†æ­¥ç‰©ç†æ¨ç†æµç¨‹ï¼Œå¯¼è‡´æœç´¢ç©ºé—´è¿‡å¤§ã€å¯¹å™ªå£°æ•æ„Ÿä¸”ç¼ºä¹ç‰©ç†çº¦æŸã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šKeplerAgentç”¨LLMåè°ƒä¸€ç³»åˆ—ç‰©ç†å·¥å…·ï¼Œå…ˆä»æ•°æ®ä¸­æ¨æ–­å¯¹ç§°æ€§ç­‰ç‰©ç†ç»“æ„ï¼Œå†æ®æ­¤é…ç½®PySINDyã€PySRç­‰ç¬¦å·å›å½’å™¨çš„å‡½æ•°åº“å’Œç»“æ„çº¦æŸï¼Œå®ç°ç‰©ç†å…ˆéªŒå¼•å¯¼çš„å…¬å¼æœç´¢ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šä¸ªç‰©ç†æ–¹ç¨‹åŸºå‡†ä¸Šï¼ŒKeplerAgentåœ¨ç¬¦å·å‡†ç¡®ç‡å’Œå™ªå£°é²æ£’æ€§ä¸Šéƒ½ä¼˜äºä¼ ç»Ÿç¬¦å·å›å½’å’Œç›´æ¥LLMç”Ÿæˆæ–¹æ¡ˆï¼Œè¡¨æ˜æ˜¾å¼æ¨¡ä»¿ç§‘å­¦æ¨ç†è¿‡ç¨‹èƒ½æ˜¾è‘—æå‡æ–¹ç¨‹å‘ç°èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šLLM, å¤§è¯­è¨€æ¨¡å‹, agenticæ¡†æ¶, ç‰©ç†å¼•å¯¼ç¬¦å·å›å½’, æ–¹ç¨‹å‘ç°, ç§‘å­¦æ¨ç†, å¤šæ­¥æ¨ç†, ç‰©ç†å…ˆéªŒçº¦æŸ, å™ªå£°é²æ£’æ€§

**è¯„åˆ†**ï¼š63

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12259v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12259v1.pdf)

---

## [3. "Sorry, I Didn't Catch That": How Speech Models Miss What Matters Most](https://arxiv.org/abs/2602.12249v1)

**ä½œè€…**ï¼šKaitlyn Zhou, Martijn Bartelds, Federico Bianchi ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL, cs.CY  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡å‘ç°å½“å‰ä¸»æµè¯­éŸ³è¯†åˆ«åœ¨çœŸå®åœºæ™¯ä¸­å¯¹è¡—é“åç­‰çŸ­ä¸”é«˜é£é™©è¯­å¥é”™è¯¯ç‡æé«˜ï¼Œå¹¶æå‡ºç”¨å°‘é‡åˆæˆæ•°æ®å¾®è°ƒå³å¯æ˜¾è‘—æ”¹å–„ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ ‡å‡†åŸºå‡†ä¸Šçš„ä½è¯é”™è¯¯ç‡æ©ç›–äº†çœŸå®åº”ç”¨ä¸­å¯¹å…³é”®ä¸“æœ‰åè¯ï¼ˆå¦‚è¡—é“åï¼‰å’Œå°‘æ•°æ—è£”/éè‹±è¯­æ¯è¯­è€…çš„é«˜å¤±è´¥ç‡ï¼Œä¸”è¿™äº›é”™è¯¯ä¼šå¸¦æ¥ä¸¥é‡å¯¼èˆªä¸å…¬å¹³æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…é‡‡é›†å¤šè¯­è¨€èƒŒæ™¯çš„ç¾å›½è¯´è¯äººè¯»ç¾å›½è¡—é“åçš„è¯­éŸ³æ•°æ®ï¼Œè¯„ä¼°å››å®¶å‚å•†15ä¸ªæ¨¡å‹çš„è½¬å†™è¡¨ç°åŠå…¶åœ¨åœ°ç†è·¯ç”±ä¸Šçš„è¯¯å·®å½±å“ï¼Œå¹¶ä½¿ç”¨å¼€æºTTSåˆæˆå¤šæ ·åŒ–è¡—é“åå‘éŸ³ï¼ŒåŸºäºä¸è¶³1000æ¡åˆæˆæ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šç°æœ‰è¯­éŸ³æ¨¡å‹åœ¨è¡—é“åè½¬å†™ä¸Šçš„å¹³å‡é”™è¯¯ç‡é«˜è¾¾44%ï¼Œå¯¹éè‹±è¯­æ¯è¯­è€…é€ æˆçº¦ä¸¤å€çš„è·¯ç”±è·ç¦»è¯¯å·®ï¼›è€Œé€šè¿‡ç®€å•çš„åˆæˆå‘éŸ³æ•°æ®å¾®è°ƒï¼Œå¯å°†éè‹±è¯­æ¯è¯­è€…è¡—é“åè¯†åˆ«å‡†ç¡®ç‡ç›¸å¯¹æå‡è¿‘60%ï¼Œè¡¨æ˜å½“å‰åŸºå‡†ä¸çœŸå®å¯é æ€§å­˜åœ¨å…³é”®é¸¿æ²Ÿä¸”æœ‰å¯æ‰©å±•çš„ç¼“è§£è·¯å¾„ã€‚

**å…³é”®è¯**ï¼šè¯­éŸ³è¯†åˆ«, æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, ç”Ÿæˆå¼æ•°æ®å¢å¼º, åˆæˆè¯­éŸ³æ•°æ®, å‘½åå®ä½“è¯†åˆ«, å¤šè¯­è¨€å£éŸ³é²æ£’æ€§, é«˜é£é™©åœºæ™¯è½¬å½•, rag

**è¯„åˆ†**ï¼š38

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12249v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12249v1.pdf)

---

## [4. SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation](https://arxiv.org/abs/2602.12173v1)

**ä½œè€…**ï¼šChengxi Zeng, Yuxuan Jiang, Ge Gao ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSAM3-LiteTexté€šè¿‡åˆ†æåˆ†å‰²åœºæ™¯ä¸­çš„æ–‡æœ¬æç¤ºå†—ä½™ï¼Œæå‡ºç”¨è’¸é¦å¾—åˆ°çš„å°å‹MobileCLIPæ–‡æœ¬ç¼–ç å™¨æ›¿æ¢åŸSAM3æ–‡æœ¬ç¼–ç å™¨ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±åˆ†å‰²ç²¾åº¦çš„å‰æä¸‹å¤§å¹…é™ä½å‚æ•°ä¸å†…å­˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¦‚SAM3çš„è§†è§‰-è¯­è¨€åˆ†å‰²æ¨¡å‹æ²¿ç”¨å¤§è€Œé€šç”¨çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½†å®é™…åˆ†å‰²æç¤ºå¾€å¾€çŸ­å°ã€ç»“æ„åŒ–ä¸”è¯­ä¹‰ç©ºé—´å—é™ï¼Œå¯¼è‡´æ–‡æœ¬ç¼–ç å™¨ä¸¥é‡è¶…é…å¹¶å¸¦æ¥ä¸å¿…è¦çš„è®¡ç®—å’Œæ˜¾å­˜å¼€é”€ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…é¦–å…ˆå¯¹40ä¸‡ä½™æ¡çœŸå®åˆ†å‰²æç¤ºè¿›è¡Œâ€œè§£å‰–å¼â€ç»Ÿè®¡åˆ†æï¼Œé‡åŒ–ä¸Šä¸‹æ–‡çª—å£åˆ©ç”¨ç‡ã€è¯æ±‡ç¨€ç–æ€§å’ŒåµŒå…¥æµå½¢ç»´åº¦ç­‰å†—ä½™ç°è±¡ï¼›åœ¨æ­¤åŸºç¡€ä¸Šè®¾è®¡SAM3-LiteTextæ¡†æ¶ï¼Œç”¨æ›´å°çš„MobileCLIPä½œä¸ºå­¦ç”Ÿæ¨¡å‹å¯¹åŸSAM3æ–‡æœ¬ç¼–ç å™¨è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå¹¶æ— ç¼æ›¿æ¢è¿›SAM3ä»¥å®ç°é«˜æ•ˆæ–‡æœ¬ç¼–ç ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒSAM3-LiteTextåœ¨å¤šä¸ªå›¾åƒä¸è§†é¢‘åˆ†å‰²åŸºå‡†ä¸Šå‡ ä¹ä¿æŒåŸæœ‰åˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œå°†æ–‡æœ¬ç¼–ç å™¨å‚æ•°é‡æœ€å¤šå‹ç¼©88%ï¼Œæ˜¾è‘—é™ä½é™æ€å†…å­˜å ç”¨ï¼Œè¯æ˜é’ˆå¯¹åˆ†å‰²æç¤ºå®šåˆ¶è½»é‡æ–‡æœ¬ç¼–ç å™¨æ—¢é«˜æ•ˆåˆå®ç”¨ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, transformer, æ–‡æœ¬ç¼–ç è’¸é¦, è½»é‡çº§æ–‡æœ¬ç¼–ç å™¨, è§†è§‰è¯­è¨€åˆ†å‰², MobileCLIPå­¦ç”Ÿæ¨¡å‹, çŸ¥è¯†è’¸é¦è®­ç»ƒ, å›¾åƒåˆ†å‰²åŸºå‡†, è§†é¢‘åˆ†å‰²åŸºå‡†

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12173v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12173v1.pdf)

---

## [5. Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation](https://arxiv.org/abs/2602.12172v1)

**ä½œè€…**ï¼šBowei He, Yankai Chen, Xiaokun Zhang ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºä¸€ä¸ªå—æ•™è‚²å­¦å¯å‘çš„ä¸‰é˜¶æ®µçŸ¥è¯†è’¸é¦æ¡†æ¶ IOAï¼Œé€šè¿‡è¯Šæ–­å­¦ç”Ÿæ¨¡å‹è–„å¼±ç‚¹ã€è®¾è®¡æ¸è¿›å­¦ä¹ è·¯å¾„å¹¶é€‚é…éš¾åº¦ï¼Œä½¿å°æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šæ¥è¿‘å¤§æ¨¡å‹æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŸºäºåˆæˆæ•°æ®çš„è’¸é¦æ–¹æ³•ç¼ºä¹â€œæ•™å­¦æ³•â€è§†è§’ï¼Œå°†çŸ¥è¯†è¿ç§»è§†ä¸ºä¸€æ¬¡æ€§æ•°æ®ç”Ÿæˆå’Œè®­ç»ƒï¼Œå¯¼è‡´çŸ¥è¯†ä¼ é€’æ•ˆç‡ä½ã€å¯¹å¤æ‚æ¨ç†èƒ½åŠ›æå‡æœ‰é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ¡†æ¶ç”±çŸ¥è¯†è¯†åˆ«å™¨ï¼ˆè¯†åˆ«å­¦ç”Ÿä¸æ•™å¸ˆçš„çŸ¥è¯†å·®è·ï¼‰ã€ç»„ç»‡å™¨ï¼ˆåŸºäºå…ˆä¿®çŸ¥è¯†ä¸éš¾åº¦é€’è¿›æ„å»ºè¯¾ç¨‹ï¼‰å’Œé€‚é…å™¨ï¼ˆå°†æ•™å¸ˆè¾“å‡ºè½¬æ¢æˆé€‚åˆå­¦ç”Ÿè®¤çŸ¥å®¹é‡çš„è¡¨ç¤ºï¼‰ä¸‰éƒ¨åˆ†ç»„æˆï¼Œå¹¶ç»“åˆâ€œæŒæ¡å­¦ä¹ â€å’Œâ€œæœ€è¿‘å‘å±•åŒºâ€ï¼ŒåŠ¨æ€æ§åˆ¶å­¦ä¹ é¡ºåºä¸éš¾åº¦å‡çº§ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ LLaMA-3.1/3.2 å’Œ Qwen2.5 ç­‰å­¦ç”Ÿæ¨¡å‹ä¸Šï¼ŒIOA åœ¨ä¿æŒä¸åˆ°ååˆ†ä¹‹ä¸€å‚æ•°è§„æ¨¡çš„å‰æä¸‹ï¼Œåœ¨ DollyEval ä¸Šä¿ç•™äº† 94.7% çš„æ•™å¸ˆæ€§èƒ½ï¼Œå¹¶åœ¨ MATH å’Œ HumanEval ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šåˆ†åˆ«è¾ƒç°æœ‰è’¸é¦æ–¹æ³•æå‡çº¦ 19.2% å’Œ 22.3%ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, çŸ¥è¯†è’¸é¦, åˆæˆæ•°æ®, æ•™å¸ˆå­¦ç”Ÿæ¨¡å‹, è¯¾ç¨‹å­¦ä¹ , åˆ†é˜¶æ®µè®­ç»ƒ, å¤æ‚æ¨ç†èƒ½åŠ›æå‡, æ•™è‚²å­¦å¯å‘æ–¹æ³•, æ¨¡å‹å‹ç¼©, å‚æ•°é«˜æ•ˆéƒ¨ç½², llm

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12172v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12172v1.pdf)

---

## [6. Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision](https://arxiv.org/abs/2602.12164v1)

**ä½œè€…**ï¼šXiaohan He, Shiyang Feng, Songtao Huang ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSci-CoEæå‡ºä¸€ä¸ªä¸¤é˜¶æ®µâ€œè‡ªæ¼”åŒ–â€æ¡†æ¶ï¼Œè®©åŒä¸€ä¸ªLLMåŒæ—¶è¿›åŒ–æˆæ›´å¼ºçš„ç§‘å­¦æ±‚è§£å™¨å’ŒéªŒè¯å™¨ï¼Œåœ¨æå°‘æ ‡æ³¨ä¸å¤§é‡æ— æ ‡æ³¨æ•°æ®ä¸Šæ˜¾è‘—æå‡ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰LLMåœ¨ç§‘å­¦æ¨ç†ä¸­æ˜“å‡ºé”™ï¼Œä¸»è¦ç”±äºç¼ºä¹å¯é çš„è‡ªåŠ¨åˆ¤åˆ†æœºåˆ¶å’Œå¤šæ ·åŒ–éªŒè¯ç­–ç•¥ï¼Œå¯¼è‡´éš¾ä»¥åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨ç§‘å­¦æ•°æ®ä¸Šç¨³å®šè‡ªè®­ç»ƒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç¬¬ä¸€é˜¶æ®µç”¨å°‘é‡æ ‡æ³¨æ•°æ®è®­ç»ƒVerifierï¼Œå»ºç«‹åŸºç¡€æ­£ç¡®æ€§åˆ¤æ–­é”šç‚¹ï¼›ç¬¬äºŒé˜¶æ®µåœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ä¸Šï¼Œå¼•å…¥åŒæ—¶è€ƒè™‘ç­”æ¡ˆå…±è¯†åº¦ã€åˆ¤æ–­å¯é æ€§å’Œè§£æ³•å¤šæ ·æ€§çš„å‡ ä½•å¼å¥–åŠ±æœºåˆ¶ï¼Œé©±åŠ¨æ±‚è§£å™¨ä¸éªŒè¯å™¨è”åˆè‡ªè¿­ä»£æ¼”åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§é€šç”¨ç§‘å­¦æ¨ç†åŸºå‡†ä¸Šï¼ŒSci-CoEæ˜¾è‘—æå‡å¤æ‚æ¨ç†è¡¨ç°å¹¶å…·å¤‡è‰¯å¥½æ‰©å±•æ€§ï¼Œèƒ½æ„å»ºæ›´é²æ£’ä¸”å¤šæ ·åŒ–çš„è‡ªåŠ¨è¯„æµ‹ä½“ç³»ï¼Œä¸ºç§‘å­¦é¢†åŸŸLLMè‡ªç›‘ç£æ¼”åŒ–æä¾›æœ‰æ•ˆè·¯å¾„ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, ç§‘å­¦æ¨ç†, co-evolvingæ¡†æ¶, Verifierè‡ªåšå¼ˆ, å‡ ä½•å¥–åŠ±æœºåˆ¶, ç¨€ç–ç›‘ç£, æ— ç›‘ç£è‡ªè¿­ä»£, å¤šæ ·æ€§éªŒè¯ç­–ç•¥, è‡ªç›‘ç£è¯„ä¼°ç³»ç»Ÿ, llm

**è¯„åˆ†**ï¼š48

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12164v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12164v1.pdf)

---

## cs.CL

## [7. ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images](https://arxiv.org/abs/2602.12203v1)

**ä½œè€…**ï¼šMathieu Sibue, Andres MuÃ±oz Garza, Samuel Mensah ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šExStrucTiny æå‡ºä¸€ä¸ªé¢å‘å¤šç§æ–‡æ¡£ç±»å‹ã€å¯å˜æ¨¡å¼(schema-variable)çš„ç»“æ„åŒ–ä¿¡æ¯æŠ½å–åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„æµ‹é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡æ¡£ä¿¡æ¯æŠ½å–ä¸Šçš„çœŸå®èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ–‡æ¡£ç†è§£æ•°æ®é›†å¤šèšç„¦äºå›ºå®šå®ä½“ç±»åˆ«ã€ç®€å•é—®ç­”æˆ–å•ä¸€æ–‡æ¡£ç±»å‹ï¼Œéš¾ä»¥åæ˜ ä¼ä¸šæ–‡æ¡£ä¸­å¤šæ ·ä¸”å¯å˜çš„ç»“æ„åŒ–æŠ½å–éœ€æ±‚ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªæ›´è´´è¿‘å®é™…åº”ç”¨ã€å…¼é¡¾ KEE/RE/VQA çš„ç»Ÿä¸€åŸºå‡†ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…è®¾è®¡äº†ä¸€æ¡ç»“åˆäººå·¥æ ‡æ³¨ä¸åˆæˆæ•°æ®å¹¶ç”±äººæ ¡éªŒçš„æ„å»ºç®¡çº¿ï¼Œç”Ÿæˆè¦†ç›–å¤šæ–‡æ¡£ç±»å‹ã€å¤šæŠ½å–åœºæ™¯çš„ ExStrucTiny æ•°æ®é›†ï¼Œå°†å…³é”®å®ä½“æŠ½å–ã€å…³ç³»æŠ½å–ä¸è§†è§‰é—®ç­”æ•´åˆä¸ºå¯å˜ schema çš„ç»“æ„åŒ–ä¿¡æ¯æŠ½å–ä»»åŠ¡ï¼Œå¹¶åœ¨å…¶ä¸Šç³»ç»Ÿè¯„æµ‹å¼€æºä¸é—­æº VLMã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜å½“å‰é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨¡å¼è‡ªé€‚åº”ã€åœ¨ä¿¡æ¯ä¸å®Œå…¨å®šä¹‰æ—¶çš„æŸ¥è¯¢ç†è§£ä»¥åŠç­”æ¡ˆåœ¨æ–‡æ¡£ä¸­çš„ç²¾ç¡®å®šä½ç­‰æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾æŒ‘æˆ˜ï¼ŒExStrucTiny å¯ä½œä¸ºæ¨åŠ¨é¢å‘æ–‡æ¡£ç»“æ„åŒ–ä¿¡æ¯æŠ½å–çš„é€šç”¨æ¨¡å‹ç ”ç©¶å’Œæ”¹è¿›çš„åŸºç¡€åŸºå‡†ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹, ä¿¡æ¯æŠ½å–, æ–‡æ¡£å›¾åƒç†è§£, ç»“æ„åŒ–è§£æ, æ¨¡å¼è‡ªé€‚åº”, é—®ç­”å¼æ ‡æ³¨, ä¼ä¸šæ–‡æ¡£å¤„ç†, workflow

**è¯„åˆ†**ï¼š36

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12203v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12203v1.pdf)

---

## [8. Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education](https://arxiv.org/abs/2602.12196v1)

**ä½œè€…**ï¼šMohamed Huti, Alasdair Mackintosh, Amy Waldock ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªæºè‡ªçœŸå®å°å­¦è€ƒè¯•çš„è§†è§‰æ¨ç†åŸºå‡† VRBï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨è¯¾å ‚çœŸå®è§†è§‰é¢˜ç›®ä¸Šçš„èƒ½åŠ›è¾¹ç•Œã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†ä¸Šè¡¨ç°çªå‡ºï¼Œä½†åœ¨å°å­¦æ•°å­¦ç­‰é«˜åº¦ä¾èµ–å›¾å½¢ã€ç©ºé—´å’Œå…³ç³»æ¨ç†çš„çœŸå®è¯¾å ‚åœºæ™¯ä¸­è¡¨ç°æœªçŸ¥ä¸”å¯èƒ½å­˜åœ¨é£é™©ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªæ•™è‚²åœºæ™¯åŸæ±åŸå‘³çš„è§†è§‰æ¨ç†åŸºå‡†æ¥è¯„ä¼°å…¶å®é™…å¯ç”¨æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ä»èµæ¯”äºšå’Œå°åº¦çš„å°å­¦è€ƒè¯•ä¸­æ”¶é›†701é“åŸå§‹è§†è§‰é¢˜ç›®ï¼Œæ¶µç›–ç±»æ¯”æ¨ç†ã€æ¨¡å¼è¡¥å…¨ã€ç©ºé—´åŒ¹é…ç­‰ï¼Œå¤šä¿æŒæœ€å°‘æ–‡å­—å’ŒåŸå§‹ç‰ˆå¼ï¼Œæ„å»ºVRBåŸºå‡†å¹¶ç³»ç»Ÿæµ‹è¯•å¤šç§MLLMåœ¨ä¸åŒé¢˜å‹ä¸Šçš„è¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒå‘ç°æ¨¡å‹åœ¨è®¡æ•°ã€ç¼©æ”¾ç­‰é™æ€æŠ€èƒ½ä¸Šè¡¨ç°å°šå¯ï¼Œä½†åœ¨æŠ˜å ã€ç¿»è½¬ã€æ—‹è½¬ç­‰åŠ¨æ€ç©ºé—´æ“ä½œä¸Šå­˜åœ¨æ˜æ˜¾â€œç©ºé—´å¤©èŠ±æ¿â€ï¼Œå‘ˆç°èƒ½åŠ›å‚å·®çš„â€œé”¯é½¿è¾¹ç•Œâ€ï¼Œè¿™åœ¨çœŸå®è¯¾å ‚ä½¿ç”¨ä¸­å¯èƒ½å¯¼è‡´é”™è¯¯è¯„åˆ†ä¸è¯¯å¯¼å­¦ç”Ÿï¼Œå› è€Œå¼ºè°ƒéœ€è¦æ•™è‚²ä¸“ç”¨çš„åŸºå‡†æ¥åˆ’å®šå’Œç†è§£å¤šæ¨¡æ€å·¥å…·åœ¨æ•™å­¦ä¸­çš„åŠŸèƒ½è¾¹ç•Œã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹, è§†è§‰æ¨ç†, ç¥ç»ç½‘ç»œ, transformer, å°å­¦æ•°å­¦æ•™è‚², ç©ºé—´å…³ç³»ç†è§£, å›¾åƒé¢˜è‡ªåŠ¨è¯„åˆ†, æ•™è‚²è¯„æµ‹åŸºå‡†

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12196v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12196v1.pdf)

---

## [9. Query-focused and Memory-aware Reranker for Long Context Processing](https://arxiv.org/abs/2602.12192v1)

**ä½œè€…**ï¼šYuqing Li, Jiangnan Li, Mo Yu ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§åˆ©ç”¨å¤§æ¨¡å‹æ£€ç´¢æ³¨æ„åŠ›å¤´ã€é¢å‘æŸ¥è¯¢ä¸”å…·è®°å¿†æ„ŸçŸ¥çš„åˆ—è¡¨å¼é‡æ’æ¡†æ¶ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¶…è¿‡ç°æœ‰ç‚¹å¼å’Œåˆ—å¼é‡æ’å™¨ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰é‡æ’å™¨éš¾ä»¥é«˜æ•ˆåˆ©ç”¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„æ•´ä½“å€™é€‰åˆ—è¡¨ä¿¡æ¯ï¼Œä¸”å¸¸ä¾èµ–äººå·¥Likertæ ‡æ³¨ã€æ¨¡å‹è§„æ¨¡å¤§å’Œå¯¹é•¿å¯¹è¯è®°å¿†çš„å¤„ç†ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»é€‰å®šæ³¨æ„åŠ›å¤´ä¸­æå–passage-queryæ³¨æ„åŠ›åˆ†æ•°ï¼Œæ„å»ºèƒ½å¤Ÿè¾“å‡ºè¿ç»­ç›¸å…³åº¦çš„åˆ—è¡¨å¼é‡æ’æ¨¡å‹ï¼›åœ¨4Bè§„æ¨¡æ¨¡å‹ä¸Šè®­ç»ƒï¼Œå¹¶å¯æ‰©å±•ä¸ºåŠ å…¥ä¸Šä¸‹æ–‡å¢å¼ºå€™é€‰ç‰‡æ®µæˆ–ä½¿ç”¨ä¸­å±‚æ³¨æ„åŠ›å¤´ä»¥æé«˜æ•ˆç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•åœ¨Wikipediaã€é•¿å™äº‹æ•°æ®é›†å’ŒLoCoMoé•¿å¯¹è¯è®°å¿†åŸºå‡†ä¸Šå‡å–å¾—SOTAè¡¨ç°ï¼Œåœ¨ä¿æŒæ¨¡å‹è½»é‡çš„åŒæ—¶æå‡é•¿ä¸Šä¸‹æ–‡ç†è§£ä¸è®°å¿†åˆ©ç”¨èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºå‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, æ·±åº¦å­¦ä¹ , æ£€ç´¢å¢å¼ºç”Ÿæˆ, é•¿ä¸Šä¸‹æ–‡å¤„ç†, æ³¨æ„åŠ›å¤´é‡æ’åº, æŸ¥è¯¢ç›¸å…³æ€§å»ºæ¨¡, å¯¹è¯è®°å¿†ç†è§£, LoCoMoåŸºå‡†æµ‹è¯•, rag

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12192v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12192v1.pdf)

---

## cs.CV

## [10. Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching](https://arxiv.org/abs/2602.12280v1)

**ä½œè€…**ï¼šHuai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§åœ¨çŸ¢é‡ç´ æä¸­ï¼Œé€šè¿‡é€æ­¥æ·»åŠ ç¬”ç”»è®©åŒä¸€å›¾åƒåœ¨ä¸åŒé˜¶æ®µå‘ˆç°æˆªç„¶ä¸åŒè¯­ä¹‰ï¼ˆå¦‚ä»é¸­åˆ°ç¾Šï¼‰çš„ç”Ÿæˆæ¡†æ¶ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è§†è§‰é”™è§‰å¤šä¾èµ–ç©ºé—´å¸ƒå±€ä¸å¤šè§†è§’ï¼Œè€Œç¼ºä¹åœ¨â€œéšæ—¶é—´ç»˜åˆ¶è¿‡ç¨‹â€ä¸­å®ç°è¯­ä¹‰é”™ä½ä¸æ¸è¿›å¼è§†è§‰åŒå…³çš„ç³»ç»Ÿæ–¹æ³•ï¼Œå› æ­¤ä½œè€…å¸Œæœ›åœ¨çŸ¢é‡è‰å›¾é¢†åŸŸæ„å»ºå¯æ§çš„â€œæ—¶é—´ç»´åº¦è§†è§‰å­—è°œâ€ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º Stroke of Surprise æ¡†æ¶ï¼Œå°†è‰å›¾ç¬”ç”»å»ºæ¨¡ä¸ºå¯ä¼˜åŒ–çš„çŸ¢é‡åºåˆ—ï¼Œè®¾è®¡åŒåˆ†æ”¯çš„ Score Distillation Sampling å¯¹å‰ç¼€ç›®æ ‡å’Œæœ€ç»ˆç›®æ ‡åŒæ—¶æ–½åŠ çº¦æŸï¼Œå¹¶é€šè¿‡åºåˆ—æ„ŸçŸ¥çš„è”åˆä¼˜åŒ–åŠ¨æ€è°ƒæ•´å‰ç¼€ç¬”ç”»ä»¥å¯»æ‰¾ä¸¤ç§è¯­ä¹‰çš„å…¬å…±ç»“æ„å­ç©ºé—´ï¼ŒåŒæ—¶å¼•å…¥ Overlay Loss ä¿è¯æ–°å¢ç¬”ç”»ä¸å·²æœ‰ç»“æ„æ˜¯äº’è¡¥èåˆè€Œéé®æŒ¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨è‰å›¾å¯è¯†åˆ«åº¦å’Œâ€œè¯­ä¹‰é”™è§‰å¼ºåº¦â€ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒæˆåŠŸå°†è§†è§‰å˜ä½å­—çš„æ¦‚å¿µä»çº¯ç©ºé—´æ‰©å±•åˆ°æ—¶é—´ç»´åº¦çš„ç»˜åˆ¶è¿‡ç¨‹ï¼Œä¸ºç”Ÿæˆå¼ç»˜ç”»ä¸äº¤äº’å¼è§†è§‰è¡¨è¾¾æä¾›äº†æ–°çš„èŒƒå¼ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆå¼æ¨¡å‹, æ·±åº¦å­¦ä¹ , æ‰©æ•£æ¨¡å‹, ScoreDistillationSampling, å‘é‡ç´ æ, æ¸è¿›å¼è¯­ä¹‰é”™è§‰, åŒåˆ†æ”¯è”åˆä¼˜åŒ–, ç»“æ„å­ç©ºé—´, OverlayLoss, è§†è§‰å¹»è§‰ç”Ÿæˆ, generative

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12280v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12280v1.pdf)

---

## [11. UniT: Unified Multimodal Chain-of-Thought Test-time Scaling](https://arxiv.org/abs/2602.12279v1)

**ä½œè€…**ï¼šLeon Liangyu Chen, Haoyu Ma, Zhipeng Fan ç­‰ 14 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šUniT æå‡ºä¸€ç§ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„é“¾å¼æ€ç»´æµ‹è¯•æ—¶æ‰©å±•æ¡†æ¶ï¼Œè®©åŒä¸€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ç”Ÿæˆã€éªŒè¯ä¸ä¿®æ­£å¤šè½®è¾“å‡ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹å¤šä¸ºå•æ¬¡å‰å‘æ¨ç†ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ç©ºé—´å…³ç³»ã€å¤šç‰©ä½“äº¤äº’å’ŒåŠ¨æ€æŒ‡ä»¤ç­‰éœ€è¦åˆ†è§£ä¸é€æ­¥æ ¡éªŒçš„ä»»åŠ¡ï¼Œä¸”æµ‹è¯•æ—¶æ‰©å±•åœ¨å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ä¸Šä»æœªè¢«ç³»ç»Ÿæ¢ç´¢ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šUniT é€šè¿‡æ„å»ºå¸¦æœ‰æ¨ç†ä¸ç¼–è¾‘è½¨è¿¹çš„ä»£ç†å¼æ•°æ®ã€è®­ç»ƒå•ä¸€ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åŒæ—¶æ”¯æŒç†è§£ä¸ç”Ÿæˆï¼Œå¹¶åœ¨æµ‹è¯•é˜¶æ®µé‡‡ç”¨å¤šè½®é¡ºåºé“¾å¼æ¨ç†ä¸éªŒè¯æœºåˆ¶ï¼Œå®ç°åˆ†è§£å­ç›®æ ‡ã€å†…å®¹è®°å¿†å’Œè‡ªæˆ‘æ ¡æ­£ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼šç»Ÿä¸€æ¨¡å‹åœ¨çŸ­æ¨ç†è½¨è¿¹ä¸Šè®­ç»ƒå³å¯åœ¨æµ‹è¯•æ—¶æ³›åŒ–åˆ°æ›´é•¿æ¨ç†é“¾ï¼›é¡ºåºé“¾å¼æ¨ç†ç›¸æ¯”å¹¶è¡Œé‡‡æ ·æ›´å…·è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼›åŠ å…¥ç”Ÿæˆä¸ç¼–è¾‘è½¨è¿¹è®­ç»ƒæ˜¾è‘—æå‡äº†åˆ†å¸ƒå¤–è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä»è€ŒéªŒè¯äº†å¤šæ¨¡æ€æµ‹è¯•æ—¶æ‰©å±•ä½œä¸ºç»Ÿä¸€æ¨¡å‹æå‡èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹, chain-of-thoughtæ¨ç†, test-time scaling, agenticæ•°æ®åˆæˆ, ç»Ÿä¸€æ¨¡å‹è®­ç»ƒ, è¿­ä»£æ¨ç†, è§†è§‰æ¨ç†, å†…å®¹è®°å¿†

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12279v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12279v1.pdf)

---

## [12. MonarchRT: Efficient Attention for Real-Time Video Generation](https://arxiv.org/abs/2602.12271v1)

**ä½œè€…**ï¼šKrish Agarwal, Zhuoming Chen, Cheng Luo ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMonarchRT æå‡ºä¸€ç§åŸºäº Monarch çŸ©é˜µçš„é«˜æ•ˆæ³¨æ„åŠ›å‚æ•°åŒ–ï¼Œä½¿å®æ—¶æ‰©æ•£ Transformer è§†é¢‘ç”Ÿæˆåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶å®ç°é«˜ç¨€ç–åº¦å’Œå¤§å¹…æé€Ÿã€‚

**ç ”ç©¶åŠ¨æœº**ï¼š3D è‡ªæ³¨æ„åŠ›åœ¨å°‘æ­¥ã€è‡ªåŠ¨å›å½’çš„å®æ—¶è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­è®¡ç®—ä»£ä»·å‘ˆäºŒæ¬¡å¢é•¿ï¼Œä¸”ä¼ ç»Ÿç¨€ç–æ³¨æ„åŠ›åœ¨æ­¤åœºæ™¯ä¸‹å¤±æ•ˆï¼Œå› ä¸ºè§†é¢‘æ³¨æ„åŠ›æ—¢æœ‰å‘¨æœŸæ€§ç»“æ„åˆåŒ…å«åŠ¨æ€ç¨€ç–è¯­ä¹‰å’Œè‡´å¯†æ··åˆï¼Œéš¾ä»¥ç”¨ç®€å• top-k ç¨€ç–è¿‘ä¼¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åˆ†æè§†é¢‘æ³¨æ„åŠ›ç»“æ„ç‰¹æ€§åï¼Œå¼•å…¥ Monarch-RTï¼šå°†æ³¨æ„åŠ›ç”¨ Monarch çŸ©é˜µè¿›è¡Œåˆ†å—å› å¼åˆ†è§£ï¼Œå¹¶é€šè¿‡å¯¹é½å—ç»“æ„ä¸æ‰©å±• tiled Monarch å‚æ•°åŒ–æå‡è¡¨è¾¾åŠ›ï¼ŒåŒæ—¶é…åˆ Triton è‡ªå®šä¹‰ç®—å­ä¸å¾®è°ƒä»¥æ¶ˆé™¤é¢å¤–å¼€é”€ã€‚

**ä¸»è¦ç»“è®º**ï¼šMonarch-RT ç›¸æ¯”ç°æœ‰ä¸ºåŒå‘æ‰©æ•£è®¾è®¡çš„ç¨€ç–æ³¨æ„åŠ›åŸºçº¿æ•ˆæœæ›´ä¼˜ï¼Œåœ¨ SOTA æ¨¡å‹ Self-Forcing ä¸­å¯å®ç°é«˜è¾¾ 95% æ³¨æ„åŠ›ç¨€ç–åº¦è€Œæ— è´¨é‡æŸå¤±ï¼Œå¹¶åœ¨ RTX 5090ã€H100ã€B200 ä¸Šç›¸è¾ƒ FlashAttention-2/3/4 è·å¾— 1.4â€“11.8 å€ kernel åŠ é€Ÿï¼Œä½¿å•å¼  RTX 5090 ä¸Šä»¥ 16 FPS å®ç°çœŸæ­£å®æ—¶è§†é¢‘ç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, DiffusionTransformer, Transformer, ç¨€ç–æ³¨æ„åŠ›, è§†é¢‘ç”Ÿæˆ, å®æ—¶ç”Ÿæˆ, MonarchçŸ©é˜µ, è‡ªæ³¨æ„åŠ›åŠ é€Ÿ, æ˜¾å­˜ä¼˜åŒ–, é«˜å¸§ç‡æ¨ç†

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12271v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12271v1.pdf)

---

## [13. Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching](https://arxiv.org/abs/2602.12221v1)

**ä½œè€…**ï¼šOnkar Susladkar, Tushar Prakash, Gayatri Deshmukh ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šUniDFlow æå‡ºä¸€ä¸ªç»Ÿä¸€çš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦ç†è§£ä¸ç”Ÿæˆå¹¶ç»“åˆå‚è€ƒå¼åå¥½å¯¹é½ï¼Œåœ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°é«˜æ€§èƒ½ä¸å¼ºæ³›åŒ–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆç›®æ ‡ä¸Šå¸¸å‡ºç°ä¼˜åŒ–ç›®æ ‡å†²çªä¸è¡¨ç¤ºè€¦åˆï¼ŒåŒæ—¶ç¼ºä¹åœ¨å¤šç§ç¼–è¾‘/æ§åˆ¶ä»»åŠ¡ä¸Šçš„ç»Ÿä¸€æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æé«˜å¯æ§æ€§ä¸å¿ å®åº¦å¾€å¾€éœ€è¦å¤§è§„æ¨¡é‡è®­ç»ƒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ–¹æ³•ä¸Šä½¿ç”¨ç»Ÿä¸€çš„ç¦»æ•£ flow-matching ä½œä¸ºç”Ÿæˆéª¨å¹²ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„ä½ç§©é€‚é…å™¨åˆ†åˆ«ä¼˜åŒ–ç†è§£ä¸ç”Ÿæˆä»¥é¿å…ç›®æ ‡å¹²æ‰°ï¼Œå¹¶å¼•å…¥åŸºäºå‚è€ƒç»“æœçš„å¤šæ¨¡æ€åå¥½å¯¹é½ï¼Œåœ¨ç›¸åŒæ¡ä»¶ä¸‹å¯¹ä¸åŒè¾“å‡ºè¿›è¡Œç›¸å¯¹æ¯”è¾ƒä¼˜åŒ–ï¼Œæé«˜ç”Ÿæˆçš„å¿ å®åº¦å’Œå¯æ§æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šUniDFlow åœ¨å…«ä¸ªåŸºå‡†ä¸Šå–å¾— SOTA è¡¨ç°ï¼Œå¹¶åœ¨å›¾åƒä¿®è¡¥ã€ä¸Šä¸‹æ–‡å›¾åƒç”Ÿæˆã€å‚è€ƒå¼ç¼–è¾‘ä¸ç»„åˆç”Ÿæˆç­‰æœªæ˜¾å¼è®­ç»ƒä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†ç»Ÿä¸€ç¦»æ•£æµæ¡†æ¶ä¸å‚è€ƒå¼åå¥½å¯¹é½åœ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¸Šçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , å¤šæ¨¡æ€æ¨ç†, å¤šæ¨¡æ€ç”Ÿæˆ, ç¦»æ•£æµåŒ¹é…, ä½ç§©é€‚é…å™¨, å¤šæ¨¡æ€åå¥½å¯¹é½, å‚è€ƒå¼•å¯¼ç¼–è¾‘, RAG

**è¯„åˆ†**ï¼š42

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12221v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12221v1.pdf)

---

## [14. DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation](https://arxiv.org/abs/2602.12160v1)

**ä½œè€…**ï¼šXu Guo, Fulong Ye, Qichao Sun ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šDreamID-Omni æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå®ç°å¤šäººç‰©ã€å¤šå£°çº¿çš„äººä½“ä¸­å¿ƒå¯æ§éŸ³è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘ï¼Œå¹¶åœ¨å¤šä»»åŠ¡ä¸Šå–å¾— SOTA è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ–¹æ³•æŠŠå‚è€ƒç”Ÿæˆã€è§†é¢‘ç¼–è¾‘å’ŒéŸ³é¢‘é©±åŠ¨åŠ¨ç”»è§†ä¸ºå‰²è£‚ä»»åŠ¡ï¼Œä¸”éš¾ä»¥åœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°å¤šè§’è‰²èº«ä»½å’Œå£°çº¹çš„ç²¾ç»†è§£è€¦ä¸æ§åˆ¶ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ§çš„éŸ³è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºå¯¹å¤šæ¨¡æ€æ¡ä»¶å¯¹ç§°æ³¨å…¥çš„ Symmetric Conditional Diffusion Transformerï¼Œå¹¶é€šè¿‡ä¿¡å·çº§çš„ Synchronized RoPE å’Œè¯­ä¹‰çº§çš„ Structured Captions å®ç°èº«ä»½-å£°çº¹åŒå±‚è§£è€¦ï¼Œå†ç»“åˆå¤šä»»åŠ¡æ¸è¿›è®­ç»ƒï¼Œç”¨å¼±çº¦æŸç”Ÿæˆå…ˆéªŒå»æ­£åˆ™å¼ºçº¦æŸä»»åŠ¡ã€é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ DreamID-Omni åœ¨è§†é¢‘è´¨é‡ã€éŸ³é¢‘è´¨é‡åŠè§†å¬ä¸€è‡´æ€§ä¸Šå…¨é¢ä¼˜äºç°æœ‰å­¦æœ¯ä¸å•†ä¸šç³»ç»Ÿï¼Œå®ç°ç»Ÿä¸€æ¡†æ¶ä¸‹çš„äººä½“ä¸­å¿ƒéŸ³è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘ï¼Œå…·å¤‡å®é™…è½åœ°æ½œåŠ›ï¼Œä»£ç å°†å¼€æºä»¥ä¿ƒè¿›ç ”ç©¶ä¸åº”ç”¨ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , æ‰©æ•£æ¨¡å‹, Transformer, ç”Ÿæˆå¼éŸ³è§†é¢‘, å¤šäººç‰©èº«ä»½æ§åˆ¶, è¯­éŸ³éŸ³è‰²è§£è€¦, æ¡ä»¶ç”Ÿæˆæ¡†æ¶, å¤šä»»åŠ¡æ¸è¿›è®­ç»ƒ, äººç±»ä¸­å¿ƒè§†é¢‘ç”Ÿæˆ

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12160v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12160v1.pdf)

---

## [15. TexSpot: 3D Texture Enhancement with Spatially-uniform Point Latent Representation](https://arxiv.org/abs/2602.12157v1)

**ä½œè€…**ï¼šZiteng Lu, Yushuang Wu, Chongjie Ye ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.GR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

High-quality 3D texture generation remains a fundamental challenge due to the view-inconsistency inherent in current mainstream multi-view diffusion pipelines. Existing representations either rely on UV maps, which suffer from distortion during unwrapping, or point-based methods, which tightly couple texture fidelity to geometric density that limits high-resolution texture generation. To address these limitations, we introduce TexSpot, a diffusion-based texture enhancement framework. At its core is Texlet, a novel 3D texture representation that merges the geometric expressiveness of point-based 3D textures with the compactness of UV-based representation. Each Texlet latent vector encodes a local texture patch via a 2D encoder and is further aggregated using a 3D encoder to incorporate global shape context. A cascaded 3D-to-2D decoder reconstructs high-quality texture patches, enabling the Texlet space learning. Leveraging this representation, we train a diffusion transformer conditioned on Texlets to refine and enhance textures produced by multi-view diffusion methods. Extensive experiments demonstrate that TexSpot significantly improves visual fidelity, geometric consistency, and robustness over existing state-of-the-art 3D texture generation and enhancement approaches. Project page: https://anonymous.4open.science/w/TexSpot-page-2D91.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šTexSpotæå‡ºä¸€ç§åä¸ºTexletçš„ç‚¹æ½œè¡¨ç¤ºï¼Œå¹¶ç»“åˆæ‰©æ•£Transformerå¯¹ç°æœ‰å¤šè§†è§’æ–¹æ³•ç”Ÿæˆçš„3Dçº¹ç†è¿›è¡Œä¸€è‡´æ€§å¢å¼ºä¸ç»†èŠ‚æå‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰3Dçº¹ç†ç”Ÿæˆè¦ä¹ˆä¾èµ–å­˜åœ¨å±•å¼€ç•¸å˜çš„UVè´´å›¾ï¼Œè¦ä¹ˆä¾èµ–å¼ºç»‘å®šå‡ ä½•å¯†åº¦çš„ç‚¹åŸºè¡¨ç¤ºï¼Œå¯¼è‡´è§†è§’ä¸ä¸€è‡´å’Œéš¾ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çº¹ç†ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ—¢è¡¨è¾¾åŠ›å¼ºåˆç´§å‡‘ã€ä¸”ä¸å‡ ä½•è§£è€¦çš„3Dçº¹ç†è¡¨ç¤ºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºTexletï¼šç”¨2Dç¼–ç å™¨å°†å±€éƒ¨çº¹ç†å—ç¼–ç ä¸ºç‚¹æ½œå‘é‡ï¼Œå†ç”¨3Dç¼–ç å™¨èšåˆå…¨å±€å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡çº§è”3Dåˆ°2Dè§£ç å™¨é‡å»ºé«˜è´¨é‡çº¹ç†å—ï¼Œåœ¨æ­¤è¡¨ç¤ºä¸Šè®­ç»ƒæ¡ä»¶æ‰©æ•£Transformerï¼Œå¯¹å¤šè§†è§’æ‰©æ•£ç”Ÿæˆçš„åˆå§‹çº¹ç†è¿›è¡Œç»Ÿä¸€ç²¾ä¿®ä¸å¢å¼ºã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜TexSpotåœ¨è§†è§‰è´¨é‡ã€å‡ ä½•ä¸è§†è§’ä¸€è‡´æ€§ä»¥åŠé²æ£’æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰3Dçº¹ç†ç”Ÿæˆä¸å¢å¼ºæ–¹æ³•ï¼ŒéªŒè¯äº†Texletè¡¨ç¤ºä¸æ‰©æ•£å¢å¼ºæ¡†æ¶åœ¨é«˜è´¨é‡3Dçº¹ç†ç”Ÿæˆä¸Šçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, transformer, ç”Ÿæˆå¼çº¹ç†å¢å¼º, 3Dçº¹ç†ç”Ÿæˆ, ç‚¹äº‘è¡¨ç¤º, UVè´´å›¾èåˆ, å¤šè§†è§’ä¸€è‡´æ€§

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12157v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12157v1.pdf)

---

## [16. FAIL: Flow Matching Adversarial Imitation Learning for Image Generation](https://arxiv.org/abs/2602.12155v1)

**ä½œè€…**ï¼šYeyao Ma, Chen Li, Xiaosong Zhang ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºå°†æµåŒ¹é…å›¾åƒç”Ÿæˆæ¨¡å‹çš„åè®­ç»ƒå½¢å¼åŒ–ä¸ºæ¨¡ä»¿å­¦ä¹ ï¼Œå¹¶ç”¨å¯¹æŠ—å¼æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ FAIL æ›¿ä»£åå¥½ä¼˜åŒ–ï¼Œåœ¨å°‘é‡ç¤ºä¾‹ä¸‹å¤§å¹…æå‡å›¾åƒç”Ÿæˆè´¨é‡ä¸å¯¹é½èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŸºäºæµåŒ¹é…çš„ç”Ÿæˆæ¨¡å‹åè®­ç»ƒå¤šä¾èµ–ç›‘ç£å¾®è°ƒæˆ–åå¥½ä¼˜åŒ–ï¼šå‰è€…æ— æ³•çº æ­£æœªè§çŠ¶æ€ä¸‹çš„ç­–ç•¥æ¼‚ç§»ï¼Œåè€…åˆéœ€è¦æ˜‚è´µçš„åå¥½æ ‡æ³¨æˆ–å¥–åŠ±å»ºæ¨¡ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ— éœ€æ˜¾å¼å¥–åŠ±ã€ä½†èƒ½ç›´æ¥æœ€å°åŒ–ç­–ç•¥ä¸ä¸“å®¶åˆ†å¸ƒå·®å¼‚çš„é«˜æ•ˆæ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†æµåŒ¹é…åè®­ç»ƒè§†ä¸ºæ¨¡ä»¿å­¦ä¹ é—®é¢˜ï¼Œå¼•å…¥å¯¹æŠ—å¼è®­ç»ƒæ¥ä¼°è®¡å¹¶æœ€å°åŒ–ç”Ÿæˆç­–ç•¥ä¸ä¸“å®¶åˆ†å¸ƒçš„å·®å¼‚ï¼Œæå‡ºä¸¤ç§ç®—æ³•ï¼šFAIL-PD åˆ©ç”¨å¯å¾®åˆ† ODE æ±‚è§£å™¨è¿›è¡Œä½æ–¹å·®è·¯å¾„æ¢¯åº¦ä¼˜åŒ–ï¼ŒFAIL-PG åˆ™ä»¥é»‘ç›’ç­–ç•¥æ¢¯åº¦æ–¹å¼æ”¯æŒç¦»æ•£æˆ–ç®—åŠ›å—é™åœºæ™¯ï¼Œå¹¶åœ¨å›¾åƒ/è§†é¢‘ç”Ÿæˆä¸­ç»Ÿä¸€åº”ç”¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ä»…ä½¿ç”¨çº¦ 1.3 ä¸‡æ¡ Nano Banana pro ç¤ºèŒƒçš„æ¡ä»¶ä¸‹ï¼Œå¯¹ FLUX è¿›è¡Œ FAIL å¾®è°ƒå³å¯åœ¨æŒ‡ä»¤è·Ÿéšä¸ç¾å­¦æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ‰ç«äº‰åŠ›ç”šè‡³æ›´ä¼˜è¡¨ç°ï¼›è¯¥æ¡†æ¶å¯æ¨å¹¿åˆ°ç¦»æ•£å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œå¹¶åœ¨å¥–åŠ±ä¼˜åŒ–åœºæ™¯ä¸­å……å½“ç¨³å®šæ­£åˆ™é¡¹ï¼Œæœ‰æ•ˆç¼“è§£ reward hacking é—®é¢˜ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç”Ÿæˆå¼æ¨¡å‹, flow matching, å¯¹æŠ—å¼æ¨¡ä»¿å­¦ä¹ , æ‰©æ•£æ¨¡å‹, å›¾åƒç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ, ç­–ç•¥ä¼˜åŒ–, å¥–åŠ±é»‘å®¢é˜²æŠ¤, reward model

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12155v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12155v1.pdf)

---

## cs.LG

## [17. Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage](https://arxiv.org/abs/2602.12274v1)

**ä½œè€…**ï¼šXin Ju, Jiachen Yao, Anima Anandkumar ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, physics.geo-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºFun-DDPSæ¡†æ¶ï¼Œå°†å‡½æ•°ç©ºé—´æ‰©æ•£æ¨¡å‹ä¸å¯å¾®åˆ†ç¥ç»ç®—å­è§£è€¦ç»“åˆï¼Œå®ç°å¯¹CCSåœ°ä¸‹æµåŠ¨çš„é«˜ç²¾åº¦å‰å‘ä¸åæ¼”å»ºæ¨¡ï¼Œå°¤å…¶åœ¨è§‚æµ‹æåº¦ç¨€ç–åœºæ™¯ä¸‹è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šCCSåœºæ™¯ä¸­åœ°ä¸‹æµåŠ¨çš„å‚æ•°åæ¼”é—®é¢˜é«˜åº¦ç—…æ€ä¸”è§‚æµ‹ç¨€ç–ï¼Œä¼ ç»Ÿç¡®å®šæ€§/ä»£ç†æ¨¡å‹åœ¨ç¼ºå¤±ä¿¡æ¯å’Œä¸ç¡®å®šæ€§è¡¨å¾æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå› æ­¤éœ€è¦åŒæ—¶èƒ½è¡¨è¾¾å…ˆéªŒåˆ†å¸ƒã€åˆ©ç”¨ç‰©ç†è§„å¾‹å¹¶æ”¯æŒé«˜æ•ˆè´å¶æ–¯åæ¼”çš„ç”Ÿæˆå¼æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ–¹æ³•å…ˆç”¨å•é€šé“å‡½æ•°ç©ºé—´æ‰©æ•£æ¨¡å‹å­¦ä¹ åœ°è´¨å‚æ•°ï¼ˆgeomodelï¼‰çš„å…ˆéªŒåˆ†å¸ƒï¼Œå†ç”¨å±€éƒ¨ç¥ç»ç®—å­LNOä½œä¸ºç‰©ç†ä¸€è‡´çš„å¯å¾®ä»£ç†ï¼Œå¯¹åŠ¨åŠ›å­¦åœºè¿›è¡Œè·¨åœºæ¡ä»¶ä¸æ¢¯åº¦å¼•å¯¼ï¼Œä»è€Œå®ç°â€œå…ˆéªŒç”Ÿæˆ + ç‰©ç†è§£è€¦æŒ‡å¯¼â€çš„å‰å‘ä¸åæ¼”è”åˆå»ºæ¨¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨åˆæˆCCSæ•°æ®ä¸Šï¼ŒFun-DDPSåœ¨ä»…æœ‰25%è§‚æµ‹çš„å‰å‘é¢„æµ‹ä¸­å°†ç›¸å¯¹è¯¯å·®ä»ä¼ ç»Ÿä»£ç†çš„86.9%é™è‡³7.7%ï¼Œå¹¶åœ¨åæ¼”ä»»åŠ¡ä¸­ä¸è¿‘ä¼¼ç²¾ç¡®çš„æ‹’ç»é‡‡æ ·åéªŒç›¸æ¯”JSD<0.06ï¼Œä¸”æ ·æœ¬æ•ˆç‡æå‡çº¦4å€å¹¶é¿å…è”åˆå»ºæ¨¡åŸºçº¿ä¸­å‡ºç°çš„é«˜é¢‘ä¼ªå½±ï¼Œè¯æ˜äº†è¯¥è§£è€¦æ‰©æ•£æ¡†æ¶åœ¨æç«¯ç¨€ç–è§‚æµ‹ä¸ç‰©ç†ä¸€è‡´æ€§ä¸Šçš„ä¼˜åŠ¿ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹, æ·±åº¦å­¦ä¹ , ç¥ç»ç®—å­, ç‰©ç†å¼•å¯¼å»ºæ¨¡, å‡½æ•°ç©ºé—´å»ºæ¨¡, ç¢³æ•é›†ä¸å°å­˜, åœ°è´¨å‚æ•°åæ¼”, ç¨€ç–è§‚æµ‹æ•°æ®åŒåŒ–, å‰å‘å»ºæ¨¡, é€†é—®é¢˜æ±‚è§£, generative

**è¯„åˆ†**ï¼š26

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12274v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12274v1.pdf)

---

## [18. Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data](https://arxiv.org/abs/2602.12267v1)

**ä½œè€…**ï¼šDuy Nguyen, Jiachen Yao, Jiayun Wang ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO's robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§åŸºäºæµåŒ¹é…ä¸ç¥ç»ç®—å­çš„æ–°å‹è‡ªç›‘ç£æ¡†æ¶FGNOï¼Œé€šè¿‡åœ¨å‡½æ•°ç©ºé—´ä¸­å­¦ä¹ ä¸åŒè…èš€å¼ºåº¦ä¸‹çš„æ—¶é—´åºåˆ—è¡¨å¾ï¼Œæ˜¾è‘—æå‡å¤šç§ç”Ÿç‰©åŒ»å­¦æ—¶é—´åºåˆ—ä»»åŠ¡æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ—¶é—´åºåˆ—è‡ªç›‘ç£æ–¹æ³•å¤šé‡‡ç”¨å›ºå®šæ©ç æˆ–å›ºå®šå™ªå£°ç­–ç•¥ï¼Œç¼ºä¹å¯¹â€œè…èš€å¼ºåº¦â€è¿™ä¸€æ–°è‡ªç”±åº¦çš„ç³»ç»Ÿåˆ©ç”¨ï¼Œéš¾ä»¥åœ¨ä½æ•°æ®åœºæ™¯ä¸‹å­¦åˆ°æ—¢ç»†ç²’åº¦åˆå…¨å±€çš„é€šç”¨è¡¨å¾ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šFGNOä½¿ç”¨çŸ­æ—¶å‚…é‡Œå¶å˜æ¢å°†ä¸åŒæ—¶é—´åˆ†è¾¨ç‡ç»Ÿä¸€åˆ°å‡½æ•°ç©ºé—´ï¼Œç»“åˆæµåŒ¹é…å­¦ä¹ ä»å™ªå£°åˆ°å¹²å‡€ä¿¡å·çš„æ˜ å°„ï¼Œå¹¶ä»ä¸åŒç½‘ç»œå±‚å’Œä¸åŒæµæ—¶é—´ï¼ˆå¯¹åº”ä¸åŒå™ªå£°å¼ºåº¦ï¼‰æå–å¤šå±‚æ¬¡ç‰¹å¾ï¼›è®­ç»ƒæ—¶é€šè¿‡å™ªå£°è…èš€è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæ¨ç†æ—¶ä»…ç”¨å¹²å‡€è¾“å…¥æå–ç¨³å®šè¡¨å¾ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ç¥ç»ä¿¡å·è§£ç ã€çš®è‚¤æ¸©åº¦é¢„æµ‹å’Œç¡çœ é˜¶æ®µåˆ†ç±»ç­‰ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦åŸºå‡†ä¸Šï¼ŒFGNOåœ¨AUROCã€RMSEã€å‡†ç¡®ç‡å’Œmacro-F1ç­‰æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¸»æµåŸºçº¿ï¼Œå±•ç°å‡ºå¯¹æ•°æ®ç¨€ç¼ºå’Œå¤šä»»åŠ¡åœºæ™¯çš„å¼ºé²æ£’æ€§å’Œè¡¨å¾èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šè‡ªç›‘ç£å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, æ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ , æµåŒ¹é…æ¨¡å‹, NeuralOperator, é¢‘åŸŸç‰¹å¾æå–, ç”Ÿç‰©åŒ»å­¦ä¿¡å·åˆ†æ, æ— ç›‘ç£é¢„è®­ç»ƒ, generative

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12267v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12267v1.pdf)

---

## [19. Community Concealment from Unsupervised Graph Learning-Based Clustering](https://arxiv.org/abs/2602.12250v1)

**ä½œè€…**ï¼šDalyapraz Manatova, Pablo Moriano, L. Jean Camp  
**åˆ†ç±»**ï¼šcs.LG, cs.CR, cs.SI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç ”ç©¶å¦‚ä½•åœ¨æœ‰é™æ‰°åŠ¨é¢„ç®—ä¸‹ï¼Œé€šè¿‡ä¿®æ”¹å›¾ç»“æ„å’ŒèŠ‚ç‚¹ç‰¹å¾æ¥éšè”½ç‰¹å®šç¤¾åŒºï¼Œä½¿åŸºäºGNNçš„æ— ç›‘ç£èšç±»/ç¤¾åŒºæ£€æµ‹éš¾ä»¥è¯†åˆ«è¯¥ç¤¾åŒºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šGNNåœ¨æ— ç›‘ç£ç¤¾åŒºæ£€æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†ä¼šæš´éœ²æ•æ„Ÿç¾¤ä½“å’Œç³»ç»Ÿç»“æ„ï¼Œå¸¦æ¥ç¾¤ä½“å±‚é¢çš„éšç§ä¸å®‰å…¨é£é™©ï¼Œå› æ­¤éœ€è¦é¢å‘â€œæ•°æ®å‘å¸ƒè€…â€çš„é˜²å¾¡æœºåˆ¶æ¥éšè—ç‰¹å®šç¤¾åŒºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åˆ†æå½±å“ç¤¾åŒºéšè”½æ€§çš„ä¸¤ä¸ªå…³é”®å› ç´ â€”â€”ç¤¾åŒºè¾¹ç•Œè¿é€šæ€§ä¸ä¸é‚»æ¥ç¤¾åŒºçš„ç‰¹å¾ç›¸ä¼¼åº¦ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†ä¸€ç§åœ¨æ‰°åŠ¨é¢„ç®—ä¸‹é‡è¿éƒ¨åˆ†è¾¹å’Œä¿®æ”¹èŠ‚ç‚¹ç‰¹å¾çš„ç­–ç•¥ï¼Œä»¥å‰Šå¼±GNNæ¶ˆæ¯ä¼ é€’æ‰€ä¾èµ–çš„åŒºåˆ†æ€§ä¿¡æ¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨åˆæˆä¸çœŸå®ç½‘ç»œä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ç›¸åŒæ‰°åŠ¨é¢„ç®—ä¸‹ç›¸è¾ƒDICEå¯å¸¦æ¥çº¦20â€“45%çš„ä¸­ä½ç›¸å¯¹éšè”½æ€§æå‡ï¼Œè¯´æ˜æœ‰å¯èƒ½é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å›¾æ‰°åŠ¨æœ‰æ•ˆå¯¹æŠ—åŸºäºGNNçš„ç¤¾åŒºå­¦ä¹ ï¼ŒåŒæ—¶æ­ç¤ºäº†å›¾å­¦ä¹ å›ºæœ‰çš„ç¾¤ä½“éšç§é£é™©ã€‚

**å…³é”®è¯**ï¼šå›¾ç¥ç»ç½‘ç»œ, æ— ç›‘ç£å­¦ä¹ , ç¤¾åŒºæ£€æµ‹, å›¾è¡¨ç¤ºå­¦ä¹ , éšç§ä¿æŠ¤, å¯¹æŠ—æ‰°åŠ¨, è¾¹é‡è¿, ç‰¹å¾æ‰°åŠ¨, ç¤¾äº¤ç½‘ç»œå®‰å…¨, neural network

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12250v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12250v1.pdf)

---

## [20. ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction](https://arxiv.org/abs/2602.12247v1)

**ä½œè€…**ï¼šNick Ferguson, Josh Pennington, Narek Beghian ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šExtractBench æå‡ºé¦–ä¸ªé¢å‘å¤æ‚ä¼ä¸šçº§ JSON Schema çš„ PDFâ†’JSON ç»“æ„åŒ–æŠ½å–åŸºå‡†å’Œå¯æ‰§è¡Œè¯„æµ‹æ¡†æ¶ï¼Œå¹¶æ˜¾ç¤ºå½“å‰å‰æ²¿ LLM åœ¨æ­¤ä»»åŠ¡ä¸Šä»ç„¶å¾ˆä¸å¯é ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å·¥ä½œç¼ºä¹åŒæ—¶è¦†ç›–å¤§è§„æ¨¡ã€å¤šå±‚çº§ JSON Schema çš„ç«¯åˆ°ç«¯ PDF æŠ½å–åŸºå‡†ï¼Œä¹Ÿç¼ºä¹èƒ½åŒºåˆ†ä¸åŒå­—æ®µè¯­ä¹‰æ­£ç¡®æ€§ï¼ˆç²¾ç¡®åŒ¹é…ã€æ•°å€¼å®¹å·®ã€è¯­ä¹‰ç­‰ä»·ï¼‰ã€æ•°ç»„å¯¹é½åŠç¼ºå¤±ä¸å¹»è§‰çš„ç³»ç»ŸåŒ–è¯„ä¼°æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºåŒ…å«35ä¸ª PDF æ–‡æ¡£ã€é…å¥— JSON Schema ä¸äººå·¥æ ‡æ³¨é‡‘æ ‡çš„å…¬å¼€åŸºå‡†ï¼Œè¦†ç›–12,867ä¸ªå¯è¯„ä¼°å­—æ®µï¼Œå¹¶å°† schema è§†ä¸ºâ€œå¯æ‰§è¡Œè§„èŒƒâ€ï¼Œåœ¨æ¯ä¸ªå­—æ®µä¸­å£°æ˜å…·ä½“è¯„åˆ†æŒ‡æ ‡ä»¥è‡ªåŠ¨è¯„æµ‹å¤æ‚åµŒå¥—ä¸æ•°ç»„ç»“æ„çš„æŠ½å–è´¨é‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ ExtractBench ä¸Šï¼ŒGPT-5ã€Gemini-3ã€Claude 4.5 ç­‰å‰æ²¿ LLM åœ¨çœŸå®å¤æ‚ Schema ä¸Šè¡¨ç°ä¸ç¨³å®šï¼Œéšå­—æ®µè§„æ¨¡æ‰©å¤§æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œåœ¨åŒ…å«369å­—æ®µçš„è´¢æŠ¥ Schema ä¸Šæ‰€æœ‰æ¨¡å‹çš„æœ‰æ•ˆè¾“å‡ºç‡ç”šè‡³é™ä¸º 0%ï¼Œè¡¨æ˜å½“å‰ LLM ä»éš¾ä»¥å¯é èƒœä»»ä¼ä¸šçº§å¤æ‚ç»“æ„æŠ½å–ã€‚

**å…³é”®è¯**ï¼šå¤§æ¨¡å‹, ç»“æ„åŒ–ä¿¡æ¯æŠ½å–, PDFæ–‡æ¡£è§£æ, JSONæ¨¡å¼å¯¹é½, åµŒå¥—å­—æ®µè¯„ä¼°, ä¼ä¸šçº§ä¿¡æ¯æŠ½å–åŸºå‡†, å¼€æ”¾æºä»£ç è¯„æµ‹æ¡†æ¶, llm

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12247v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12247v1.pdf)

---

## [21. Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces](https://arxiv.org/abs/2602.12245v1)

**ä½œè€…**ï¼šAnthony Kobanda, Waris Radji  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæ–‡ç« ä»ç†è®ºä¸Šè¯æ˜ï¼šå½“ JEPA çš„èƒ½é‡å‡½æ•°æ˜¯ç”±æœ€å°ç´¯è®¡â€œåŠªåŠ›â€å®šä¹‰çš„å†…ç¦€èƒ½é‡æ—¶ï¼Œè¿™ä¸ªèƒ½é‡ç©ºé—´å¤©ç„¶å½¢æˆä¸€ä¸ªæ‹Ÿåº¦é‡ç©ºé—´ï¼Œä¸å¼ºåŒ–å­¦ä¹ ä¸­çš„ç›®æ ‡æ¡ä»¶ä»£ä»·å‡½æ•°æœ¬è´¨ä¸€è‡´ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ JEPA å¤šç”¨å¯¹ç§°çš„ç›¸ä¼¼åº¦/èƒ½é‡æ¥å­¦ä¹ è¡¨å¾ï¼Œä½†åœ¨å…·æœ‰æ–¹å‘æ€§çš„æ§åˆ¶ä¸åˆ°è¾¾ä»»åŠ¡ä¸­ï¼Œä»£ä»·å¾€å¾€æ˜¯ä¸å¯¹ç§°çš„ï¼Œå› æ­¤éœ€è¦æ¾„æ¸… JEPA çš„èƒ½é‡å½¢å¼ä¸å¼ºåŒ–å­¦ä¹ ä¸­â€œåˆ°ç›®æ ‡ä»£ä»·â€çš„æ•°å­¦å…³ç³»ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°† JEPA çš„èƒ½é‡å‡½æ•°é™åˆ¶ä¸ºâ€œå†…ç¦€ï¼ˆæœ€å°ä½œç”¨é‡ï¼‰èƒ½é‡â€ï¼Œå³åœ¨ä¸¤çŠ¶æ€é—´æ‰€æœ‰å¯è¡Œè½¨è¿¹ä¸Šå¯¹å±€éƒ¨åŠªåŠ›çš„ç´¯ç§¯å–ä¸‹ç¡®ç•Œï¼Œå¹¶åœ¨æ¸©å’Œçš„é—­åŒ…å’Œå¯åŠ æ€§å‡è®¾ä¸‹è¯æ˜å…¶æ»¡è¶³æ‹Ÿåº¦é‡ï¼ˆquasimetricï¼‰æ€§è´¨ï¼Œå¹¶å°†å…¶ä¸æœ€ä¼˜ cost-to-go çš„å½¢å¼è¿›è¡Œå¯¹æ¯”ã€‚

**ä¸»è¦ç»“è®º**ï¼šåªè¦ JEPA å­¦ä¹ çš„æ˜¯å†…ç¦€èƒ½é‡ï¼Œå®ƒä»¬è¯±å¯¼çš„è¡¨ç¤ºç©ºé—´å°±æ˜¯æ‹Ÿåº¦é‡ç©ºé—´ï¼Œæ°å¥½å¯¹åº” QRL ä¸­çš„æœ€ä¼˜ cost-to-go ç±»å‡½æ•°ï¼›åŒæ—¶ï¼Œå¯¹ç§°ä¸”æœ‰é™çš„èƒ½é‡ç»“æ„ä¸å•å‘å¯è¾¾æ€§ä¸åŒ¹é…ï¼Œå› æ­¤åœ¨æ–¹å‘æ€§ä»»åŠ¡ä¸­åº”é‡‡ç”¨ä¸å¯¹ç§°çš„æ‹Ÿåº¦é‡èƒ½é‡å»ºæ¨¡ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, embedding, æ£€ç´¢, è¡¨ç¤ºå­¦ä¹ , èƒ½é‡æ¨¡å‹, ç›®æ ‡å¯¼å‘æ§åˆ¶, å¼ºåŒ–å­¦ä¹ , ä»·å€¼å‡½æ•°é¢„ä¼°, ä¸å¯¹ç§°è·ç¦»

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12245v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12245v1.pdf)

---

## [22. Olmix: A Framework for Data Mixing Throughout LM Development](https://arxiv.org/abs/2602.12237v1)

**ä½œè€…**ï¼šMayee F. Chen, Tyler Murray, David Heineman ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šOlmix æå‡ºäº†ä¸€å¥—è´¯ç©¿å¤§æ¨¡å‹å¼€å‘å…¨å‘¨æœŸçš„æ•°æ®æ··é…æ¡†æ¶ï¼Œåœ¨å¤šè½®æ•°æ®é›†å˜æ›´åœºæ™¯ä¸‹é«˜æ•ˆæ›´æ–°å„é¢†åŸŸæ•°æ®æ¯”ä¾‹ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ•°æ®æ··é…æ–¹æ³•è®¾è®¡é€‰æ‹©åˆ†æ•£ä¸”ç¼ºä¹ç³»ç»Ÿæ€§æ¯”è¾ƒï¼Œè€Œä¸”å¤§å¤šå‡è®¾é¢†åŸŸé›†åˆå›ºå®šï¼Œæ— æ³•åº”å¯¹å®é™…å¼€å‘ä¸­æ•°æ®ä¸æ–­å¢åˆ å’Œé‡åˆ’åˆ†çš„åŠ¨æ€åœºæ™¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ç³»ç»Ÿå®è¯åˆ†æä¸åŒæ··é…è®¾è®¡ç©ºé—´ï¼ˆå¦‚çº¦æŸã€æœç´¢ç­–ç•¥ç­‰ï¼‰ï¼Œå¹¶æå‡ºâ€œmixture reuseâ€æœºåˆ¶ï¼šåœ¨é¢†åŸŸé›†åˆå˜æ›´æ—¶å¤ç”¨æ—§æ··é…æ¯”ä¾‹ï¼Œä»…å¯¹å—å½±å“é¢†åŸŸé‡æ–°è®¡ç®—ï¼Œä»è€ŒèŠ‚çœè®¡ç®—ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ¨¡æ‹ŸçœŸå®å¼€å‘æµç¨‹çš„äº”æ¬¡é¢†åŸŸé›†æ›´æ–°åºåˆ—ä¸Šï¼Œmixture reuse åœ¨ä»…ç”¨ 26% è®¡ç®—é‡çš„æƒ…å†µä¸‹è¾¾åˆ°ä¸æ¯æ¬¡å®Œå…¨é‡ç®—ç›¸å½“çš„æ•ˆæœï¼Œå¹¶ç›¸å¯¹æ— æ··é…è®­ç»ƒåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¸¦æ¥çº¦ 11.6% æ€§èƒ½æå‡ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , è¯­è¨€æ¨¡å‹, æ•°æ®æ··åˆ, é¢†åŸŸè‡ªé€‚åº”, è®­ç»ƒæ•°æ®é…æ¯”, æ¨¡å‹å¼€å‘æµç¨‹, å¢é‡æ›´æ–°, æ··åˆæ¯”é‡å¤ç”¨, å¤§è§„æ¨¡é¢„è®­ç»ƒ, ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æå‡, rag

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12237v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12237v1.pdf)

---

## [23. Categorical Flow Maps](https://arxiv.org/abs/2602.12233v1)

**ä½œè€…**ï¼šDaan Roos, Oscar Davis, Floor Eijkelboom ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§é’ˆå¯¹ç¦»æ•£/ç±»åˆ«æ•°æ®çš„æµåŒ¹é…æ–°æ¡†æ¶ Categorical Flow Mapsï¼Œé€šè¿‡è¿ç»­åŒ–åˆ°å•çº¯å½¢å¹¶ç»“åˆè‡ªè’¸é¦ï¼Œå®ç°å›¾åƒã€åˆ†å­å›¾å’Œæ–‡æœ¬çš„é«˜è´¨é‡å°‘æ­¥ç”šè‡³å•æ­¥ç”Ÿæˆã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ‰©æ•£ä¸æµæ¨¡å‹åœ¨ç±»åˆ«æ•°æ®ä¸Šé€šå¸¸éœ€è¦è¾ƒå¤šé‡‡æ ·æ­¥æ•°ä¸”ç¦»æ•£ç»“æ„é™åˆ¶äº†è’¸é¦ä¸æŒ‡å¯¼ç­‰åŠ é€ŸæŠ€æœ¯çš„ä½¿ç”¨ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ—¢é€‚åˆç¦»æ•£æ•°æ®åˆèƒ½é«˜æ•ˆå°‘æ­¥ç”Ÿæˆçš„ç»Ÿä¸€æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨å•çº¯å½¢ä¸Šå®šä¹‰ä»åˆå§‹åˆ†å¸ƒåˆ°é¢„æµ‹ç»ˆç‚¹åˆ†å¸ƒçš„è¿ç»­æµæ˜ å°„ï¼Œä½¿æ¦‚ç‡è´¨é‡æ²¿è¿ç»­è½¨è¿¹ç§»åŠ¨å¹¶å¤©ç„¶æ»¡è¶³æ¦‚ç‡çº¦æŸï¼›åœ¨æ­¤åŸºç¡€ä¸Šç»“åˆç°æœ‰è’¸é¦æŠ€æœ¯ä¸æ–°çš„â€œç»ˆç‚¹ä¸€è‡´æ€§â€ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå¤ç”¨è¿ç»­æµåŒ¹é…é¢†åŸŸçš„æŒ‡å¯¼ä¸é‡åŠ æƒæŠ€å·§æ¥å¼•å¯¼ç±»åˆ«é‡‡æ ·ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•åœ¨å›¾åƒã€åˆ†å­å›¾å’Œæ–‡æœ¬ç­‰ç¦»æ•£ä»»åŠ¡ä¸Šå®ç°äº†å½“å‰æœ€ä¼˜çš„å°‘æ­¥ç”Ÿæˆæ•ˆæœï¼Œå³ä½¿åœ¨å•æ­¥ç”Ÿæˆåœºæ™¯ä¹Ÿä¿æŒè¾ƒå¼ºæ€§èƒ½ï¼ŒåŒæ—¶è¯æ˜è¿ç»­åŒ–çš„ç±»åˆ«æµæ˜ å°„æ—¢èƒ½åŠ é€Ÿæ¨ç†åˆèƒ½çµæ´»æ”¯æŒå„ç±»ä¸‹æ¸¸ç›®æ ‡å¼•å¯¼ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, flow matching, è‡ªè’¸é¦, ç”Ÿæˆæ¨¡å‹, åˆ†ç±»æ•°æ®å»ºæ¨¡, ç«¯ç‚¹ä¸€è‡´æ€§è®­ç»ƒ, è¿ç»­è½¨è¿¹é‡‡æ ·, å›¾åƒåˆ†å­å›¾æ–‡æœ¬ç”Ÿæˆ, diffusion

**è¯„åˆ†**ï¼š47

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12233v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12233v1.pdf)

---

## [24. Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser](https://arxiv.org/abs/2602.12229v1)

**ä½œè€…**ï¼šZijing Ou, Jacob Si, Junyi Zhu ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡å°†æ‰©æ•£å¯¹é½è§†ä¸ºä¸€ä¸ªå¸¦é‡è¦æ€§é‡‡æ ·çš„SMCè¿‡ç¨‹ï¼Œæå‡ºç”¨â€œå¯¹æ•°é‡è¦æ€§æƒé‡æ–¹å·®æœ€å°åŒ–â€æ›¿ä»£ä¼ ç»ŸKLä½œä¸ºæ›´ç»Ÿä¸€æœ‰æ•ˆçš„æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ç›®æ ‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ‰©æ•£å¯¹é½æ–¹æ³•å¤šä»¥KLä¸ºç›®æ ‡ï¼Œéš¾ä»¥ç»Ÿä¸€ç†è§£ä¸åŒç®—æ³•å½¢å¼ï¼Œä¸”åœ¨é‡‡æ ·æ•ˆç‡å’Œç¨³å®šæ€§ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå› æ­¤ä½œè€…å°è¯•ä»é‡è¦æ€§é‡‡æ ·æ–¹å·®çš„è§’åº¦é‡æ–°åˆ»ç”»ä¸ä¼˜åŒ–å¯¹é½è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†æ‰©æ•£å¯¹é½å»ºæ¨¡ä¸ºSMCï¼šå»å™ªæ¨¡å‹æ˜¯proposalï¼Œå¥–åŠ±å¼•å…¥importance weightï¼Œå¹¶æå‡ºVMPOï¼Œå°†ä¼˜åŒ–ç›®æ ‡è®¾ä¸ºæœ€å°åŒ–logé‡è¦æ€§æƒé‡çš„æ–¹å·®ï¼›ç†è®ºä¸Šè¯æ˜å…¶æœ€ä¼˜è§£æ˜¯å¥–åŠ±å€¾æ–œåˆ†å¸ƒï¼Œä¸”åœ¨on-policyæ¡ä»¶ä¸‹æ¢¯åº¦ä¸KLå¯¹é½ç­‰ä»·ï¼Œå¹¶å±•ç¤ºä¸åŒåŠ¿å‡½æ•°ä¸æ–¹å·®æœ€å°åŒ–ç­–ç•¥å¦‚ä½•ç»Ÿä¸€å¹¶æ‰©å±•å·²æœ‰æ–¹æ³•ã€‚

**ä¸»è¦ç»“è®º**ï¼šVMPOæä¾›äº†ä¸€ä¸ªä»¥æ–¹å·®æœ€å°åŒ–ä¸ºæ ¸å¿ƒçš„ç»Ÿä¸€è§†è§’ï¼Œå°†å¤šç§æ‰©æ•£å¯¹é½æ–¹æ³•å½’ä¸€åˆ°åŒä¸€æ¡†æ¶ä¸­ï¼Œå¹¶åœ¨ä¸å±€é™äºKLçš„å‰æä¸‹ç»™å‡ºæ–°çš„è®¾è®¡ç»´åº¦ï¼Œç†è®ºä¸Šä¿æŒä¸KLç›®æ ‡ä¸€è‡´çš„æœ€ä¼˜åˆ†å¸ƒï¼ŒåŒæ—¶ä¸ºæ„é€ æ›´ç¨³å®šã€é«˜æ•ˆçš„æ‰©æ•£å¯¹é½ç®—æ³•æä¾›äº†æ–¹å‘ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, ç”Ÿæˆå¼å»ºæ¨¡, policyä¼˜åŒ–, variance minimisation, å¥–åŠ±å»ºæ¨¡, Sequential Monte Carlo, importance sampling, å¯¹é½æ–¹æ³•, é‡‡æ ·ç­–ç•¥, ç›®æ ‡åˆ†å¸ƒ, diffusion

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12229v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12229v1.pdf)

---

## [25. Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222v1)

**ä½œè€…**ï¼šMiaosen Zhang, Yishan Liu, Shuxia Lin ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºåˆ†å¸ƒåˆ¤åˆ«ç†è®ºï¼ˆDDTï¼‰åŠå…¶è¡ç”Ÿçš„åœ¨åˆ†å¸ƒå¾®è°ƒå’Œæç¤ºè§£ç æ–¹æ³•ï¼Œä½¿ä¼ ç»ŸSFTåœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶æ¥è¿‘ç¦»çº¿RLï¼ˆå¦‚DPOã€SimPOï¼‰çš„æ³›åŒ–æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰SFTè™½ç„¶é«˜æ•ˆä½†å› ä½¿ç”¨ç¦»çº¿ã€è„±ç­–ç•¥æ•°æ®ï¼Œæ³›åŒ–æ˜æ˜¾å¼±äºä½¿ç”¨åœ¨çº¿ï¼ˆon-policyï¼‰æ•°æ®çš„RLæ–¹æ³•ï¼Œå› æ­¤éœ€è¦åœ¨ä¸å¼•å…¥å¤æ‚RLè®­ç»ƒçš„å‰æä¸‹ï¼Œè®©SFTå…·å¤‡ç±»ä¼¼on-policyä¼˜åŠ¿ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æå‡ºåˆ†å¸ƒåˆ¤åˆ«ç†è®ºï¼ˆDDTï¼‰ç”¨äºå®šé‡åˆ»ç”»è®­ç»ƒæ•°æ®ä¸æ¨¡å‹è¯±å¯¼åˆ†å¸ƒçš„å¯¹é½ç¨‹åº¦ï¼Œå¹¶åŸºäºæ­¤è®¾è®¡ï¼š1ï¼‰åœ¨æŸå¤±å±‚é¢é‡æ–°åŠ æƒä¸è°ƒæ•´çš„åœ¨åˆ†å¸ƒå¾®è°ƒï¼ˆIDFTï¼‰ï¼Œ2ï¼‰é€šè¿‡ç‰¹æ®Šè§£ç /æç¤ºç­–ç•¥é‡æ„è®­ç»ƒè¯­æ–™åˆ†å¸ƒçš„æç¤ºè§£ç ï¼ˆHinted Decodingï¼‰ï¼Œä»è€Œå®ç°â€œOn-Policy SFTâ€ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼Œè¯¥On-Policy SFTæ¡†æ¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§èƒ½å¯ä¸DPOã€SimPOç­‰ä¸»æµç¦»çº¿RLç®—æ³•ç›¸å½“ï¼ŒåŒæ—¶ä¿ç•™SFTçš„è®¡ç®—ä¸å®ç°ç®€æ´æ€§ï¼Œä¸ºRLä¸æ˜“éƒ¨ç½²çš„åœºæ™¯æä¾›äº†å®ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å·²å¼€æºå®ç°ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, ç›‘ç£å¾®è°ƒ, On-Policy SFT, åˆ†å¸ƒåˆ¤åˆ«ç†è®º, In-Distribution Finetuning, Hinted Decoding, ç¦»çº¿å¼ºåŒ–å­¦ä¹ , DPO, SimPO, å¯¹é½æ³›åŒ–èƒ½åŠ›

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12222v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12222v1.pdf)

---

## [26. The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics](https://arxiv.org/abs/2602.12218v1)

**ä½œè€…**ï¼šChristian InternÃ², Jumpei Yamaguchi, Loren Amdahl-Culleton ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $Ï> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($Ï\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æŒ‡å‡ºï¼Œç”¨é«˜å®¹é‡ä¸‹æ¸¸é€‚é…æ¥è¯„ä¼°ä¸–ç•Œæ¨¡å‹ä¼šâ€œåŠ¨æ‘‡â€å…¶æ½œåœ¨è¡¨å¾ï¼Œä»è€Œæ©ç›–æ¨¡å‹å·²å­¦åˆ°çš„ç‰©ç†ç»“æ„ï¼Œä½å®¹é‡çº¿æ€§æ¢é’ˆæ›´èƒ½å¦‚å®åæ˜ æ½œåœ¨ç‰©ç†ä¸–ç•Œæ¨¡å‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è¯„ä¼°å¸¸é€šè¿‡å¾®è°ƒæˆ–é«˜å®¹é‡æ¢é’ˆæ¥æ£€æµ‹ç¥ç»ç½‘ç»œæ˜¯å¦å­¦åˆ°ç‰©ç†è§„å¾‹ï¼Œä½†è¿™äº›é€‚é…æœ¬èº«ä¼šæ”¹å˜æ½œåœ¨è¡¨ç¤ºï¼Œä½¿äººéš¾ä»¥åŒºåˆ†è‡ªç›‘ç£é˜¶æ®µçœŸæ­£å­¦åˆ°çš„ç‰©ç†ç»“æ„ä¸é€‚é…é˜¶æ®µæ–°å¼•å…¥çš„â€œæŠ•æœºæ·å¾„â€ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æå‡ºéä¾µå…¥è¯„ä¼°åè®® PhyIPï¼Œåœ¨æµä½“åŠ›å­¦å’Œè½¨é“åŠ›å­¦ä»»åŠ¡ä¸Šå†»ç»“è‡ªç›‘ç£è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä»…ç”¨ä½å®¹é‡çº¿æ€§æ¢é’ˆè§£ç æ½œåœ¨ç‰©ç†é‡ï¼ˆå¦‚å†…èƒ½ã€åå¹³æ–¹å®šå¾‹å‚æ•°ï¼‰ï¼Œå¹¶å¯¹æ¯”ä¸å¾®è°ƒç­‰é€‚é…å¼è¯„ä¼°åœ¨åˆ†å¸ƒå¤–æƒ…å½¢ä¸‹çš„è¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šå½“è‡ªç›‘ç£è¯¯å·®è¾ƒä½æ—¶ï¼Œç‰©ç†é‡åœ¨æ½œåœ¨ç©ºé—´ä¸­å˜å¾—çº¿æ€§å¯è§£ç ï¼ŒPhyIP å¯åœ¨ OOD æµ‹è¯•ä¸­æ¢å¤é«˜ç›¸å…³çš„å†…èƒ½å’Œç‰›é¡¿åå¹³æ–¹ç»“æ„ï¼Œè€ŒåŸºäºé€‚é…çš„è¯„ä¼°åè€Œä¼šç ´åè¿™ç§ç»“æ„ï¼Œè¯´æ˜å¯¹ç‰©ç†ä¸–ç•Œæ¨¡å‹çš„è¯„ä¼°åº”ä¼˜å…ˆä½¿ç”¨ä½å®¹é‡ã€éä¾µå…¥å¼æ¢é’ˆè€Œéé«˜å®¹é‡é€‚é…ã€‚

**å…³é”®è¯**ï¼šç¥ç»ç½‘ç»œ, è‡ªç›‘ç£å­¦ä¹ , ç‰©ç†ä¸–ç•Œæ¨¡å‹, çº¿æ€§å¯åˆ†è¡¨ç¤º, ä½å®¹é‡probe, æµä½“åŠ›å­¦æ¨¡æ‹Ÿ, è½¨é“åŠ›å­¦é¢„æµ‹, OODæ³›åŒ–åˆ†æ, agent

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12218v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12218v1.pdf)

---

## [27. Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction](https://arxiv.org/abs/2602.12204v1)

**ä½œè€…**ï¼šIbne Farabi Shihab, Sanjeda Akter, Anuj Sharma  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \textbf{88\%} of attention operations retrieve information already predictable from the model's hidden state, and this redundancy does \emph{not} decrease during training. Motivated by this observation, we introduce \textbf{\ours{}} (\textbf{C}onsolidation-based \textbf{R}outing for \textbf{A}daptive \textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \ours{} exhibits \emph{decreasing attention utilization} over training, achieving a \textbf{37.8$\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \emph{impossible} without consolidation: any static routing scheme requires $Î©(f \cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \ours{} achieves \textbf{100\% retrieval accuracy} at 1.6\% attention compute (vs.\ 68\% for baselines), and consolidated patterns transfer to unseen tasks with \textbf{48--52\%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($Î³= 0.43$ vs.\ $Î³_{\text{human}} \approx 0.4$--$0.5$). Code and benchmarks are available at [anonymized].

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§åä¸ºCRAMçš„â€œä¼šé—å¿˜æ³¨æ„åŠ›â€çš„è®°å¿†å·©å›ºæœºåˆ¶ï¼Œè®©æ¨¡å‹åœ¨è®­ç»ƒä¸­é€æ­¥å‡å°‘å¯¹æ³¨æ„åŠ›æ£€ç´¢çš„ä¾èµ–ï¼Œå¤§å¹…èŠ‚çœæ³¨æ„åŠ›è®¡ç®—è€Œä¿æŒç”šè‡³æå‡æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä½œè€…å‘ç°GPT-2ä¸­çº¦88%çš„æ³¨æ„åŠ›æ“ä½œåœ¨æ£€ç´¢æœ¬å°±å¯ç”±éšè—çŠ¶æ€é¢„æµ‹çš„ä¿¡æ¯ï¼Œä¸”è¿™ç§å†—ä½™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶ä¸ä¼šè‡ªç„¶ä¸‹é™ï¼Œå› æ­¤å¸Œæœ›è®¾è®¡ä¸€ç§æœºåˆ¶ï¼Œè®©æ¨¡å‹åœ¨é‡åˆ°é‡å¤æ¨¡å¼æ—¶èƒ½é€æ¸â€œå­¦ä¼šä¸çœ‹â€æ³¨æ„åŠ›ï¼Œä»è€Œè‡ªé€‚åº”é™ä½è®¡ç®—é‡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºCRAMï¼ˆConsolidation-based Routing for Adaptive Memoryï¼‰ï¼šé€šè¿‡å°†åå¤é€šè¿‡æ³¨æ„åŠ›æ£€ç´¢åˆ°çš„â€œæƒ…æ™¯è®°å¿†â€é€æ­¥è’¸é¦è¿›å‚æ•°åŒ–çš„â€œè¯­ä¹‰è®°å¿†â€ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªéšè®­ç»ƒè¿›ç¨‹æ¼”åŒ–çš„è·¯ç”±ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹åœ¨ç†Ÿæ‚‰æ¨¡å¼ä¸Šé€æ¸ç»•è¿‡æ³¨æ„åŠ›ï¼Œä»…åœ¨æ–°é¢–æˆ–æœªå·©å›ºçš„ä¿¡æ¯ä¸Šä½¿ç”¨æ³¨æ„åŠ›ã€‚

**ä¸»è¦ç»“è®º**ï¼šç†è®ºä¸Šè¯æ˜è‹¥æ²¡æœ‰å·©å›ºæœºåˆ¶ï¼Œä»»ä½•é™æ€è·¯ç”±åœ¨å«é‡å¤æ¨¡å¼çš„ä»»åŠ¡ä¸Šéƒ½éœ€è¦Î©(fÂ·n)çº§åˆ«çš„æ³¨æ„åŠ›ï¼›å®éªŒè¯æ˜CRAMåœ¨SRCDåŸºå‡†ä¸Šåœ¨ä»…1.6%çš„æ³¨æ„åŠ›è®¡ç®—ä¸‹ä»è¾¾100%æ£€ç´¢å‡†ç¡®ç‡ï¼Œå¹¶åœ¨çº¦3Kæ­¥å‡ºç°æ³¨æ„åŠ›ä½¿ç”¨çš„ç›¸å˜ï¼ˆæ€»ä½“å‡å°‘37.8å€ï¼‰ï¼Œä¸”å…¶å·©å›ºåŠ¨åŠ›å­¦ä¸äººç±»ä»æƒ…æ™¯è®°å¿†åˆ°è¯­ä¹‰è®°å¿†çš„è½¬å˜æ›²çº¿é«˜åº¦ä¸€è‡´ï¼Œå¹¶èƒ½é›¶æ ·æœ¬è¿ç§»åˆ°æ–°ä»»åŠ¡æ—¶ç»§ç»­èŠ‚çœçº¦50%çš„æ³¨æ„åŠ›ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, transformer, æ³¨æ„åŠ›æœºåˆ¶, è®°å¿†å·©å›º, è‡ªé€‚åº”è®¡ç®—, ç¨€ç–è·¯ç”±, å‚æ•°åŒ–è¯­ä¹‰è®°å¿†, åºåˆ—å»ºæ¨¡, è¯­è¨€æ¨¡å‹

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12204v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12204v1.pdf)

---

## [28. How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics](https://arxiv.org/abs/2602.12180v1)

**ä½œè€…**ï¼šYurong Chen, Yu He, Michael I. Jordan ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.GT  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ä»ç†è®ºä¸Šåˆ†æäº†é‡‡æ ·ç­–ç•¥å’Œå‚è€ƒç­–ç•¥å¦‚ä½•å¡‘é€ åå¥½å¯¹é½è®­ç»ƒä¸­çš„LLMè¡Œä¸ºï¼Œå¹¶æ­ç¤ºå…¶å¯èƒ½å¯¼è‡´æ›´å¼ºæ’åºæ€§èƒ½ã€è¿‡åº¦é›†ä¸­ç‰¹æ€§ä»¥åŠè¿­ä»£è®­ç»ƒä¸­çš„æŒ¯è¡æˆ–ç†µåç¼©ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰ä¸»æµåå¥½å¯¹é½æ–¹æ³•ï¼ˆå¦‚IPO/DPOï¼‰å¤§é‡ä¾èµ–ä»æ¨¡å‹é‡‡æ ·çš„å€™é€‰å›ç­”å’Œå‚è€ƒç­–ç•¥ï¼Œä½†é‡‡æ ·æ–¹å¼å’Œå‚è€ƒé€‰å–å¯¹æœ€ç»ˆå¯¹é½æ•ˆæœå’Œç¨³å®šæ€§çš„ä½œç”¨ç¼ºä¹ç³»ç»Ÿç†è®ºç†è§£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨â€œIdentity Preference Optimizationâ€æ¡†æ¶ä¸‹ï¼Œå½¢å¼åŒ–åˆ†æä¸åŒå®ä¾‹ç›¸å…³é‡‡æ ·å’Œåç½®çš„on-policyé‡‡æ ·å¯¹æ’åºä¿è¯å’Œåˆ†å¸ƒç†µçš„å½±å“ï¼Œå¹¶å»ºç«‹è¿­ä»£å¯¹é½åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆè®­ç»ƒç­–ç•¥åå“ºåç»­é‡‡æ ·ä¸å‚è€ƒï¼‰ï¼Œæ¨å¯¼å‡ºå¯èƒ½å‡ºç°æŒ¯è¡ã€ç†µåç¼©åŠå…¶ç¨³å®šæ¡ä»¶ï¼Œå¹¶å°†ç†è®ºæ‰©å±•åˆ°DPOç­‰æ–¹æ³•ã€‚

**ä¸»è¦ç»“è®º**ï¼šåˆç†çš„ã€å®ä¾‹ä¾èµ–çš„é‡‡æ ·å¯ä»¥æ˜¾è‘—æå‡åå¥½æ’åºè´¨é‡ï¼Œè€Œè¿‡åº¦åå‘å½“å‰ç­–ç•¥çš„é‡‡æ ·åœ¨ç»“æ„åŒ–åå¥½ä¸‹ä¼šå¯¼è‡´åˆ†å¸ƒè¿‡åº¦é›†ä¸­ï¼›åœ¨è¿­ä»£ç”Ÿæˆåå¥½æ•°æ®çš„å¯¹é½æµç¨‹ä¸­ï¼Œä¸æ°å½“çš„å‚æ•°ä¼šäº§ç”ŸæŒç»­æŒ¯è¡æˆ–ç†µåç¼©ï¼Œè€Œåœ¨ç‰¹å®šå‚æ•°ä¸è®¾å®šä¸‹å¯ä¿è¯æ”¶æ•›ä¸ç¨³å®šï¼Œè¿™äº›ç°è±¡åŒæ ·é€‚ç”¨äºæ›´å¹¿æ³›çš„åå¥½å¯¹é½æ–¹æ³•å¹¶å¾—åˆ°çœŸå®æ•°æ®å®éªŒçš„æ”¯æŒã€‚

**å…³é”®è¯**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹, åå¥½å¯¹é½, DirectPreferenceOptimization, é‡‡æ ·ç­–ç•¥, å‚è€ƒç­–ç•¥, è¿­ä»£è®­ç»ƒåŠ¨æ€, ç¨³å®šæ€§åˆ†æ, ç†µå¡Œç¼©, äººç±»åé¦ˆå­¦ä¹ , ç†è®ºåˆ†æå®éªŒéªŒè¯, llm

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12180v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12180v1.pdf)

---

## [29. Amortized Molecular Optimization via Group Relative Policy Optimization](https://arxiv.org/abs/2602.12162v1)

**ä½œè€…**ï¼šMuhammad bin Javaid, Hasham Hussain, Ashima Khanna ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Molecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as "Instance Optimizers'', expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºGRXFormä¸Group Relative Policy Optimizationæ–¹æ³•ï¼Œå®ç°å¯¹åˆ†å­ç»“æ„çš„æ‘Šé”€å¼ä¼˜åŒ–ï¼Œå¹¶åœ¨æ— éœ€æ¨ç†æ—¶è°ƒç”¨æ‰“åˆ†å™¨çš„å‰æä¸‹å–å¾—æ¥è¿‘å®ä¾‹ä¼˜åŒ–å™¨çš„æ€§èƒ½ä¸”å…·å¤‡æ›´å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åˆ†å­ç»“æ„ä¼˜åŒ–æ–¹æ³•å¤šä¸ºå¯¹æ¯ä¸ªè¾“å…¥ç»“æ„å•ç‹¬æœç´¢çš„â€œå®ä¾‹ä¼˜åŒ–å™¨â€ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥å°†æœç´¢ç­–ç•¥æ³›åŒ–åˆ°æ–°åˆ†å­ç»“æ„ï¼Œè€ŒåŸºäºæ¨¡å‹çš„ç­–ç•¥ä¼˜åŒ–åˆå› ä¸åŒèµ·å§‹åˆ†å­éš¾åº¦å·®å¼‚å¤§è€Œå¯¼è‡´é«˜æ–¹å·®ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åŸºäºé¢„è®­ç»ƒGraph Transformeræ„å»ºGRXFormï¼Œé€šè¿‡åºåˆ—åŒ–çš„åŸå­å’Œé”®æ·»åŠ æ¥ä¼˜åŒ–åˆ†å­ï¼Œå¹¶æå‡ºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼ŒæŒ‰èµ·å§‹åˆ†å­åˆ†ç»„å¹¶ä½¿ç”¨ç›¸å¯¹å¥–åŠ±å½’ä¸€åŒ–ï¼Œä»¥é™ä½ç­–ç•¥æ¢¯åº¦æ–¹å·®å¹¶å®ç°é¢å‘ç›®æ ‡çš„å¾®è°ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¯æ˜GRXFormåœ¨å¤šç›®æ ‡åˆ†å­ä¼˜åŒ–ä»»åŠ¡ä¸Šèƒ½å¯¹åˆ†å¸ƒå¤–éª¨æ¶å®ç°æœ‰æ•ˆæ³›åŒ–ï¼Œåœ¨æ— éœ€æ¨ç†æ—¶è°ƒç”¨ç›®æ ‡æ‰“åˆ†å™¨æˆ–é¢å¤–ç²¾ä¿®çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¸é¢†å…ˆçš„å®ä¾‹ä¼˜åŒ–å™¨ç›¸ç«äº‰ï¼Œä»è€Œå±•ç¤ºäº†æ‘Šé”€å¼åˆ†å­ä¼˜åŒ–çš„æ•ˆç‡ä¸å®ç”¨æ€§ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , transformer, å¼ºåŒ–å­¦ä¹ , ç­–ç•¥ä¼˜åŒ–, åˆ†å­ç”Ÿæˆ, å›¾ç¥ç»ç½‘ç»œ, å¤šç›®æ ‡ä¼˜åŒ–, ç¦»çº¿è®­ç»ƒ, ç»“æ„ä¼˜åŒ–

**è¯„åˆ†**ï¼š38

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12162v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12162v1.pdf)

---

## [30. SafeNeuron: Neuron-Level Safety Alignment for Large Language Models](https://arxiv.org/abs/2602.12158v1)

**ä½œè€…**ï¼šZhaoxin Wang, Jiaming Liang, Fengbin Zhu ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSafeNeuron é€šè¿‡åœ¨ç¥ç»å…ƒå±‚é¢åˆ†æ•£å®‰å…¨è¡¨ç¤ºã€æ„å»ºå†—ä½™å®‰å…¨é€šè·¯ï¼Œæ˜¾è‘—æå‡å¤§æ¨¡å‹çš„å®‰å…¨é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å®‰å…¨å¯¹é½é›†ä¸­åœ¨å°‘é‡å‚æ•°å’Œè¡Œä¸ºå±‚é¢ï¼Œå®¹æ˜“è¢«ç¥ç»å…ƒå‰ªæç­‰æ”»å‡»ç»•è¿‡ï¼Œä¸”éš¾ä»¥ç›´æ¥æ§åˆ¶æ¨¡å‹å†…éƒ¨çš„å®‰å…¨æœºåˆ¶ï¼Œå› æ­¤éœ€è¦æ›´ç²¾ç»†ã€æ›´ç¨³å¥çš„å†…éƒ¨å¯¹é½æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šSafeNeuron é¦–å…ˆè¯†åˆ«è´Ÿè´£å®‰å…¨è¡Œä¸ºçš„ç¥ç»å…ƒå¹¶åœ¨åå¥½ä¼˜åŒ–é˜¶æ®µå†»ç»“è¿™äº›ç¥ç»å…ƒï¼Œè¿«ä½¿æ¨¡å‹åœ¨å…¶ä»–ç¥ç»å…ƒä¸­é‡å»ºå¹¶å†—ä½™åŒ–å®‰å…¨è¡¨ç¤ºï¼Œä»è€Œåœ¨ç½‘ç»œå„å±‚åˆ†å¸ƒæ›´å‡åŒ€çš„å®‰å…¨è¡¨å¾ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒSafeNeuron èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨ç¥ç»å…ƒå‰ªææ”»å‡»ä¸‹çš„å®‰å…¨é²æ£’æ€§ï¼Œé™ä½å¼€æºæ¨¡å‹è¢«å½“ä½œçº¢é˜Ÿç”Ÿæˆå™¨æ»¥ç”¨çš„é£é™©ï¼Œä¸”åŸºæœ¬ä¸æŸä¼¤é€šç”¨èƒ½åŠ›ï¼›å±‚çº§åˆ†æè¿›ä¸€æ­¥æ˜¾ç¤ºå®‰å…¨è¡Œä¸ºä¾æ‰˜ç¨³å®šä¸”å¯å…±äº«çš„å†…éƒ¨è¡¨ç¤ºï¼Œä¸ºå®‰å…¨å¯¹é½æä¾›äº†æ›´å¯è§£é‡Šçš„è§†è§’ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, å®‰å…¨å¯¹é½, ç¥ç»ç½‘ç»œ, å¤šæ¨¡æ€LLM, åå¥½ä¼˜åŒ–, ç¥ç»å…ƒå‰ªææ”»å‡», è¡¨ç¤ºå­¦ä¹ , æ¨¡å‹é²æ£’æ€§

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12158v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12158v1.pdf)

---

