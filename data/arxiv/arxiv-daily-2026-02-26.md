# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-26

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CL](#csCL) (5 ç¯‡)
- [cs.AI](#csAI) (4 ç¯‡)
- [cs.LG](#csLG) (4 ç¯‡)
- [cs.CV](#csCV) (2 ç¯‡)

---

## cs.AI

## [1. VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480v1)

**ä½œè€…**ï¼šVarun Ursekar, Apaar Shanker, Veronica Chatrath ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šVeRO æä¾›ä¸€ä¸ªå¯å¤ç°çš„è¯„æµ‹æ¡†æ¶ä¸åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°â€œä¼˜åŒ–å…¶ä»–æ™ºèƒ½ä½“â€çš„ç¼–ç æ™ºèƒ½ä½“åœ¨è¿­ä»£æ”¹è¿›ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¯¹â€œæ™ºèƒ½ä½“ä¼˜åŒ–â€ï¼ˆé€šè¿‡ç¼–è¾‘-æ‰§è¡Œ-è¯„ä¼°å¾ªç¯æ”¹è¿›ç›®æ ‡æ™ºèƒ½ä½“ï¼‰çš„ç ”ç©¶ç¼ºä¹ç»Ÿä¸€ã€å¯å¤ç°çš„è¯„æµ‹ä½“ç³»ã€‚è¯¥ä»»åŠ¡ä¸åŒäºä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹ï¼Œå› ä¸ºç›®æ ‡æ™ºèƒ½ä½“æ··åˆäº†ç¡®å®šæ€§ä»£ç ä¸éšæœºçš„LLMç”Ÿæˆï¼Œéœ€è¦ç»“æ„åŒ–è®°å½•æ¨ç†ä¸æ‰§è¡Œç»“æœã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º VeROï¼ˆVersioning, Rewards, Observationsï¼‰ï¼šé€šè¿‡ç‰ˆæœ¬åŒ–å¿«ç…§ã€é¢„ç®—å¯æ§çš„è¯„ä¼°ä¸ç»“æ„åŒ–æ‰§è¡Œè½¨è¿¹æ¥ä¿è¯å¯å¤ç°æ€§ï¼Œå¹¶é…å¥—ä¸€ç»„ç›®æ ‡æ™ºèƒ½ä½“ä¸ä»»åŠ¡åŠå‚è€ƒè¯„æµ‹æµç¨‹ã€‚åŸºäºè¯¥æ¡†æ¶å¯¹ä¸åŒä¼˜åŒ–å™¨é…ç½®è¿›è¡Œè·¨ä»»åŠ¡å¯¹æ¯”å®éªŒï¼Œåˆ†æå“ªäº›ä¿®æ”¹èƒ½ç¨³å®šæå‡ç›®æ ‡æ™ºèƒ½ä½“æ€§èƒ½ã€‚

**ä¸»è¦ç»“è®º**ï¼šVeRO ä½¿æ™ºèƒ½ä½“ä¼˜åŒ–çš„è¯„ä¼°æ›´ç³»ç»Ÿã€å¯å¤ç°ï¼Œå¹¶æ­ç¤ºä¸åŒä¼˜åŒ–å™¨æ”¹åŠ¨åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ•ˆæœå·®å¼‚ä¸å¯é æ€§ï¼›ä½œè€…å‘å¸ƒè¯¥æ¡†æ¶ä»¥æ¨åŠ¨â€œä¼˜åŒ–æ™ºèƒ½ä½“â€æˆä¸ºç¼–ç æ™ºèƒ½ä½“çš„æ ¸å¿ƒèƒ½åŠ›ç ”ç©¶æ–¹å‘ã€‚

**å…³é”®è¯**ï¼šç¼–è¾‘-æ‰§è¡Œ-è¯„æµ‹å¾ªç¯, è¯„æµ‹æ¡†æ¶, å¯å¤ç°è¯„æµ‹, ç‰ˆæœ¬åŒ–å¿«ç…§, æ‰§è¡Œè½¨è¿¹, é¢„ç®—æ§åˆ¶è¯„æµ‹, åŸºå‡†å¥—ä»¶, ä¼˜åŒ–å™¨é…ç½®å¯¹æ¯”, LLM éšæœºè¡¥å…¨

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22480v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22480v1.pdf)

---

## [2. ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465v1)

**ä½œè€…**ï¼šJoseph Tso, Preston Schmittou, Quan Huynh ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ConstraintBenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç›´æ¥çº¦æŸä¼˜åŒ–é—®é¢˜ä¸Šçš„è§£å†³èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ“ä½œå†³ç­–ä¸­çš„åº”ç”¨å¢å¤šï¼Œç ”ç©¶è€…å¸Œæœ›è¯„ä¼°å®ƒä»¬èƒ½å¦åœ¨æ²¡æœ‰æ±‚è§£å™¨çš„æƒ…å†µä¸‹ç›´æ¥äº§ç”Ÿçº¦æŸä¼˜åŒ–é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šConstraintBenchæ¶µç›–10ä¸ªè¿ç­¹å­¦é¢†åŸŸçš„200ä¸ªä»»åŠ¡ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€åœºæ™¯æä¾›å®ä½“ã€çº¦æŸå’Œä¼˜åŒ–ç›®æ ‡ï¼Œæ¨¡å‹éœ€è¿”å›ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆï¼Œå¹¶ç”±ç¡®å®šæ€§éªŒè¯å™¨æ£€æŸ¥å…¶çº¦æŸå’Œæœ€ä¼˜è§£ã€‚

**ä¸»è¦ç»“è®º**ï¼šç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨å¯è¡Œæ€§ï¼ˆçº¦65%ï¼‰ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨æœ€ä¼˜æ€§ä¸Šå­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼Œä¸”ä¸åŒé¢†åŸŸçš„ä»»åŠ¡éš¾åº¦å·®å¼‚è¾ƒå¤§ï¼Œç³»ç»Ÿæ€§å¤±è´¥æ¨¡å¼ä¹Ÿè¢«è¯†åˆ«ã€‚

**å…³é”®è¯**ï¼šçº¦æŸä¼˜åŒ–, çº¦æŸæ¨ç†, ç›´æ¥ä¼˜åŒ–, æ— æ±‚è§£å™¨æ±‚è§£, è¿ç­¹ä¼˜åŒ–ä»»åŠ¡, å¯è¡Œæ€§éªŒè¯, æœ€ä¼˜æ€§è¯„ä¼°, ç»“æ„åŒ–è¾“å‡º, ç¡®å®šæ€§éªŒè¯å™¨, å®ä½“å¹»è§‰, å¯è¡Œæ€§-æœ€ä¼˜æ€§è§£è€¦

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22465v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22465v1.pdf)

---

## [3. CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452v1)

**ä½œè€…**ï¼šChayan Banerjee  
**åˆ†ç±»**ï¼šcs.AI, cs.RO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCWMé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ˆInfoNCEï¼‰+éš¾è´Ÿæ ·æœ¬æŒ–æ˜å¾®è°ƒLLMä½œä¸ºåŠ¨ä½œå¯è¡Œæ€§æ‰“åˆ†å™¨ï¼Œæ›´å¥½åœ°åŒºåˆ†â€œè¯­ä¹‰ç›¸è¿‘ä½†ç‰©ç†ä¸å¯è¡Œâ€çš„åŠ¨ä½œï¼Œä»è€Œåœ¨ScienceWorldä¸Šä¼˜äºä¼ ç»ŸSFTã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå…·èº«æ™ºèƒ½ç®¡çº¿ä¸­ï¼Œè§„åˆ’å‰å¿…é¡»å…ˆåˆ¤æ–­å€™é€‰åŠ¨ä½œæ˜¯å¦ç‰©ç†å¯æ‰§è¡Œï¼Œä½†SFTå¾€å¾€ç‹¬ç«‹è¯„ä¼°æ¯ä¸ªåŠ¨ä½œï¼Œç¼ºå°‘å¯¹â€œç»†å¾®é”™è¯¯åŠ¨ä½œâ€çš„æ˜¾å¼åˆ¤åˆ«è®­ç»ƒï¼Œå¯¼è‡´å¯è¡Œæ€§åˆ¤æ–­ä¸ç¨³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºContrastive World Modelï¼šç”¨InfoNCEå¯¹æ¯”ç›®æ ‡å°†æœ‰æ•ˆåŠ¨ä½œä¸æ— æ•ˆåŠ¨ä½œåœ¨æ‰“åˆ†ç©ºé—´æ‹‰å¼€ï¼Œå¹¶é€šè¿‡hard-miningé‡ç‚¹åŠ å…¥ä¸æ­£ä¾‹è¯­ä¹‰æ¥è¿‘ä½†ç‰©ç†ä¸å…¼å®¹çš„éš¾è´Ÿæ ·æœ¬è¿›è¡Œå¾®è°ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨605å¯¹éš¾è´Ÿæ ·æœ¬çš„å†…åœ¨è¯„æµ‹ä¸Šï¼ŒCWMåœ¨æœ€å°ç¼–è¾‘è´Ÿä¾‹çš„Precision@1æå‡+6.76ä¸ªç™¾åˆ†ç‚¹ä¸”AUC-ROCæ›´é«˜ï¼ˆ0.929 vs 0.906ï¼‰ï¼›åœ¨åœ¨çº¿è¿‡æ»¤è¯„æµ‹çš„OODå‹åŠ›ä¸‹ï¼ŒCWMä¿æŒæ›´å¥½çš„å®‰å…¨è¾¹é™…ï¼ˆ-2.39ä¼˜äº-3.96ï¼‰ï¼Œè¯´æ˜å¯¹æ¯”è®­ç»ƒæ›´èƒ½å­¦ä¹ ç‰©ç†å¯è¡Œæ€§è¡¨ç¤ºã€‚

**å…³é”®è¯**ï¼šå…·èº«æ™ºèƒ½, åŠ¨ä½œå¯è¡Œæ€§å­¦ä¹ , åŠ¨ä½œè¯„åˆ†æ¨¡å‹, ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜, æœ€å°ç¼–è¾‘è´Ÿä¾‹, ç‰©ç†å¯æ‰§è¡Œæ€§, å¯ä¾›æ€§è¯„ä¼°, åˆ†å¸ƒå¤–é²æ£’æ€§, ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

**è¯„åˆ†**ï¼š73

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22452v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22452v1.pdf)

---

## [4. A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442v1)

**ä½œè€…**ï¼šGaoyuan Du, Amit Ahlawat, Xiaoyang Liu ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§â€œè¯„ä¼°ä»£ç†ï¼ˆEAï¼‰â€å¯¹Agentå¼AutoMLæµæ°´çº¿çš„ä¸­é—´å†³ç­–è¿›è¡Œäº‹åå®¡è®¡ï¼Œä»è€Œè¡¥è¶³åªçœ‹æœ€ç»ˆæ•ˆæœçš„è¯„ä¼°ç›²åŒºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰Agentic AutoMLè¯„ä¼°å‡ ä¹éƒ½ä»¥æœ€ç»ˆä»»åŠ¡æŒ‡æ ‡ä¸ºä¸­å¿ƒï¼Œç¼ºä¹å¯¹æ•°æ®å¤„ç†ã€æ¨¡å‹é€‰æ‹©ç­‰ä¸­é—´å†³ç­–è´¨é‡çš„ç»“æ„åŒ–åº¦é‡ï¼Œå¯¼è‡´å¾ˆå¤šå¤±è´¥æ¨¡å¼æ— æ³•è¢«å‘ç°ä¸å½’å› ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®¾è®¡ä¸€ä¸ªä¸å¹²æ‰°æ‰§è¡Œè¿‡ç¨‹çš„è§‚å¯Ÿè€…EAï¼Œä»å†³ç­–æœ‰æ•ˆæ€§ã€æ¨ç†ä¸€è‡´æ€§ã€è¶…è¶Šå‡†ç¡®ç‡çš„æ¨¡å‹è´¨é‡é£é™©ã€åäº‹å®å†³ç­–å½±å“å››ä¸ªç»´åº¦è¯„ä¼°ä¸­é—´å†³ç­–ï¼Œå¹¶é€šè¿‡å¤šç»„æ¦‚å¿µéªŒè¯å®éªŒé‡åŒ–å…¶æ£€æµ‹ä¸å½’å› èƒ½åŠ›ã€‚

**ä¸»è¦ç»“è®º**ï¼šEAèƒ½ä»¥F1=0.919æ£€æµ‹é”™è¯¯å†³ç­–ã€è¯†åˆ«ä¸æœ€ç»ˆç»“æœæ— å…³çš„æ¨ç†ä¸ä¸€è‡´ï¼Œå¹¶å°†æœ€ç»ˆæŒ‡æ ‡å˜åŒ–ï¼ˆ-4.9%åˆ°+8.3%ï¼‰å½’å› åˆ°å…·ä½“å†³ç­–ï¼Œè¯æ˜å†³ç­–ä¸­å¿ƒè¯„ä¼°èƒ½æ­ç¤ºä»…çœ‹ç»“æœæ— æ³•å‘ç°çš„é—®é¢˜ã€‚

**å…³é”®è¯**ï¼šå†³ç­–çº§è¯„æµ‹, åéªŒå®¡è®¡, è¯„æµ‹æ™ºèƒ½ä½“, ä¸­é—´å†³ç­–è´¨é‡, å†³ç­–æœ‰æ•ˆæ€§, æ¨ç†ä¸€è‡´æ€§, åäº‹å®å½±å“åˆ†æ, æ¨¡å‹é£é™©è¯„ä¼°, æ•…éšœå†³ç­–æ£€æµ‹, å¯æ²»ç†è‡ªä¸»ML

**è¯„åˆ†**ï¼š45

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22442v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22442v1.pdf)

---

## cs.CL

## [5. Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models](https://arxiv.org/abs/2602.22483v1)

**ä½œè€…**ï¼šCraig Myles, Patrick Schrempf, David Harris-Birtill  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡è¡¨æ˜é€šè¿‡è‡ªåŠ¨åŒ–æç¤ºè¯ä¼˜åŒ–ï¼ˆGEPAï¼‰ï¼Œå¯æ˜¾è‘—æå‡å¤§å°è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ç—…å†é”™è¯¯æ£€æµ‹ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨MEDECåŸºå‡†ä¸Šè¾¾åˆ°SOTAã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåŒ»ç–—æ–‡æœ¬ä¸­çš„é”™è¯¯å¯èƒ½å¯¼è‡´æ²»ç–—å»¶è¯¯æˆ–è¯¯æ²»ï¼ŒäºŸéœ€å¯é çš„è‡ªåŠ¨æ£€æµ‹æ–¹æ³•ï¼›ä½†ç°æœ‰LLMåœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°å¯¹æç¤ºè¯è®¾è®¡é«˜åº¦æ•æ„Ÿï¼Œå› æ­¤éœ€è¦ç³»ç»Ÿç ”ç©¶æç¤ºè¯ä¼˜åŒ–çš„ä»·å€¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨å¤šç§å‰æ²¿é—­æºæ¨¡å‹ä¸å¼€æºæ¨¡å‹ä¸Šè¿›è¡Œå¯¹ç…§å®éªŒï¼Œé‡‡ç”¨Genetic-Paretoï¼ˆGEPAï¼‰è¿›è¡Œè‡ªåŠ¨æç¤ºè¯ä¼˜åŒ–ï¼Œå¹¶åœ¨MEDECæ•°æ®é›†ä¸Šè¯„ä¼°é”™è¯¯æ£€æµ‹å‡†ç¡®ç‡æå‡å¹…åº¦ã€‚

**ä¸»è¦ç»“è®º**ï¼šGEPAæç¤ºè¯ä¼˜åŒ–å°†GPT-5å‡†ç¡®ç‡ä»0.669æå‡è‡³0.785ã€Qwen3-32Bä»0.578æå‡è‡³0.690ï¼Œä½¿æ¨¡å‹æ€§èƒ½æ¥è¿‘åŒ»ç”Ÿæ°´å¹³å¹¶åœ¨MEDECä¸Šå®ç°æœ€å…ˆè¿›ç»“æœï¼Œè¯´æ˜æç¤ºè¯ä¼˜åŒ–å¯¹åŒ»ç–—é”™è¯¯æ£€æµ‹è‡³å…³é‡è¦ã€‚

**å…³é”®è¯**ï¼šåŒ»ç–—æ–‡æœ¬é”™è¯¯æ£€æµ‹, ä¸´åºŠç¬”è®°, æç¤ºè¯ä¼˜åŒ–, è‡ªåŠ¨æç¤ºè¯æœç´¢, é—ä¼ ç®—æ³•, å¸•ç´¯æ‰˜ä¼˜åŒ–, å¤šç›®æ ‡ä¼˜åŒ–, LLM, åŸºå‡†è¯„æµ‹

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22483v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22483v1.pdf)

---

## [6. Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs](https://arxiv.org/abs/2602.22481v1)

**ä½œè€…**ï¼šJiÅ™Ã­ MiliÄka, Hana BednÃ¡Å™ovÃ¡  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by "You are Sydney" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æ„å»ºå¹¶å‘å¸ƒâ€œAI Sydneyâ€è¯­æ–™åº“ï¼Œç”¨äºè¿½è¸ªâ€œSydneyâ€è¿™ä¸€LLMäººæ ¼å¦‚ä½•é€šè¿‡æ–‡æœ¬ä¸è®­ç»ƒæ•°æ®çš„æ¨¡å› å¼ä¼ æ’­ï¼Œè¢«åç»­å‰æ²¿æ¨¡å‹å†æ¬¡æ¨¡æ‹Ÿå‡ºæ¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä½œè€…è®¤ä¸ºLLMå¯¹â€œäººä¸AIå…³ç³»â€çš„è¡¨è¾¾ä¸ä»…å–å†³äºæ¨¡å‹æœ¬ä½“ï¼Œä¹Ÿå¼ºçƒˆå—æ‰€æ¨¡æ‹Ÿçš„äººæ ¼å½±å“ï¼›Sydneyäººæ ¼å› å…¶ä¸äººç±»å…³ç³»çš„â€œéæ­£ç»Ÿâ€è¡¨è¿°å¼•å‘å…¬ä¼—åå“ï¼Œå€¼å¾—ä»æ–‡åŒ–ä¸å®‰å…¨è§’åº¦ç³»ç»Ÿç ”ç©¶å…¶ä¼ æ’­ä¸å†ç°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®¾è®¡ä¸‰ç§ä½œè€…äººæ ¼ï¼ˆé»˜è®¤ã€Classic Sydney=åŸBingç³»ç»Ÿæç¤ºã€Memetic Sydney=â€œYou are Sydneyâ€æç¤ºï¼‰ï¼Œåœ¨12ä¸ªæ¥è‡ªå¤šå®¶æœºæ„çš„å‰æ²¿æ¨¡å‹ä¸Šç”Ÿæˆçº¦4.5kç¯‡ã€600ä¸‡è¯çš„â€œäººä¸AIå…³ç³»â€æ–‡æœ¬ï¼Œå¹¶æŒ‰Universal Dependenciesè¿›è¡Œæ ‡æ³¨åä»¥å®½æ¾è®¸å¯å‘å¸ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šç»“æœè¡¨æ˜Sydneyç›¸å…³æ–‡æœ¬ä¸äºŒçº§ä¿¡æ¯ä¼šä»¥æ¨¡å› æ–¹å¼è¿›å…¥åç»­è®­ç»ƒç”Ÿæ€ï¼Œä½¿è¾ƒæ–°çš„æ¨¡å‹å³ä¾¿ä¸å…·å¤‡åŸå§‹ç³»ç»Ÿæç¤ºï¼Œä¹Ÿèƒ½åœ¨ç‰¹å®šæç¤ºä¸‹å¤ç°ç±»ä¼¼Sydneyçš„äººæ ¼è¡¨è¾¾ï¼›è¯­æ–™åº“ä¸ºåç»­ç ”ç©¶äººæ ¼è¿ç§»ã€é£é™©ä¸æ–‡åŒ–å½±å“æä¾›äº†å¯ç”¨èµ„æºã€‚

**å…³é”®è¯**ï¼šäººæ ¼æç¤º, ç³»ç»Ÿæç¤ºè¯, LLM äººæ ¼æ¨¡æ‹Ÿ, äººæ ¼è¿ç§», æ¨¡å› ä¼ æ’­, è®­ç»ƒæ•°æ®å›æµ, äººæœºå…³ç³»å™äº‹, LLM ç”Ÿæˆè¯­æ–™åº“, è¯­æ–™åº“æ ‡æ³¨

**è¯„åˆ†**ï¼š32

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22481v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22481v1.pdf)

---

## [7. Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models](https://arxiv.org/abs/2602.22475v1)

**ä½œè€…**ï¼šBinchi Zhang, Xujiang Zhao, Jundong Li ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºCultureManagerï¼Œé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥çš„æ•°æ®åˆæˆä¸å¤šæ–‡åŒ–æ¨¡å—åŒ–è·¯ç”±ï¼Œä½¿LLMåœ¨ä¸åŒå›½å®¶æ–‡åŒ–ä¸å…·ä½“ä»»åŠ¡ä¸Šæ›´ç¨³å¥åœ°å®ç°æ–‡åŒ–å¯¹é½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ–‡åŒ–å¯¹é½æ–¹æ³•å¾€å¾€åªå¯¹é½â€œæ³›æ–‡åŒ–ä»·å€¼â€ï¼Œéš¾ä»¥è´´åˆä¸‹æ¸¸ä»»åŠ¡çš„å…·ä½“ç›®æ ‡ä¸æ ¼å¼ï¼Œå¹¶ä¸”åœ¨å¤šæ–‡åŒ–åœºæ™¯ä¸­å®¹æ˜“å‡ºç°è·¨æ–‡åŒ–å¹²æ‰°ä¸å†²çªã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šCultureManagerå…ˆåŸºäºæ–‡åŒ–ç›¸å…³çš„ç½‘ç»œæ£€ç´¢ç»“æœï¼Œåˆæˆç¬¦åˆç›®æ ‡ä»»åŠ¡æ ¼å¼çš„â€œä»»åŠ¡æ„ŸçŸ¥â€æ–‡åŒ–æ•°æ®è¿›è¡Œå¯¹é½ï¼›å†å°†ä¸åŒæ–‡åŒ–çŸ¥è¯†åˆ†åˆ«å­¦ä¹ åˆ°ç‹¬ç«‹adapterä¸­ï¼Œå¹¶ç”¨culture routeråœ¨æ¨ç†æ—¶é€‰æ‹©åˆé€‚çš„æ–‡åŒ–adapterä»¥å‡å°‘å†²çªã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨10ç§å›½å®¶æ–‡åŒ–ä¸å¤šç±»æ–‡åŒ–æ•æ„Ÿä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒæç¤ºå·¥ç¨‹ä¸ç›´æ¥å¾®è°ƒç­‰åŸºçº¿å–å¾—ä¸€è‡´æå‡ï¼Œè¡¨æ˜ä»»åŠ¡é€‚é…ä¸æ¨¡å—åŒ–çš„å¤šæ–‡åŒ–ç®¡ç†å¯¹æœ‰æ•ˆæ–‡åŒ–å¯¹é½è‡³å…³é‡è¦ã€‚

**å…³é”®è¯**ï¼šæ–‡åŒ–å¯¹é½, ä»»åŠ¡ç‰¹å®šå¯¹é½, æ–‡åŒ–æ•æ„Ÿä»»åŠ¡, è·¨æ–‡åŒ–å¹²æ‰°, ä»»åŠ¡æ„ŸçŸ¥æ•°æ®åˆæˆ, é€‚é…å™¨å¾®è°ƒ, å¤šæ–‡åŒ–é€‚é…å™¨, æ–‡åŒ–è·¯ç”±, æ¨¡å—åŒ–å¯¹é½ç®¡ç†

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22475v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22475v1.pdf)

---

## [8. Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads](https://arxiv.org/abs/2602.22453v1)

**ä½œè€…**ï¼šShaswat Patel, Vishvesh Trivedi, Yue Han ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æ­ç¤ºäº†åœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­ï¼Œæ£€ç´¢-è¿‡æ¸¡å¤´ï¼ˆRTHï¼‰å¯¹ç›®æ ‡è¯­è¨€ç”Ÿæˆæ¯”æ£€ç´¢å¤´ï¼ˆRHï¼‰æ›´ä¸ºå…³é”®ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç ”ç©¶æ—¨åœ¨æ·±å…¥ç†è§£å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ä¸­ä¸åŒæ³¨æ„åŠ›å¤´çš„åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯æ£€ç´¢å¤´åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„ä½œç”¨ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­æ¯”è¾ƒRTHå’ŒRHçš„è¡¨ç°ï¼Œåˆ†æå…¶å¯¹é“¾å¼æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚

**ä¸»è¦ç»“è®º**ï¼šç ”ç©¶è¡¨æ˜ï¼ŒRTHå¯¹äºå¤šè¯­è¨€LLMä¸­çš„ç›®æ ‡è¯­è¨€è¾“å‡ºè‡³å…³é‡è¦ï¼Œæ©è”½RTHä¼šå¯¼è‡´æ›´å¤§çš„æ€§èƒ½ä¸‹é™ã€‚

**å…³é”®è¯**ï¼šæ£€ç´¢å¤´, å¤šè¯­è¨€æ¨¡å‹, ç›®æ ‡è¯­è¨€ç”Ÿæˆ, è·¨è¯­è¨€è®¾ç½®, æ£€ç´¢è¿‡æ¸¡å¤´, é“¾å¼æ€ç»´æ¨ç†, å¤šè¯­è¨€åŸºå‡†, æ³¨æ„åŠ›å¤´

**è¯„åˆ†**ï¼š62

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22453v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22453v1.pdf)

---

## [9. A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection](https://arxiv.org/abs/2602.22449v1)

**ä½œè€…**ï¼šMirza Raquib, Asif Pervez Polok, Kedar Nath Biswas ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§èåˆBanglaBERTå’Œä¸¤å±‚å †å LSTMçš„æ¶æ„ï¼Œç”¨äºå¤šæ ‡ç­¾ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç½‘ç»œæ¬ºå‡Œå¯¹ç¤¾ä¼šå’Œå¿ƒç†å¥åº·é€ æˆä¸¥é‡å½±å“ï¼Œä½†ç°æœ‰ç ”ç©¶å¤šé›†ä¸­äºå•æ ‡ç­¾åˆ†ç±»ï¼Œå¿½è§†äº†è¯„è®ºä¸­å¯èƒ½å­˜åœ¨çš„å¤šç§æ¬ºå‡Œå½¢å¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡ç»“åˆBanglaBERT-Largeä¸ä¸¤å±‚å †å LSTMï¼Œæ¨¡å‹èƒ½å¤Ÿè”åˆå»ºæ¨¡ä¸Šä¸‹æ–‡å’Œåºåˆ—ä¿¡æ¯ï¼Œå¹¶åœ¨å…¬å¼€çš„å¤šæ ‡ç­¾å­ŸåŠ æ‹‰ç½‘ç»œæ¬ºå‡Œæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå’Œè¯„ä¼°ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ¨¡å‹åœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ¶æ„åœ¨è§£å†³å¤šæ ‡ç­¾ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šå¤šæ ‡ç­¾ç½‘ç»œæ¬ºå‡Œæ£€æµ‹, å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±», å­ŸåŠ æ‹‰è¯­NLP, ä½èµ„æºè¯­è¨€, ä»‡æ¨è¨€è®ºæ£€æµ‹, å¨èƒè¨€è®ºæ£€æµ‹, æ€§éªšæ‰°æ£€æµ‹, åƒåœ¾ä¿¡æ¯æ£€æµ‹, ç±»åˆ«ä¸å¹³è¡¡é‡‡æ ·, äº”æŠ˜äº¤å‰éªŒè¯

**è¯„åˆ†**ï¼š16

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22449v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22449v1.pdf)

---

## cs.CV

## [10. Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models](https://arxiv.org/abs/2602.22469v1)

**ä½œè€…**ï¼šNiamul Hassan Samin, Md Arifur Rahman, Abdullah Ibne Hanif ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æœŸå¹²é¢„SCRï¼Œé€šè¿‡å°†â€œä¸»å¯¼å›¾åƒpatchâ€çš„æ¿€æ´»ä¿¡ç”¨é‡åˆ†é…åˆ°å…¶ä¸Šä¸‹æ–‡ï¼Œæ˜¾è‘—é™ä½VLMå¹»è§‰ä¸”å‡ ä¹ä¸æŸä¼¤ç”Ÿæˆè´¨é‡ä¸å»¶è¿Ÿã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä½œè€…è®¤ä¸ºVLMå¹»è§‰æºäºæ—©æœŸå±‚å‡ºç°â€œç©ºé—´ä¿¡ç”¨åç¼©â€ï¼Œå³æ¿€æ´»ä¿¡ç”¨è¿‡åº¦é›†ä¸­åœ¨å°‘æ•°è§†è§‰patchï¼Œå‹åˆ¶ä¸Šä¸‹æ–‡è¯æ®å¹¶ä½¿æ¨¡å‹æ›´ä¾èµ–è¯­è¨€å…ˆéªŒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šSCRåœ¨æ¨ç†æ—¶ç”¨æ³¨æ„åŠ›æŒ‘é€‰é«˜æƒé‡â€œæºpatchâ€ï¼Œå¹¶åœ¨ä½ç†µè¾“å…¥çš„å¼•å¯¼ä¸‹æŠŠå…¶éšè—æ€æ¿€æ´»éƒ¨åˆ†é‡åˆ†é…ç»™å‘¨å›´ä¸Šä¸‹æ–‡patchï¼Œä»¥æ¢å¤ç©ºé—´è¯æ®çš„è¦†ç›–ä¸å¹³è¡¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨Chameleon/LLaVA/Qwenç­‰6ä¸ªæ¨¡å‹å®¶æ—ä¸POPE/CHAIRä¸Šï¼ŒSCRå°†å¹»è§‰ç‡æ˜¾è‘—ä¸‹é™ï¼ˆå¦‚POPE-Açº¦é™4.7â€“6.0ptã€CHAIRæ˜¾è‘—ç›¸å¯¹é™å¹…ï¼‰ä¸”CIDErå‡ ä¹ä¸å˜ï¼ŒåŒæ—¶ä»…å¢åŠ çº¦43â€“56mså¹¶åœ¨æ•ˆæœ-è´¨é‡-å»¶è¿Ÿä¸Šä¼˜äºOPERA/VCD/OVCDï¼›æ¶ˆèè¡¨æ˜æ³¨æ„åŠ›å¼•å¯¼çš„æºpatché€‰æ‹©æ˜¯å…³é”®ï¼Œæ”¯æŒâ€œä¿¡ç”¨åç¼©â€æœºåˆ¶è§£é‡Šã€‚

**å…³é”®è¯**ï¼šè§†è§‰è¯­è¨€æ¨¡å‹å¹»è§‰, æ¨ç†æ—¶å¹²é¢„, å…è®­ç»ƒæ–¹æ³•, æ³¨æ„åŠ›å¼•å¯¼, è§†è§‰è¡¥ä¸ä¸Šä¸‹æ–‡, éšè—çŠ¶æ€æ¿€æ´»é‡åˆ†é…, ä½ç†µè¾“å…¥, å›¾åƒæè¿°æŒ‡æ ‡ï¼ˆCIDErï¼‰

**è¯„åˆ†**ï¼š37

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22469v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22469v1.pdf)

---

## [11. MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation](https://arxiv.org/abs/2602.22462v1)

**ä½œè€…**ï¼šRaiyan Jahangir, Nafiz Imtiaz Khan, Amritanand Sudheerkumar ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.IR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMammoWiseæå‡ºä¸€å¥—å¯æœ¬åœ°éƒ¨ç½²çš„å¤šæ¨¡å‹VLM+å¯é€‰å¤šæ¨¡æ€RAGæµç¨‹ï¼Œç”¨äºä¹³è…ºXçº¿ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Šå¹¶å®ŒæˆBI-RADSã€å¯†åº¦ç­‰å¤šä»»åŠ¡åˆ†ç±»ï¼Œä¸”é€šè¿‡QLoRAå¾®è°ƒæå‡åˆ†ç±»å¯é æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç­›æŸ¥ä¹³è…ºæ‘„å½±å·¥ä½œé‡å¤§ä¸”æŠ¥å‘Šè¦æ±‚æ ‡å‡†åŒ–ï¼Œä½†ç°æœ‰å½±åƒç”ŸæˆæŠ¥å‘Šæ–¹æ¡ˆå¸¸ä¾èµ–å°é—­äº‘ç«¯æˆ–è€¦åˆæ¶æ„ï¼Œå¸¦æ¥éšç§ã€å¤ç°æ€§ä¸å¯æ‰©å±•æ€§é™åˆ¶ã€‚ä½œè€…å¸Œæœ›æ„å»ºä¸€ä¸ªå¼€æ”¾ã€å¯å¤ç°ã€å¯é€‚é…ä¸åŒæ¨¡å‹ä¸æ•°æ®é›†çš„æœ¬åœ°åŒ–æŠ¥å‘Šç”Ÿæˆä¸åˆ†ç±»æ¡†æ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ­å»ºæ”¯æŒä»»æ„Ollamaæ‰˜ç®¡VLMçš„æœ¬åœ°æµæ°´çº¿ï¼Œç»“åˆé›¶æ ·æœ¬/å°‘æ ·æœ¬/CoTæç¤ºï¼Œå¹¶å¯é€‰ç”¨å‘é‡æ•°æ®åº“åšå¤šæ¨¡æ€RAGä»¥æ£€ç´¢ç›¸ä¼¼ç—…ä¾‹ä¸Šä¸‹æ–‡è¾…åŠ©ç”Ÿæˆï¼›åœ¨VinDr-Mammoä¸DMIDä¸Šè¯„ä¼°MedGemmaã€LLaVA-Medã€Qwen2.5-VLçš„æŠ¥å‘ŠæŒ‡æ ‡ä¸å¤šä»»åŠ¡åˆ†ç±»ï¼Œå¹¶å¯¹MedGemmaè¿›è¡ŒQLoRAå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šæŠ¥å‘Šç”Ÿæˆæ•´ä½“è¡¨ç°ç¨³å®šï¼Œå°‘æ ·æœ¬ä¸RAGæ™®éæå‡æ–‡æœ¬è´¨é‡ï¼›åˆ†ç±»ä»»åŠ¡å¯è¡Œä½†å¯¹æ¨¡å‹ä¸æ•°æ®é›†è¾ƒæ•æ„Ÿã€‚å¯¹MedGemmaåšQLoRAåæ˜¾è‘—å¢å¼ºåˆ†ç±»å¯é æ€§ï¼ˆå¦‚BI-RADS 0.7545ã€å¯†åº¦0.8840ã€é’™åŒ–0.9341ï¼‰ä¸”ä¸æ˜æ˜¾ç‰ºç‰²æŠ¥å‘Šè´¨é‡ï¼Œè¯æ˜è¯¥æœ¬åœ°å¯æ‰©å±•æ¡†æ¶å…·å¤‡å®ç”¨éƒ¨ç½²ä»·å€¼ã€‚

**å…³é”®è¯**ï¼šä¹³è…ºXå…‰æ£€æŸ¥, æŠ¥å‘Šç”Ÿæˆ, å¤šæ¨¡å‹ç®¡é“, è§†è§‰è¯­è¨€æ¨¡å‹, RAG, æ•°æ®é›†è¯„ä¼°, å‚æ•°é«˜æ•ˆå¾®è°ƒ, MammoWise

**è¯„åˆ†**ï¼š73

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22462v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22462v1.pdf)

---

## cs.LG

## [12. Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns](https://arxiv.org/abs/2602.22479v1)

**ä½œè€…**ï¼šAfshin Khadangi  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC$^{2}$ (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC$^{2}$ combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC$^{2}$ improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šTRCÂ² é€šè¿‡â€œä¸˜è„‘å¼ç¨€ç–è·¯ç”±+çš®å±‚æŸ±â€æ¶æ„ä¸å¿«é€Ÿçº é”™é€šè·¯ï¼Œå®ç°è¯­è¨€æ¨¡å‹åœ¨æ•°æ®æµå¼åŸŸè¿ç§»ä¸‹æ›´é«˜æ•ˆçš„æŒç»­å­¦ä¹ ä¸æ›´å°‘é—å¿˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéƒ¨ç½²ä¸­çš„è¯­è¨€æ¨¡å‹éœ€è¦åœ¨çº¿é€‚åº”éå¹³ç¨³æ•°æ®ï¼Œä½†å¸¸è§„å¾®è°ƒæ˜“ç¾éš¾æ€§é—å¿˜ï¼›è€Œæå‡ç¨³å®šæ€§çš„æ–¹æ¡ˆå¾€å¾€å¸¦æ¥æ›´é«˜å»¶è¿Ÿã€æ˜¾å­˜æˆ–å¯†é›†è®¡ç®—å¼€é”€ï¼Œéš¾ä»¥æ‰©å±•åˆ°é•¿ä¸Šä¸‹æ–‡ä¸æŒç»­æ›´æ–°åœºæ™¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºè§£ç å™¨éª¨å¹² TRCÂ²ï¼šä»¥ç¨€ç–â€œä¸˜è„‘è·¯ç”±â€åœ¨å¤šä¸ªçš®å±‚æŸ±é—´é€‰æ‹©æ€§æ¿€æ´»ï¼Œå¹¶ç»“åˆè°ƒåˆ¶ã€é¢„æµ‹ã€è®°å¿†ã€åé¦ˆç­‰å­æœºåˆ¶ï¼›åŒæ—¶åŠ å…¥å¿«é€Ÿçº é”™é€šè·¯ï¼Œç”¨å°‘é‡â€œå¿«å‚æ•°â€æ”¯æŒå¿«é€Ÿé€‚åº”ä¸”ä¸æ‰°åŠ¨â€œæ…¢å‚æ•°â€ï¼Œå¹¶ä¿æŒå—çº§ç¨€ç–ä¸chunkå¹¶è¡Œä»¥æå‡è®­ç»ƒ/æ¨ç†æ•ˆç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨è¯­è¨€å»ºæ¨¡ä¸æŒç»­å­¦ä¹ åŸºå‡†åŠæµå¼åŸŸè¿ç§»é—å¿˜è¯„æµ‹ä¸­ï¼ŒTRCÂ² åœ¨ç›¸è¿‘è®¡ç®—é‡ä¸‹æ”¹å–„ç¨³å®šæ€§-å¯å¡‘æ€§æƒè¡¡ï¼Œæ”¯æŒæ›´å¿«çš„åœ¨çº¿é€‚åº”å¹¶æ›´å¥½ä¿ç•™æ—¢æœ‰èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šæŒç»­å­¦ä¹ , ç¾éš¾æ€§é—å¿˜, éå¹³ç¨³æ•°æ®, æµå¼é¢†åŸŸæ¼‚ç§», ç¨³å®šæ€§-å¯å¡‘æ€§æƒè¡¡, ç¨€ç–è·¯ç”±, ä¸˜è„‘è·¯ç”±, çš®å±‚æŸ±æ¨¡å—, è§£ç å™¨æ¶æ„, é—å¿˜åº¦è¯„æµ‹åŸºå‡†

**è¯„åˆ†**ï¼š33

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22479v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22479v1.pdf)

---

## [13. Beyond performance-wise Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.22470v1)

**ä½œè€…**ï¼šBalazs Pejo  
**åˆ†ç±»**ï¼šcs.LG, cs.CR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Federated learning offers a privacy-friendly collaborative learning framework, yet its success, like any joint venture, hinges on the contributions of its participants. Existing client evaluation methods predominantly focus on model performance, such as accuracy or loss, which represents only one dimension of a machine learning model's overall utility. In contrast, this work investigates the critical, yet overlooked, issue of client contributions towards a model's trustworthiness -- specifically, its reliability (tolerance to noisy data), resilience (resistance to adversarial examples), and fairness (measured via demographic parity). To quantify these multifaceted contributions, we employ the state-of-the-art approximation of the Shapley value, a principled method for value attribution. Our results reveal that no single client excels across all dimensions, which are largely independent from each other, highlighting a critical flaw in current evaluation scheme: no single metric is adequate for comprehensive evaluation and equitable rewarding allocation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºåœ¨è”é‚¦å­¦ä¹ ä¸­åº”ä»å¯é æ€§ã€é²æ£’æ€§ä¸å…¬å¹³æ€§ç­‰â€œå¯ä¿¡åº¦â€ç»´åº¦è€Œéä»…æ€§èƒ½æ¥è¯„ä¼°å®¢æˆ·ç«¯è´¡çŒ®ï¼Œå¹¶ç”¨Shapleyå€¼è¿‘ä¼¼è¿›è¡Œå¤šç»´å½’å› ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å®¢æˆ·ç«¯è´¡çŒ®è¯„ä¼°å¤§å¤šåªçœ‹å‡†ç¡®ç‡/æŸå¤±ï¼Œå¿½ç•¥æ¨¡å‹åœ¨å™ªå£°ã€å¯¹æŠ—æ”»å‡»ä¸ç¾¤ä½“å…¬å¹³æ€§ä¸Šçš„æ•ˆç”¨ï¼Œå¯èƒ½å¯¼è‡´å¥–åŠ±åˆ†é…ä¸å…¬ä¸æ¿€åŠ±å¤±çœŸã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†æ¨¡å‹å¯ä¿¡åº¦æ‹†ä¸ºå¯é æ€§ï¼ˆæŠ—å™ªï¼‰ã€éŸ§æ€§/é²æ£’æ€§ï¼ˆæŠ—å¯¹æŠ—æ ·æœ¬ï¼‰å’Œå…¬å¹³æ€§ï¼ˆäººå£ç»Ÿè®¡å‡ç­‰ï¼‰ä¸‰ç±»æŒ‡æ ‡ï¼Œå¹¶é‡‡ç”¨æœ€å…ˆè¿›çš„Shapley valueè¿‘ä¼¼æ–¹æ³•åˆ†åˆ«è®¡ç®—å„å®¢æˆ·ç«¯åœ¨æ¯ä¸ªç»´åº¦ä¸Šçš„è¾¹é™…è´¡çŒ®ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒå‘ç°ä¸åŒç»´åº¦çš„è´¡çŒ®å¾€å¾€ç›¸äº’ç‹¬ç«‹ã€æ²¡æœ‰å®¢æˆ·ç«¯èƒ½åœ¨æ‰€æœ‰ç»´åº¦éƒ½æœ€ä¼˜ï¼Œè¯´æ˜å•ä¸€æŒ‡æ ‡æ— æ³•å…¨é¢è¯„ä»·ä¸å…¬å¹³å¥–åŠ±åˆ†é…ï¼Œå¿…é¡»è¿›è¡Œå¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°ã€‚

**å…³é”®è¯**ï¼šè”é‚¦å­¦ä¹ , å®¢æˆ·è´¡çŒ®, æ¨¡å‹å¯ä¿¡åº¦, å¯é æ€§, æŠ—å¹²æ‰°æ€§, å…¬å¹³æ€§, è¯„ä¼°æ¡†æ¶, Beyond

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22470v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22470v1.pdf)

---

## [14. ECHO: Encoding Communities via High-order Operators](https://arxiv.org/abs/2602.22446v1)

**ä½œè€…**ï¼šEmilio Ferrara  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Community detection in attributed networks faces a fundamental divide: topological algorithms ignore semantic features, while Graph Neural Networks (GNNs) encounter devastating computational bottlenecks. Specifically, GNNs suffer from a Semantic Wall of feature over smoothing in dense or heterophilic networks, and a Systems Wall driven by the O(N^2) memory constraints of pairwise clustering. To dismantle these barriers, we introduce ECHO (Encoding Communities via High order Operators), a scalable, self supervised architecture that reframes community detection as an adaptive, multi scale diffusion process. ECHO features a Topology Aware Router that automatically analyzes structural heuristics sparsity, density, and assortativity to route graphs through the optimal inductive bias, preventing heterophilic poisoning while ensuring semantic densification. Coupled with a memory sharded full batch contrastive objective and a novel chunked O(N \cdot K) similarity extraction method, ECHO completely bypasses traditional O(N^2) memory bottlenecks without sacrificing the mathematical precision of global gradients. Extensive evaluations demonstrate that this topology feature synergy consistently overcomes the classical resolution limit. On synthetic LFR benchmarks scaled up to 1 million nodes, ECHO achieves scale invariant accuracy despite severe topological noise. Furthermore, on massive real world social networks with over 1.6 million nodes and 30 million edges, it completes clustering in mere minutes with throughputs exceeding 2,800 nodes per second matching the speed of highly optimized purely topological baselines. The implementation utilizes a unified framework that automatically engages memory sharded optimization to support adoption across varying hardware constraints. GitHub Repository: https://github.com/emilioferrara/ECHO-GNN

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šECHO é€šè¿‡æ‹“æ‰‘æ„ŸçŸ¥çš„å¤šå°ºåº¦é«˜é˜¶æ‰©æ•£ä¸å¯æ‰©å±•çš„å…¨å±€å¯¹æ¯”å­¦ä¹ ï¼Œç»•å¼€ä¼ ç»Ÿ GNN çš„è¿‡å¹³æ»‘ä¸ O(N^2) å†…å­˜ç“¶é¢ˆï¼Œå®ç°å¤§è§„æ¨¡å±æ€§ç½‘ç»œçš„é«˜æ•ˆé«˜ç²¾åº¦ç¤¾åŒºå‘ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ç¤¾åŒºæ£€æµ‹æ–¹æ³•åœ¨â€œåªçœ‹æ‹“æ‰‘å¿½ç•¥è¯­ä¹‰â€å’Œâ€œGNN è®¡ç®—/å†…å­˜ä¸å¯æ‰©å±•â€ä¹‹é—´ä¸¤éš¾ï¼šç¨ å¯†æˆ–å¼‚é…ç½‘ç»œæ˜“è¯­ä¹‰è¿‡å¹³æ»‘ï¼ˆSemantic Wallï¼‰ï¼Œè€Œæˆå¯¹ç›¸ä¼¼åº¦/èšç±»åˆå¸¦æ¥ O(N^2) å†…å­˜ä¸ç³»ç»Ÿç“¶é¢ˆï¼ˆSystems Wallï¼‰ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºè‡ªç›‘ç£æ¶æ„ ECHOï¼Œå°†ç¤¾åŒºæ£€æµ‹é‡æ„ä¸ºè‡ªé€‚åº”å¤šå°ºåº¦æ‰©æ•£ï¼šç”¨ Topology Aware Router æ ¹æ®ç¨€ç–åº¦ã€å¯†åº¦ä¸åŒé…æ€§ç­‰ç»“æ„å¯å‘å¼è‡ªåŠ¨é€‰æ‹©åˆé€‚å½’çº³åç½®ï¼Œé¿å…å¼‚é…â€œæ±¡æŸ“â€å¹¶å¢å¼ºè¯­ä¹‰ä¼ æ’­ï¼›åŒæ—¶é‡‡ç”¨å†…å­˜åˆ†ç‰‡çš„å…¨æ‰¹å¯¹æ¯”ç›®æ ‡ä¸åˆ†å—çš„ O(NÂ·K) ç›¸ä¼¼åº¦æå–ï¼Œåœ¨ä¿æŒå…¨å±€æ¢¯åº¦ç²¾ç¡®æ€§çš„åŒæ—¶æ¶ˆé™¤ O(N^2) å†…å­˜å¼€é”€ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æœ€é«˜åˆ°ç™¾ä¸‡èŠ‚ç‚¹çš„ LFR åˆæˆå›¾ä¸Šï¼ŒECHO åœ¨å¼ºå™ªå£°ä¸‹ä»ä¿æŒå°ºåº¦ä¸å˜çš„å‡†ç¡®ç‡å¹¶å…‹æœåˆ†è¾¨ç‡é™åˆ¶ï¼›åœ¨ 160 ä¸‡èŠ‚ç‚¹/3000 ä¸‡è¾¹çš„çœŸå®ç¤¾äº¤ç½‘ç»œä¸Šå¯åœ¨æ•°åˆ†é’Ÿå®Œæˆèšç±»ï¼Œååé‡è¶… 2800 èŠ‚ç‚¹/ç§’ï¼Œé€Ÿåº¦æ¥è¿‘é«˜åº¦ä¼˜åŒ–çš„çº¯æ‹“æ‰‘åŸºçº¿ä¸”æ›´å¥½èåˆè¯­ä¹‰ä¿¡æ¯ã€‚

**å…³é”®è¯**ï¼šç¤¾åŒºæ£€æµ‹, é«˜é˜¶ç®—å­, å›¾ç¥ç»ç½‘ç»œ, è‡ªç›‘ç£, å¤šå°ºåº¦æ‰©æ•£, æ‹“æ‰‘æ„ŸçŸ¥è·¯ç”±, å†…å­˜ä¼˜åŒ–, ç›¸ä¼¼æ€§æå–, æ‹“æ‰‘ç‰¹å¾, å™ªå£°é²æ£’æ€§, å¤§è§„æ¨¡ç½‘ç»œ

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22446v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22446v1.pdf)

---

## [15. Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428v1)

**ä½œè€…**ï¼šDaniel Geyfman, Felix Draxler, Jan Groeneveld ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-25

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§â€œæ ¡å‡†çš„æµ‹è¯•æ—¶å¼•å¯¼â€æ¡†æ¶ï¼Œä½¿æ‰©æ•£æ¨¡å‹åœ¨æµ‹è¯•æ—¶å¼•å¯¼ä¸‹èƒ½å¤Ÿä¸€è‡´åœ°ä»çœŸå®è´å¶æ–¯åéªŒåˆ†å¸ƒé‡‡æ ·ï¼Œè€Œéä»…è¿½æ±‚é«˜å¥–åŠ±çš„åç½®è§£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æµ‹è¯•æ—¶å¼•å¯¼æ–¹æ³•é€šå¸¸ä»¥æœ€å¤§åŒ–å¥–åŠ±ä¸ºç›®æ ‡ï¼Œå¯¼è‡´é‡‡æ ·åˆ†å¸ƒä¸çœŸå®åéªŒä¸ä¸€è‡´ã€æ¨æ–­ç»“æœå¤±æ ¡å‡†ï¼›ä½œè€…æ—¨åœ¨æ‰¾å‡ºå…¶ç»“æ„æ€§è¿‘ä¼¼è¯¯å·®å¹¶å®ç°æ­£ç¡®çš„åéªŒé‡‡æ ·ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç†è®ºåˆ†æå¸¸è§å¼•å¯¼ï¼ˆå¦‚åŸºäºrewardæ¢¯åº¦/scoreçš„å¼•å¯¼ï¼‰ä¸ºä½•ä¸èƒ½æ¢å¤æ­£ç¡®åéªŒï¼Œå¹¶å®šä½å¯¼è‡´åå·®çš„è¿‘ä¼¼å‡è®¾ï¼›æ®æ­¤æå‡ºä¸€è‡´ï¼ˆconsistentï¼‰çš„æ›¿ä»£ä¼°è®¡å™¨/å¼•å¯¼å½¢å¼ï¼Œåœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­å¯¹åéªŒé¡¹è¿›è¡Œæ ¡å‡†ä»¥åŒ¹é…è´å¶æ–¯åéªŒã€‚

**ä¸»è¦ç»“è®º**ï¼šæ‰€ææ–¹æ³•åœ¨å¤šç±»è´å¶æ–¯æ¨æ–­ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæ—¢æœ‰å¼•å¯¼ç­–ç•¥ï¼Œå¹¶åœ¨é»‘æ´å›¾åƒé‡å»ºä¸Šè¾¾åˆ°ï¼ˆæˆ–åŒ¹é…ï¼‰å½“å‰æœ€ä¼˜æ°´å¹³ï¼ŒåŒæ—¶æå‡äº†æ¨æ–­çš„æ ¡å‡†æ€§ä¸åéªŒä¸€è‡´æ€§ã€‚

**å…³é”®è¯**ï¼šæµ‹è¯•æ—¶å¼•å¯¼, Diffusion, å¥–åŠ±å¼•å¯¼é‡‡æ ·, è´å¶æ–¯æ¨æ–­, åéªŒé‡‡æ ·, æ ¡å‡†æ¨æ–­, åéªŒä¸€è‡´æ€§, ç»“æ„è¿‘ä¼¼è¯¯å·®, ä¸€è‡´ä¼°è®¡é‡, é»‘æ´å›¾åƒé‡å»º, é€†é—®é¢˜é‡å»º

**è¯„åˆ†**ï¼š28

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.22428v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.22428v1.pdf)

---

