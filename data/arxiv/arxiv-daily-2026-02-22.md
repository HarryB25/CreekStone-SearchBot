# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-22

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.LG](#csLG) (7 ç¯‡)
- [cs.CV](#csCV) (6 ç¯‡)
- [cs.AI](#csAI) (1 ç¯‡)
- [cs.CL](#csCL) (1 ç¯‡)

---

## cs.AI

## [1. Time Series, Vision, and Language: Exploring the Limits of Alignment in Contrastive Representation Spaces](https://arxiv.org/abs/2602.19367v1)

**ä½œè€…**ï¼šPratham Yashwante, Rose Yu  
**åˆ†ç±»**ï¼šcs.AI, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The Platonic Representation Hypothesis posits that learned representations from models trained on different modalities converge to a shared latent structure of the world. However, this hypothesis has largely been examined in vision and language, and it remains unclear whether time series participate in such convergence. We first examine this in a trimodal setting and find that independently pretrained time series, vision, and language encoders exhibit near-orthogonal geometry in the absence of explicit coupling. We then apply post-hoc alignment by training projection heads over frozen encoders using contrastive learning, and analyze the resulting representations with respect to geometry, scaling behavior, and dependence on information density and input modality characteristics. Our investigation reveals that overall alignment in contrastive representation spaces improves with model size, but this alignment is asymmetric: time series align more strongly with visual representations than with text, and images can act as effective intermediaries between time series and language. We further see that richer textual descriptions improve alignment only up to a threshold; training on denser captions does not lead to further improvement. Analogous effects are observed for visual representations. Our findings shed light on considerations for building multimodal systems involving non-conventional data modalities beyond vision and language.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡å‘ç°æ—¶é—´åºåˆ—ã€è§†è§‰ä¸è¯­è¨€çš„é¢„è®­ç»ƒè¡¨å¾åœ¨æœªæ˜¾å¼è€¦åˆæ—¶å‡ ä¹æ­£äº¤ï¼Œä½†å¯é€šè¿‡å†»ç»“ç¼–ç å™¨+å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´å®ç°åéªŒå¯¹é½ï¼Œä¸”å¯¹é½å‘ˆç°è§„æ¨¡æå‡ä¸æ¨¡æ€éå¯¹ç§°æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šâ€œè¡¨å¾è¶‹åŒ/æŸæ‹‰å›¾è¡¨å¾å‡è¯´â€å¤šåœ¨è§†è§‰-è¯­è¨€ä¸ŠéªŒè¯ï¼Œå°šä¸æ¸…æ¥šæ—¶é—´åºåˆ—æ˜¯å¦ä¹Ÿä¼šä¸å…¶ä»–æ¨¡æ€å…±äº«æ½œåœ¨ç»“æ„ã€‚ä½œè€…å¸Œæœ›ç³»ç»Ÿåˆ»ç”»ä¸‰æ¨¡æ€åœ¨å¯¹æ¯”è¡¨å¾ç©ºé—´ä¸­çš„å¯å¯¹é½æ€§è¾¹ç•ŒåŠå…¶å½±å“å› ç´ ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆåœ¨ä¸‰æ¨¡æ€ï¼ˆæ—¶é—´åºåˆ—/å›¾åƒ/æ–‡æœ¬ï¼‰ä¸Šåˆ†æå„è‡ªç‹¬ç«‹é¢„è®­ç»ƒç¼–ç å™¨çš„å‡ ä½•å…³ç³»ï¼Œè§‚å¯Ÿæ˜¯å¦å¤©ç„¶å¯¹é½ï¼›å†å†»ç»“ç¼–ç å™¨ï¼Œä»…è®­ç»ƒæŠ•å½±å¤´è¿›è¡Œå¯¹æ¯”å­¦ä¹ å¼çš„åéªŒå¯¹é½ï¼Œå¹¶ä»å‡ ä½•ã€è§„æ¨¡å¾‹ã€ä¿¡æ¯å¯†åº¦ï¼ˆæ›´ä¸°å¯Œæè¿°/æ›´å¯†é›†captionç­‰ï¼‰ä¸æ¨¡æ€ç‰¹æ€§è§’åº¦è¯„ä¼°å¯¹é½æ•ˆæœã€‚

**ä¸»è¦ç»“è®º**ï¼šå¯¹æ¯”ç©ºé—´æ•´ä½“å¯¹é½åº¦éšæ¨¡å‹è§„æ¨¡å¢å¤§è€Œæå‡ï¼Œä½†å­˜åœ¨æ˜¾è‘—éå¯¹ç§°ï¼šæ—¶é—´åºåˆ—æ›´å®¹æ˜“å¯¹é½åˆ°è§†è§‰è€Œéæ–‡æœ¬ï¼Œä¸”å›¾åƒå¯ä½œä¸ºæ—¶é—´åºåˆ—ä¸è¯­è¨€ä¹‹é—´çš„æœ‰æ•ˆâ€œä¸­ä»‹â€ã€‚æ›´ä¸°å¯Œçš„æ–‡æœ¬/è§†è§‰æè¿°å¯¹å¯¹é½æœ‰å¸®åŠ©ä½†å­˜åœ¨é˜ˆå€¼ï¼Œè¶…è¿‡ä¸€å®šä¿¡æ¯å¯†åº¦åç»§ç»­åŠ å¯†captionæ”¶ç›Šä¸å†å¢é•¿ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€è¡¨ç¤ºå¯¹é½, æ—¶é—´åºåˆ—ç¼–ç å™¨, è§†è§‰-è¯­è¨€å¯¹é½, ä¸‰æ¨¡æ€å­¦ä¹ , åéªŒå¯¹é½, æŠ•å½±å¤´, å†»ç»“ç¼–ç å™¨, è¡¨ç¤ºå‡ ä½•, ç¼©æ”¾è§„å¾‹, ä¿¡æ¯å¯†åº¦

**è¯„åˆ†**ï¼š21

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19367v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19367v1.pdf)

---

## cs.CL

## [2. PerSoMed: A Large-Scale Balanced Dataset for Persian Social Media Text Classification](https://arxiv.org/abs/2602.19333v1)

**ä½œè€…**ï¼šIsun Chehreh, Ebrahim Ansari  
**åˆ†ç±»**ï¼šcs.CL, cs.IR, cs.SI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

This research introduces the first large-scale, well-balanced Persian social media text classification dataset, specifically designed to address the lack of comprehensive resources in this domain. The dataset comprises 36,000 posts across nine categories (Economic, Artistic, Sports, Political, Social, Health, Psychological, Historical, and Science & Technology), each containing 4,000 samples to ensure balanced class distribution. Data collection involved 60,000 raw posts from various Persian social media platforms, followed by rigorous preprocessing and hybrid annotation combining ChatGPT-based few-shot prompting with human verification. To mitigate class imbalance, we employed undersampling with semantic redundancy removal and advanced data augmentation strategies integrating lexical replacement and generative prompting. We benchmarked several models, including BiLSTM, XLM-RoBERTa (with LoRA and AdaLoRA adaptations), FaBERT, SBERT-based architectures, and the Persian-specific TookaBERT (Base and Large). Experimental results show that transformer-based models consistently outperform traditional neural networks, with TookaBERT-Large achieving the best performance (Precision: 0.9622, Recall: 0.9621, F1- score: 0.9621). Class-wise evaluation further confirms robust performance across all categories, though social and political texts exhibited slightly lower scores due to inherent ambiguity. This research presents a new high-quality dataset and provides comprehensive evaluations of cutting-edge models, establishing a solid foundation for further developments in Persian NLP, including trend analysis, social behavior modeling, and user classification. The dataset is publicly available to support future research endeavors.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºå¹¶å…¬å¼€äº†ä¸€ä¸ªé¦–ä¸ªå¤§è§„æ¨¡ã€ç±»åˆ«å‡è¡¡çš„æ³¢æ–¯è¯­ç¤¾äº¤åª’ä½“æ–‡æœ¬åˆ†ç±»æ•°æ®é›†PerSoMedï¼ˆ36K/9ç±»ï¼‰ï¼Œå¹¶ç³»ç»Ÿè¯„æµ‹å¤šç§æ¨¡å‹ï¼ŒTookaBERT-Largeå–å¾—æœ€ä½³æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ³¢æ–¯è¯­ç¤¾äº¤åª’ä½“æ–‡æœ¬åˆ†ç±»ç¼ºä¹è§„æ¨¡è¶³å¤Ÿä¸”ç±»åˆ«å‡è¡¡çš„é«˜è´¨é‡å…¬å¼€æ•°æ®ï¼Œé™åˆ¶äº†ç›¸å…³NLPç ”ç©¶ä¸åº”ç”¨å‘å±•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»å¤šå¹³å°é‡‡é›†6ä¸‡åŸå§‹å¸–å­ï¼Œç»ä¸¥æ ¼é¢„å¤„ç†åç”¨â€œChatGPTå°‘æ ·æœ¬æ ‡æ³¨+äººå·¥å¤æ ¸â€çš„æ··åˆæ ‡æ³¨ç”Ÿæˆ9ç±»å„4000æ¡ï¼›é€šè¿‡è¯­ä¹‰å†—ä½™å»é™¤çš„æ¬ é‡‡æ ·ä¸ç»“åˆè¯æ±‡æ›¿æ¢/ç”Ÿæˆå¼æç¤ºçš„æ•°æ®å¢å¼ºç¼“è§£ä¸å‡è¡¡ï¼Œå¹¶åŸºå‡†æµ‹è¯•BiLSTMã€XLM-R(LoRA/AdaLoRA)ã€FaBERTã€SBERTä¸TookaBERTç­‰æ¨¡å‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šTransformerç±»æ¨¡å‹æ•´ä½“æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç¥ç»ç½‘ç»œï¼Œå…¶ä¸­TookaBERT-Largeè¾¾åˆ°æœ€é«˜F1=0.9621ï¼›æŒ‰ç±»è¯„ä¼°æ˜¾ç¤ºæ€»ä½“ç¨³å¥ï¼Œä½†ç¤¾äº¤ä¸æ”¿æ²»ç±»å› è¯­ä¹‰æ­§ä¹‰ç•¥ä½ï¼Œæ•°æ®é›†ä¸ºåç»­æ³¢æ–¯è¯­ç¤¾åª’åˆ†æä¸ç”¨æˆ·å»ºæ¨¡ç­‰ç ”ç©¶æä¾›åŸºç¡€ã€‚

**å…³é”®è¯**ï¼šç¤¾äº¤åª’ä½“æ–‡æœ¬åˆ†ç±», æ•°æ®é›†æ„å»º, ç±»åˆ«å‡è¡¡, æ··åˆæ ‡æ³¨, å°‘æ ·æœ¬æç¤º, æ•°æ®å¢å¼º, æ¬ é‡‡æ ·, è¯­ä¹‰å†—ä½™å»é™¤, å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰

**è¯„åˆ†**ï¼š21

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19333v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19333v1.pdf)

---

## cs.CV

## [3. Adaptive Data Augmentation with Multi-armed Bandit: Sample-Efficient Embedding Calibration for Implicit Pattern Recognition](https://arxiv.org/abs/2602.19385v1)

**ä½œè€…**ï¼šMinxue Tang, Yangyang Yu, Aolin Ding ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.CL, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recognizing implicit visual and textual patterns is essential in many real-world applications of modern AI. However, tackling long-tail pattern recognition tasks remains challenging for current pre-trained foundation models such as LLMs and VLMs. While finetuning pre-trained models can improve accuracy in recognizing implicit patterns, it is usually infeasible due to a lack of training data and high computational overhead. In this paper, we propose ADAMAB, an efficient embedding calibration framework for few-shot pattern recognition. To maximally reduce the computational costs, ADAMAB trains embedder-agnostic light-weight calibrators on top of fixed embedding models without accessing their parameters. To mitigate the need for large-scale training data, we introduce an adaptive data augmentation strategy based on the Multi-Armed Bandit (MAB) mechanism. With a modified upper confidence bound algorithm, ADAMAB diminishes the gradient shifting and offers theoretically guaranteed convergence in few-shot training. Our multi-modal experiments justify the superior performance of ADAMAB, with up to 40% accuracy improvement when training with less than 5 initial data samples of each class.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šADAMABé€šè¿‡åœ¨å†»ç»“çš„åµŒå…¥æ¨¡å‹ä¸Šè®­ç»ƒè½»é‡æ ¡å‡†å™¨ï¼Œå¹¶ç”¨åŸºäºå¤šè‡‚è€è™æœºçš„è‡ªé€‚åº”æ•°æ®å¢å¼ºï¼Œåœ¨æå°‘æ ·æœ¬ä¸‹æ˜¾è‘—æå‡éšå¼æ¨¡å¼è¯†åˆ«å‡†ç¡®ç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šé•¿å°¾çš„éšå¼è§†è§‰/æ–‡æœ¬æ¨¡å¼è¯†åˆ«å¯¹ç°æœ‰é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä»å…·æŒ‘æˆ˜ï¼Œè€Œç›´æ¥å¾®è°ƒå¾€å¾€å—é™äºæ•°æ®ç¨€ç¼ºä¸ç®—åŠ›æˆæœ¬è¿‡é«˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºåµŒå…¥å™¨æ— å…³çš„è½»é‡æ ¡å‡†å™¨ï¼ˆä¸è®¿é—®/æ›´æ–°åº•åº§æ¨¡å‹å‚æ•°ï¼‰ä»¥ä½æˆæœ¬è¿›è¡Œembeddingæ ¡å‡†ï¼›åŒæ—¶ç”¨æ”¹è¿›çš„UCBå¤šè‡‚è€è™æœºç­–ç•¥è‡ªé€‚åº”é€‰æ‹©æ•°æ®å¢å¼ºæ–¹å¼ï¼Œå‡å°‘few-shotè®­ç»ƒä¸­çš„æ¢¯åº¦æ¼‚ç§»å¹¶ç»™å‡ºæ”¶æ•›ä¿è¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šå¤šæ¨¡æ€å®éªŒè¡¨æ˜ADAMABåœ¨æ¯ç±»å°‘äº5ä¸ªåˆå§‹æ ·æœ¬æ—¶ä»èƒ½ç¨³å®šæ”¶æ•›å¹¶å–å¾—æ˜¾è‘—æ”¶ç›Šï¼Œæœ€é«˜å¯å¸¦æ¥çº¦40%çš„å‡†ç¡®ç‡æå‡ã€‚

**å…³é”®è¯**ï¼šéšå¼æ¨¡å¼è¯†åˆ«, é•¿å°¾è¯†åˆ«, å°æ ·æœ¬å­¦ä¹ , åµŒå…¥æ ¡å‡†, å‚æ•°é«˜æ•ˆå¾®è°ƒ, è‡ªé€‚åº”æ•°æ®å¢å¼º, å¤šè‡‚è€è™æœºï¼ˆMABï¼‰, å†»ç»“åµŒå…¥æ¨¡å‹, è½»é‡æ ¡å‡†å™¨, å¤šæ¨¡æ€å­¦ä¹ , åŸºç¡€æ¨¡å‹ï¼ˆLLM/VLMï¼‰

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19385v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19385v1.pdf)

---

## [4. Detector-in-the-Loop Tracking: Active Memory Rectification for Stable Glottic Opening Localization](https://arxiv.org/abs/2602.19380v1)

**ä½œè€…**ï¼šHuayu Wang, Bahaa Alattar, Cheng-Yen Yang ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Temporal stability in glottic opening localization remains challenging due to the complementary weaknesses of single-frame detectors and foundation-model trackers: the former lacks temporal context, while the latter suffers from memory drift. Specifically, in video laryngoscopy, rapid tissue deformation, occlusions, and visual ambiguities in emergency settings require a robust, temporally aware solution that can prevent progressive tracking errors. We propose Closed-Loop Memory Correction (CL-MC), a detector-in-the-loop framework that supervises Segment Anything Model 2(SAM2) through confidence-aligned state decisions and active memory rectification. High-confidence detections trigger semantic resets that overwrite corrupted tracker memory, effectively mitigating drift accumulation with a training-free foundation tracker in complex endoscopic scenes. On emergency intubation videos, CL-MC achieves state-of-the-art performance, significantly reducing drift and missing rate compared with the SAM2 variants and open loop based methods. Our results establish memory correction as a crucial component for reliable clinical video tracking. Our code will be available in https://github.com/huayuww/CL-MR.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºDetector-in-the-Loopçš„é—­ç¯è®°å¿†æ ¡æ­£ï¼ˆCL-MCï¼‰ï¼Œç”¨é«˜ç½®ä¿¡æ£€æµ‹å¯¹SAM2è·Ÿè¸ªå™¨è¿›è¡Œè¯­ä¹‰é‡ç½®ä¸è®°å¿†çº åï¼Œä»è€Œç¨³å®šå®šä½å£°é—¨å¼€å£å¹¶æ˜¾è‘—å‡å°‘æ¼‚ç§»ä¸æ¼æ£€ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå•å¸§æ£€æµ‹å™¨ç¼ºä¹æ—¶åºä¸Šä¸‹æ–‡ï¼Œè€ŒåŸºç¡€æ¨¡å‹è·Ÿè¸ªå™¨æ˜“å‘ç”Ÿè®°å¿†æ¼‚ç§»ï¼›åœ¨æ€¥è¯Šå–‰é•œè§†é¢‘ä¸­å­˜åœ¨å¿«é€Ÿå½¢å˜ã€é®æŒ¡ä¸æ¨¡ç³Šï¼Œå¯¼è‡´è·Ÿè¸ªè¯¯å·®ä¼šéšæ—¶é—´ç´¯ç§¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºCL-MCé—­ç¯æ¡†æ¶ï¼šå°†æ£€æµ‹å™¨è¾“å‡ºçš„ç½®ä¿¡åº¦ä¸çŠ¶æ€å†³ç­–å¯¹é½ï¼Œåœ¨æ£€æµ‹é«˜ç½®ä¿¡æ—¶è§¦å‘â€œè¯­ä¹‰é‡ç½®â€ï¼Œç”¨æ£€æµ‹ç»“æœè¦†ç›–/çº æ­£SAM2çš„å†…éƒ¨è®°å¿†ä»¥ä¸»åŠ¨æ¶ˆé™¤æ¼‚ç§»ï¼›æ•´ä½“ä¸ºtraining-freeåœ°ç›‘ç£åŸºç¡€è·Ÿè¸ªå™¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ€¥è¯Šæ’ç®¡å–‰é•œè§†é¢‘ä¸Šè¾¾åˆ°SOTAï¼Œç›¸æ¯”SAM2å˜ä½“ä¸å¼€ç¯æ–¹æ³•æ˜¾è‘—é™ä½æ¼‚ç§»ä¸ç¼ºå¤±ç‡ï¼›ç»“æœè¡¨æ˜â€œè®°å¿†æ ¡æ­£â€æ˜¯ä¸´åºŠè§†é¢‘å¯é è·Ÿè¸ªçš„å…³é”®ç»„ä»¶ã€‚

**å…³é”®è¯**ï¼šé—­ç¯è®°å¿†æ ¡æ­£, è§†é¢‘å–‰é•œ, è·Ÿè¸ªç¨³å®šæ€§, è¯­ä¹‰é‡ç½®, å†…å¾ªç¯æ£€æµ‹å™¨, å†…å­˜æ¼‚ç§», ç´§æ€¥æ’ç®¡, å¤æ‚å†…çª¥é•œåœºæ™¯

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19380v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19380v1.pdf)

---

## [5. Referring Layer Decomposition](https://arxiv.org/abs/2602.19358v1)

**ä½œè€…**ï¼šFangyi Chen, Yaojie Shen, Lu Xu ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Precise, object-aware control over visual content is essential for advanced image editing and compositional generation. Yet, most existing approaches operate on entire images holistically, limiting the ability to isolate and manipulate individual scene elements. In contrast, layered representations, where scenes are explicitly separated into objects, environmental context, and visual effects, provide a more intuitive and structured framework for interpreting and editing visual content. To bridge this gap and enable both compositional understanding and controllable editing, we introduce the Referring Layer Decomposition (RLD) task, which predicts complete RGBA layers from a single RGB image, conditioned on flexible user prompts, such as spatial inputs (e.g., points, boxes, masks), natural language descriptions, or combinations thereof. At the core is the RefLade, a large-scale dataset comprising 1.11M image-layer-prompt triplets produced by our scalable data engine, along with 100K manually curated, high-fidelity layers. Coupled with a perceptually grounded, human-preference-aligned automatic evaluation protocol, RefLade establishes RLD as a well-defined and benchmarkable research task. Building on this foundation, we present RefLayer, a simple baseline designed for prompt-conditioned layer decomposition, achieving high visual fidelity and semantic alignment. Extensive experiments show our approach enables effective training, reliable evaluation, and high-quality image decomposition, while exhibiting strong zero-shot generalization capabilities.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºâ€œæŒ‡ä»£å¼å›¾å±‚åˆ†è§£â€(RLD)ä»»åŠ¡ï¼šåœ¨ç”¨æˆ·æç¤ºï¼ˆè¯­è¨€/ç‚¹æ¡†maskç­‰ï¼‰æ¡ä»¶ä¸‹ï¼Œä»å•å¼ RGBå›¾åƒé¢„æµ‹å¯ç¼–è¾‘çš„å®Œæ•´RGBAåˆ†å±‚è¡¨ç¤ºï¼Œå¹¶ç»™å‡ºæ•°æ®é›†ä¸åŸºçº¿æ¨¡å‹æ¨åŠ¨å¯è¯„æµ‹ç ”ç©¶ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å›¾åƒç¼–è¾‘/ç”Ÿæˆå¤šä»¥æ•´å›¾ä¸ºå•ä½ï¼Œéš¾ä»¥ç²¾ç¡®éš”ç¦»ä¸æ“æ§å•ä¸ªå¯¹è±¡ã€ç¯å¢ƒä¸æ•ˆæœï¼›è€Œæ˜¾å¼åˆ†å±‚æ›´ç¬¦åˆäººç±»ç¼–è¾‘æµç¨‹ä½†ç¼ºå°‘ç»Ÿä¸€ä»»åŠ¡å®šä¹‰ä¸å¤§è§„æ¨¡åŸºå‡†ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå®šä¹‰RLDä»»åŠ¡å¹¶æ„å»ºRefLadeæ•°æ®é›†ï¼ˆçº¦111ä¸‡å›¾åƒ-å›¾å±‚-æç¤ºä¸‰å…ƒç»„+10ä¸‡é«˜ä¿çœŸäººå·¥å±‚ï¼‰ï¼Œé…å¥—â€œæ„ŸçŸ¥+äººç±»åå¥½å¯¹é½â€çš„è‡ªåŠ¨è¯„æµ‹åè®®ï¼›åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºRefLayerä½œä¸ºæç¤ºæ¡ä»¶çš„åˆ†å±‚åˆ†è§£åŸºçº¿æ¨¡å‹ï¼Œå®ç°ä»RGBåˆ°å¤šRGBAå±‚çš„é¢„æµ‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜è¯¥æ•°æ®ä¸è¯„æµ‹ä½¿RLDæˆä¸ºå¯è®­ç»ƒã€å¯å¯é è¯„æµ‹çš„åŸºå‡†ä»»åŠ¡ï¼ŒRefLayerèƒ½äº§ç”Ÿé«˜ä¿çœŸä¸”è¯­ä¹‰å¯¹é½çš„åˆ†å±‚ç»“æœï¼Œå¹¶å±•ç°è¾ƒå¼ºé›¶æ ·æœ¬æ³›åŒ–ä¸å¯æ§ç¼–è¾‘æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šæŒ‡ä»£å¼å›¾å±‚åˆ†è§£, æç¤ºæ¡ä»¶åˆ†è§£, åˆ†å±‚åœºæ™¯è¡¨ç¤º, ç»„åˆå¼å›¾åƒç”Ÿæˆ, ç©ºé—´æç¤ºï¼ˆç‚¹/æ¡†/æ©è†œï¼‰, è‡ªç„¶è¯­è¨€æç¤º, å¤§è§„æ¨¡å›¾åƒ-å›¾å±‚-æç¤ºæ•°æ®é›†, äººç±»åå¥½å¯¹é½è¯„æµ‹, é›¶æ ·æœ¬æ³›åŒ–

**è¯„åˆ†**ï¼š36

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19358v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19358v1.pdf)

---

## [6. MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations](https://arxiv.org/abs/2602.19357v1)

**ä½œè€…**ï¼šNilay Yilmaz, Maitreya Patel, Naga Sai Abhiram Kusumba ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. To explore whether state-of-the-art Vision-Language Models (VLMs) exhibit this ability, we develop MentalBlackboard, an open-ended spatial visualization benchmark for Paper Folding and Hole Punching tests within two core tasks: prediction and planning. Our prediction experiments reveal that models struggle with applying symmetrical transformations, even when they predict the sequence of unfolding steps correctly. Also, rotations introduce a significant challenge to the physical situational awareness for models. The planning task reveals limitations of models in analyzing symmetrical relationships and in implementing the multi-stage symmetry process, with Claude Opus 4.1 achieving the highest planning score at an accuracy of 10\%. The top-performing model, o3, attains a peak performance of 71.6\% on the generalization task, which does not require spatial visualization but transfers spatial data; however, it achieves only 25\% accuracy on text-based prediction tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMentalBlackboard åŸºå‡†ç”¨äºè¯„ä¼°VLMåœ¨çº¸æŠ˜å /æ‰“å­”ç­‰ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸­çš„â€œé¢„æµ‹+è§„åˆ’â€èƒ½åŠ›ï¼Œç»“æœæ˜¾ç¤ºä¸»æµæ¨¡å‹åœ¨å¯¹ç§°å˜æ¢ä¸æ—‹è½¬æ¨ç†ä¸Šæ˜¾è‘—è–„å¼±ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç©ºé—´å¯è§†åŒ–æ˜¯äººç±»è®¤çŸ¥ä¸­å°†åŠ¨ä½œä¸æ„ŸçŸ¥è”ç»“çš„å…³é”®èƒ½åŠ›ï¼Œä½†ç°æœ‰VLMæ˜¯å¦å…·å¤‡å¯¹ç‰©ä½“è¿›è¡Œå¿ƒæ™ºå˜æ¢ï¼ˆå¯¹ç§°ã€æ—‹è½¬ã€å±•å¼€ï¼‰çš„èƒ½åŠ›ä»ç¼ºå°‘ç³»ç»Ÿè¯„æµ‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºå¼€æ”¾å¼åŸºå‡† MentalBlackboardï¼Œè¦†ç›–çº¸æŠ˜å ä¸æ‰“å­”ä¸¤ç±»é¢˜å‹ï¼Œå¹¶è®¾ç½®é¢„æµ‹ä»»åŠ¡ï¼ˆç»™å®šæŠ˜å /å±•å¼€è¿‡ç¨‹æ¨æ–­æœ€ç»ˆç»“æœï¼‰ä¸è§„åˆ’ä»»åŠ¡ï¼ˆåæ¨æˆ–è®¾è®¡å¤šé˜¶æ®µå¯¹ç§°è¿‡ç¨‹ï¼‰ä»¥æµ‹è¯•æ¨¡å‹çš„ç©ºé—´å˜æ¢æ¨ç†ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜æ¨¡å‹å¸¸åœ¨å¯¹ç§°å˜æ¢åº”ç”¨ä¸Šå¤±è´¥ï¼Œå³ä½¿èƒ½æ­£ç¡®é¢„æµ‹å±•å¼€æ­¥éª¤ï¼›æ—‹è½¬æ˜¾è‘—å‰Šå¼±å…¶ç‰©ç†æƒ…å¢ƒç†è§£ã€‚è§„åˆ’ä»»åŠ¡æ•´ä½“å‡†ç¡®ç‡å¾ˆä½ï¼ˆå¦‚ Claude Opus 4.1 çº¦10%ï¼‰ï¼Œè€Œ o3 åœ¨ä¸è¦æ±‚ç©ºé—´å¯è§†åŒ–ã€ä»…è¿ç§»ç©ºé—´æ•°æ®çš„æ³›åŒ–ä»»åŠ¡ä¸Šå¯è¾¾71.6%ï¼Œä½†åœ¨çº¯æ–‡æœ¬é¢„æµ‹ä¸Šä»…çº¦25%ã€‚

**å…³é”®è¯**ï¼šç©ºé—´å¯è§†åŒ–, ç©ºé—´æ¨ç†, è§†è§‰è¯­è¨€æ¨¡å‹è¯„æµ‹, ç©ºé—´æ¨ç†åŸºå‡†, æ—‹è½¬å˜æ¢, è§„åˆ’ä»»åŠ¡, é¢„æµ‹ä»»åŠ¡, æ³›åŒ–è¯„æµ‹

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19357v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19357v1.pdf)

---

## [7. UP-Fuse: Uncertainty-guided LiDAR-Camera Fusion for 3D Panoptic Segmentation](https://arxiv.org/abs/2602.19349v1)

**ä½œè€…**ï¼šRohit Mohan, Florian Drews, Yakov Miron ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

LiDAR-camera fusion enhances 3D panoptic segmentation by leveraging camera images to complement sparse LiDAR scans, but it also introduces a critical failure mode. Under adverse conditions, degradation or failure of the camera sensor can significantly compromise the reliability of the perception system. To address this problem, we introduce UP-Fuse, a novel uncertainty-aware fusion framework in the 2D range-view that remains robust under camera sensor degradation, calibration drift, and sensor failure. Raw LiDAR data is first projected into the range-view and encoded by a LiDAR encoder, while camera features are simultaneously extracted and projected into the same shared space. At its core, UP-Fuse employs an uncertainty-guided fusion module that dynamically modulates cross-modal interaction using predicted uncertainty maps. These maps are learned by quantifying representational divergence under diverse visual degradations, ensuring that only reliable visual cues influence the fused representation. The fused range-view features are decoded by a novel hybrid 2D-3D transformer that mitigates spatial ambiguities inherent to the 2D projection and directly predicts 3D panoptic segmentation masks. Extensive experiments on Panoptic nuScenes, SemanticKITTI, and our introduced Panoptic Waymo benchmark demonstrate the efficacy and robustness of UP-Fuse, which maintains strong performance even under severe visual corruption or misalignment, making it well suited for robotic perception in safety-critical settings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šUP-Fuse æå‡ºä¸€ç§ä¸ç¡®å®šæ€§å¼•å¯¼çš„ LiDAR-ç›¸æœºèåˆæ¡†æ¶ï¼Œåœ¨ç›¸æœºé€€åŒ–ã€æ ‡å®šæ¼‚ç§»æˆ–å¤±æ•ˆæ—¶ä»èƒ½ç¨³å¥è¿›è¡Œ 3D å…¨æ™¯åˆ†å‰²ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿ LiDAR-ç›¸æœºèåˆè™½èƒ½æå‡ç²¾åº¦ï¼Œä½†åœ¨é›¨é›¾ã€é®æŒ¡ã€æ›å…‰å¼‚å¸¸æˆ–æ ‡å®šåç§»ç­‰æƒ…å†µä¸‹ä¼šå¼•å…¥é”™è¯¯è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ„ŸçŸ¥ç³»ç»Ÿå¯é æ€§æ˜¾è‘—ä¸‹é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°† LiDAR æŠ•å½±åˆ° 2D range-view å¹¶ç¼–ç ï¼ŒåŒæ—¶æå–å¹¶æŠ•å½±ç›¸æœºç‰¹å¾åˆ°åŒä¸€ç©ºé—´ï¼›é€šè¿‡å­¦ä¹ çš„ä¸ç¡®å®šæ€§å›¾è¡¡é‡è§†è§‰é€€åŒ–ä¸‹çš„è¡¨å¾åˆ†æ­§ï¼ŒåŠ¨æ€æŠ‘åˆ¶ä¸å¯é çš„è·¨æ¨¡æ€äº¤äº’ï¼›å†ç”¨æ··åˆ 2D-3D Transformer è§£ç ä»¥ç¼“è§£ 2D æŠ•å½±å¸¦æ¥çš„ç©ºé—´æ­§ä¹‰å¹¶ç›´æ¥é¢„æµ‹ 3D panoptic æ©ç ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ Panoptic nuScenesã€SemanticKITTI å’Œæ–°å¢çš„ Panoptic Waymo ä¸Šï¼ŒUP-Fuse åœ¨æ­£å¸¸æ¡ä»¶ä¸‹ä¿æŒå¼ºæ€§èƒ½ï¼Œå¹¶åœ¨ä¸¥é‡è§†è§‰è…èš€ã€é”™ä½æˆ–ç›¸æœºæ•…éšœæ—¶æ˜¾è‘—ä¼˜äºå¸¸è§„èåˆæ–¹æ³•ï¼Œä½“ç°å‡ºæ›´é«˜çš„å®‰å…¨å…³é”®åœºæ™¯é²æ£’æ€§ã€‚

**å…³é”®è¯**ï¼šæ¿€å…‰é›·è¾¾-ç›¸æœºèåˆ, ä¸‰ç»´å…¨æ™¯åˆ†å‰², ä¸ç¡®å®šæ€§å¼•å¯¼èåˆ, è·¨æ¨¡æ€äº¤äº’è°ƒåˆ¶, ä¸ç¡®å®šæ€§å›¾, äºŒç»´è·ç¦»è§†å›¾, ä¼ æ„Ÿå™¨é€€åŒ–é²æ£’æ€§, æ ‡å®šæ¼‚ç§»é²æ£’æ€§, è§†è§‰è…èš€é²æ£’æ€§, è¡¨å¾å·®å¼‚åº¦é‡, æœºå™¨äººå®‰å…¨å…³é”®æ„ŸçŸ¥

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19349v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19349v1.pdf)

---

## [8. MultiDiffSense: Diffusion-Based Multi-Modal Visuo-Tactile Image Generation Conditioned on Object Shape and Contact Pose](https://arxiv.org/abs/2602.19348v1)

**ä½œè€…**ï¼šSirine Bhouri, Lan Wei, Jian-Qing Zheng ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Acquiring aligned visuo-tactile datasets is slow and costly, requiring specialised hardware and large-scale data collection. Synthetic generation is promising, but prior methods are typically single-modality, limiting cross-modal learning. We present MultiDiffSense, a unified diffusion model that synthesises images for multiple vision-based tactile sensors (ViTac, TacTip, ViTacTip) within a single architecture. Our approach uses dual conditioning on CAD-derived, pose-aligned depth maps and structured prompts that encode sensor type and 4-DoF contact pose, enabling controllable, physically consistent multi-modal synthesis. Evaluating on 8 objects (5 seen, 3 novel) and unseen poses, MultiDiffSense outperforms a Pix2Pix cGAN baseline in SSIM by +36.3% (ViTac), +134.6% (ViTacTip), and +64.7% (TacTip). For downstream 3-DoF pose estimation, mixing 50% synthetic with 50% real halves the required real data while maintaining competitive performance. MultiDiffSense alleviates the data-collection bottleneck in tactile sensing and enables scalable, controllable multi-modal dataset generation for robotic applications.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMultiDiffSense ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ CAD æ·±åº¦ä¸æ¥è§¦ä½å§¿æ¡ä»¶ä¸‹å¯æ§ç”Ÿæˆå¤šç§è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨çš„åˆæˆå›¾åƒï¼Œå¹¶æ˜¾è‘—æå‡ç”Ÿæˆè´¨é‡ä¸ä¸‹æ¸¸ä½å§¿å­¦ä¹ çš„æ•°æ®æ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¯¹é½çš„è§†è§‰-è§¦è§‰æ•°æ®é‡‡é›†æ˜‚è´µä¸”è€—æ—¶ï¼Œé™åˆ¶è§¦è§‰å­¦ä¹ è§„æ¨¡åŒ–ï¼›ç°æœ‰åˆæˆæ–¹æ³•å¤šä¸ºå•æ¨¡æ€ï¼Œéš¾ä»¥æ”¯æŒè·¨ä¼ æ„Ÿå™¨/è·¨æ¨¡æ€å­¦ä¹ ä¸å¯æ§ç”Ÿæˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºç»Ÿä¸€æ‰©æ•£ç”Ÿæˆæ¡†æ¶ï¼ŒåŒæ—¶ç”Ÿæˆ ViTacã€TacTipã€ViTacTip ç­‰å¤šä¼ æ„Ÿå™¨å›¾åƒï¼›é‡‡ç”¨â€œåŒæ¡ä»¶â€è¾“å…¥ï¼šç”± CAD æ¨å¾—å¹¶ä¸å§¿æ€å¯¹é½çš„æ·±åº¦å›¾ä½œä¸ºå½¢çŠ¶/å‡ ä½•çº¦æŸï¼Œä»¥åŠåŒ…å«ä¼ æ„Ÿå™¨ç±»å‹ä¸4-DoFæ¥è§¦ä½å§¿çš„ç»“æ„åŒ–æç¤ºä»¥å®ç°å¯æ§ã€ç‰©ç†ä¸€è‡´çš„åˆæˆã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å·²è§/æ–°ç‰©ä½“ä¸æœªè§ä½å§¿ä¸Šï¼Œè¾ƒ Pix2Pix cGAN åœ¨ SSIM ä¸Šå¤§å¹…æå‡ï¼ˆä¸åŒä¼ æ„Ÿå™¨æå‡ 36.3%~134.6%ï¼‰ï¼›åœ¨ 3-DoF ä½å§¿ä¼°è®¡ä»»åŠ¡ä¸­ï¼Œ50%åˆæˆ+50%çœŸå®æ•°æ®å¯åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å°†çœŸå®æ•°æ®éœ€æ±‚å‡åŠï¼Œç¼“è§£è§¦è§‰æ•°æ®é‡‡é›†ç“¶é¢ˆã€‚

**å…³é”®è¯**ï¼šDiffusion, å¤šæ¨¡æ€ç”Ÿæˆ, è§†è§‰-è§¦è§‰, è§¦è§‰å›¾åƒåˆæˆ, æ¡ä»¶ç”Ÿæˆ, æ·±åº¦å›¾æ¡ä»¶, CADå½¢çŠ¶å…ˆéªŒ, æ¥è§¦ä½å§¿æ¡ä»¶, è·¨æ¨¡æ€å­¦ä¹ , åˆæˆæ•°æ®å¢å¼º, æœºå™¨äººè§¦è§‰æ„ŸçŸ¥, ä½å§¿ä¼°è®¡

**è¯„åˆ†**ï¼š32

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19348v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19348v1.pdf)

---

## cs.LG

## [9. Spiking Graph Predictive Coding for Reliable OOD Generalization](https://arxiv.org/abs/2602.19392v1)

**ä½œè€…**ï¼šJing Ren, Jiapeng Du, Bowen Li ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.SI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Graphs provide a powerful basis for modeling Web-based relational data, with expressive GNNs to support the effective learning in dynamic web environments. However, real-world deployment is hindered by pervasive out-of-distribution (OOD) shifts, where evolving user activity and changing content semantics alter feature distributions and labeling criteria. These shifts often lead to unstable or overconfident predictions, undermining the trustworthiness required for Web4Good applications. Achieving reliable OOD generalization demands principled and interpretable uncertainty estimation; however, existing methods are largely post-hoc, insensitive to distribution shifts, and unable to explain where uncertainty arises especially in high-stakes settings. To address these limitations, we introduce SpIking GrapH predicTive coding (SIGHT), an uncertainty-aware plug-in graph learning module for reliable OOD Generalization. SIGHT performs iterative, error-driven correction over spiking graph states, enabling models to expose internal mismatch signals that reveal where predictions become unreliable. Across multiple graph benchmarks and diverse OOD scenarios, SIGHT consistently enhances predictive accuracy, uncertainty estimation, and interpretability when integrated with GNNs.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSIGHT é€šè¿‡è„‰å†²å¼å›¾é¢„æµ‹ç¼–ç çš„è¿­ä»£è¯¯å·®æ ¡æ­£æœºåˆ¶ï¼Œè®©GNNåœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯ä¸‹åŒæ—¶æå‡å‡†ç¡®æ€§ã€å¯é ä¸ç¡®å®šæ€§ä¼°è®¡ä¸å¯è§£é‡Šæ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šçœŸå®å›¾æ•°æ®ï¼ˆå¦‚Webç¯å¢ƒï¼‰å­˜åœ¨é¢‘ç¹çš„OODåˆ†å¸ƒæ¼‚ç§»ï¼Œä¼šå¯¼è‡´GNNé¢„æµ‹ä¸ç¨³å®šä¸”è¿‡åº¦è‡ªä¿¡ï¼Œå½±å“é«˜é£é™©åº”ç”¨çš„å¯ä¿¡éƒ¨ç½²ã€‚ç°æœ‰ä¸ç¡®å®šæ€§æ–¹æ³•å¤šä¸ºäº‹åæ ¡å‡†ã€å¯¹åˆ†å¸ƒå˜åŒ–ä¸æ•æ„Ÿï¼Œä¸”éš¾ä»¥è§£é‡Šä¸ç¡®å®šæ€§æ¥æºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºå¯æ’æ‹”æ¨¡å—SIGHTï¼Œåœ¨å›¾çš„â€œè„‰å†²/å°–å³°â€çŠ¶æ€ä¸Šè¿›è¡Œè¿­ä»£ã€è¯¯å·®é©±åŠ¨çš„é¢„æµ‹ç¼–ç æ›´æ–°ï¼Œé€šè¿‡å†…éƒ¨â€œå¤±é…/æ®‹å·®ä¿¡å·â€å¯¹è¡¨ç¤ºè¿›è¡Œçº é”™å¹¶æ˜¾å¼æš´éœ²ä¸ç¡®å®šæ€§æ¥æºã€‚å°†å…¶é›†æˆåˆ°å„ç±»GNNä¸­ï¼Œä»¥è¿­ä»£æ¨ç†è¿‡ç¨‹äº§ç”Ÿæ›´ç¨³å¥çš„é¢„æµ‹ä¸å¯è§£é‡Šçš„ç½®ä¿¡åº¦ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šä¸ªå›¾åŸºå‡†ä¸å¤šç§OODè®¾ç½®ä¸‹ï¼ŒSIGHTé›†æˆåå¯ç¨³å®šæå‡é¢„æµ‹ç²¾åº¦ã€ä¸ç¡®å®šæ€§ä¼°è®¡è´¨é‡å’Œè§£é‡Šæ€§ï¼Œè¡¨æ˜è¯¯å·®é©±åŠ¨çš„å†…éƒ¨æ ¡æ­£å¯¹å¯é OODæ³›åŒ–æœ‰æ•ˆã€‚

**å…³é”®è¯**ï¼šå›¾ç¥ç»ç½‘ç»œ, å›¾è¡¨ç¤ºå­¦ä¹ , åˆ†å¸ƒå¤–æ³›åŒ–, åˆ†å¸ƒæ¼‚ç§»é²æ£’æ€§, ä¸ç¡®å®šæ€§ä¼°è®¡, ä¸ç¡®å®šæ€§æ ¡å‡†, å¯è§£é‡Šæ€§, é¢„æµ‹ç¼–ç , è„‰å†²ç¥ç»ç½‘ç»œ, è¿­ä»£è¯¯å·®æ ¡æ­£, åŠ¨æ€å›¾å­¦ä¹ 

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19392v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19392v1.pdf)

---

## [10. VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery](https://arxiv.org/abs/2602.21381v1)

**ä½œè€…**ï¼šGene Yu, Ce Guo, Wayne Luk  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CE  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šVCDFé€šè¿‡åœ¨æ—¶é—´åˆ†å—å­åºåˆ—ä¸Šåšç¨³å®šæ€§éªŒè¯ä¸å…±è¯†èšåˆï¼Œä¸ºç°æœ‰æ—¶é—´åºåˆ—å› æœå‘ç°ç®—æ³•æä¾›ä¸€å±‚ä¸æ”¹æ¨¡å‹çš„é²æ£’æ€§å¢å¼ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ—¶é—´åºåˆ—å› æœå‘ç°æ–¹æ³•å®¹æ˜“å—å™ªå£°ã€éå¹³ç¨³æ€§å’Œé‡‡æ ·æ³¢åŠ¨å½±å“ï¼Œå¯¼è‡´å› æœè¾¹ä¸ç¨³å®šã€ç»“æœå¯é‡å¤æ€§å·®ã€‚éœ€è¦ä¸€ç§ä¸å…·ä½“ç®—æ³•æ— å…³ã€èƒ½æå‡å› æœå…³ç³»å¯é æ€§çš„é€šç”¨æ¡†æ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šVCDFå°†æ—¶é—´åºåˆ—åˆ’åˆ†ä¸ºå¤šä¸ªè¢«é˜»æ–­ï¼ˆblockedï¼‰çš„æ—¶é—´å­é›†ï¼Œåœ¨å„å­é›†ä¸Šé‡å¤è¿è¡Œä»»æ„åŸºç¡€å› æœå‘ç°ç®—æ³•ï¼ˆå¦‚VAR-LiNGAMã€PCMCIï¼‰ï¼Œå†ä¾æ®å› æœå…³ç³»åœ¨ä¸åŒå­é›†ä¸­çš„ä¸€è‡´æ€§è¿›è¡ŒéªŒè¯ä¸å…±è¯†ç­›é€‰ã€‚è¯¥æ¡†æ¶ä½œä¸ºâ€œå¤–å±‚å¯é æ€§å±‚â€å·¥ä½œï¼Œä¸è¦æ±‚ä¿®æ”¹åº•å±‚ç®—æ³•å‡è®¾æˆ–è®­ç»ƒæµç¨‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨åˆæˆæ•°æ®ä¸Šï¼ŒVCDFä½¿VAR-LiNGAMçš„window/summary F1å¹³å‡æå‡çº¦0.08â€“0.12ï¼Œä¸”å¯¹ä¸­é•¿åºåˆ—æ”¶ç›Šæ›´æ˜æ˜¾ï¼Œé•¿åº¦â‰¥1000æ—¶æœ€é«˜å¯è¾¾0.18çš„ç»å¯¹æå‡ã€‚åœ¨æ¨¡æ‹ŸfMRIä¸ITç›‘æ§ç­‰æ›´è´´è¿‘çœŸå®å™ªå£°åœºæ™¯ä¸­ï¼Œä¹Ÿè¡¨ç°å‡ºæ›´é«˜çš„ç»“æ„å‡†ç¡®æ€§ä¸ç¨³å®šæ€§ï¼Œè¯æ˜å…¶èƒ½æœ‰æ•ˆæå‡æ—¶é—´åºåˆ—å› æœå‘ç°ç»“æœçš„å¯é æ€§ã€‚

**å…³é”®è¯**ï¼šé²æ£’æ€§å¢å¼º, å› æœå…³ç³»ç¨³å®šæ€§, éå¹³ç¨³æ€§, é‡‡æ ·å˜å¼‚, å™ªå£°é²æ£’æ€§, åˆ†å—æ—¶é—´çª—å£, å…±è¯†éªŒè¯æ¡†æ¶, ITç›‘æ§æ—¶åº

**è¯„åˆ†**ï¼š19

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21381v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21381v1.pdf)

---

## [11. Stable Deep Reinforcement Learning via Isotropic Gaussian Representations](https://arxiv.org/abs/2602.19373v1)

**ä½œè€…**ï¼šAli Saheb, Johan Obando-Ceron, Aaron Courville ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Deep reinforcement learning systems often suffer from unstable training dynamics due to non-stationarity, where learning objectives and data distributions evolve over time. We show that under non-stationary targets, isotropic Gaussian embeddings are provably advantageous. In particular, they induce stable tracking of time-varying targets for linear readouts, achieve maximal entropy under a fixed variance budget, and encourage a balanced use of all representational dimensions--all of which enable agents to be more adaptive and stable. Building on this insight, we propose the use of Sketched Isotropic Gaussian Regularization for shaping representations toward an isotropic Gaussian distribution during training. We demonstrate empirically, over a variety of domains, that this simple and computationally inexpensive method improves performance under non-stationarity while reducing representation collapse, neuron dormancy, and training instability.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºç”¨â€œå„å‘åŒæ€§é«˜æ–¯â€çº¦æŸæ¥å¡‘é€ æ·±åº¦å¼ºåŒ–å­¦ä¹ è¡¨å¾ï¼Œåœ¨éå¹³ç¨³ç›®æ ‡ä¸‹æ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§å¹¶å‡å°‘è¡¨å¾é€€åŒ–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›®æ ‡ä¸æ•°æ®åˆ†å¸ƒä¸æ–­å˜åŒ–ï¼ˆéå¹³ç¨³ï¼‰ï¼Œæ˜“å¯¼è‡´è¡¨ç¤ºåå¡Œã€ç¥ç»å…ƒä¼‘çœ ä¸æ€§èƒ½éœ‡è¡ã€‚ä½œè€…å¸Œæœ›æ‰¾åˆ°ä¸€ç§ç®€å•ã€ä½å¼€é”€ä¸”æœ‰ç†è®ºæ”¯æ’‘çš„è¡¨å¾æ­£åˆ™ï¼Œä»¥å¢å¼ºå¯¹æ—¶å˜ç›®æ ‡çš„é€‚åº”ä¸ç¨³å®šè·Ÿè¸ªã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»ç†è®ºä¸Šè¯´æ˜åœ¨éå¹³ç¨³ç›®æ ‡ä¸‹ï¼Œå„å‘åŒæ€§é«˜æ–¯åµŒå…¥å¯¹çº¿æ€§è¯»å‡ºå…·æœ‰æ›´ç¨³å®šçš„æ—¶å˜ç›®æ ‡è·Ÿè¸ªæ€§è´¨ï¼Œå¹¶åœ¨å›ºå®šæ–¹å·®é¢„ç®—ä¸‹å®ç°æœ€å¤§ç†µã€ä¿ƒè¿›å„ç»´åº¦å‡è¡¡ä½¿ç”¨ã€‚åŸºäºæ­¤æå‡º Sketched Isotropic Gaussian Regularizationï¼ˆè‰å›¾å¼å„å‘åŒæ€§é«˜æ–¯æ­£åˆ™ï¼‰ï¼Œåœ¨è®­ç»ƒä¸­å°†è¡¨ç¤ºåˆ†å¸ƒæ‹‰å‘å„å‘åŒæ€§é«˜æ–¯ä¸”è®¡ç®—å¼€é”€è¾ƒä½ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§ä»»åŠ¡åŸŸçš„å®éªŒä¸­ï¼Œè¯¥æ­£åˆ™åœ¨éå¹³ç¨³ç¯å¢ƒä¸‹æå‡æ€§èƒ½å¹¶é™ä½è®­ç»ƒä¸ç¨³å®šã€‚æ–¹æ³•è¿˜èƒ½ç¼“è§£è¡¨ç¤ºåå¡Œä¸ç¥ç»å…ƒä¼‘çœ ï¼Œä½¿è¡¨å¾æ›´å‡è¡¡ã€é€‚åº”æ€§æ›´å¼ºã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å¼ºåŒ–å­¦ä¹ , éå¹³ç¨³å­¦ä¹ , è®­ç»ƒç¨³å®šæ€§, å„å‘åŒæ€§é«˜æ–¯è¡¨ç¤º, é«˜æ–¯åµŒå…¥, å„å‘åŒæ€§é«˜æ–¯æ­£åˆ™åŒ–, æœ€å¤§ç†µè¡¨ç¤º, è¡¨å¾å¡Œç¼©, ç¥ç»å…ƒä¼‘çœ 

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19373v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19373v1.pdf)

---

## [12. Golden Layers and Where to Find Them: Improved Knowledge Editing for Large Language Models Via Layer Gradient Analysis](https://arxiv.org/abs/2602.20207v1)

**ä½œè€…**ï¼šShrestha Datta, Hongfu Liu, Anshuman Chhabra  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Knowledge editing in Large Language Models (LLMs) aims to update the model's prediction for a specific query to a desired target while preserving its behavior on all other inputs. This process typically involves two stages: identifying the layer to edit and performing the parameter update. Intuitively, different queries may localize knowledge at different depths of the model, resulting in different sample-wise editing performance for a fixed editing layer. In this work, we hypothesize the existence of fixed golden layers that can achieve near-optimal editing performance similar to sample-wise optimal layers. To validate this hypothesis, we provide empirical evidence by comparing golden layers against ground-truth sample-wise optimal layers. Furthermore, we show that golden layers can be reliably identified using a proxy dataset and generalize effectively to unseen test set queries across datasets. Finally, we propose a novel method, namely Layer Gradient Analysis (LGA) that estimates golden layers efficiently via gradient-attribution, avoiding extensive trial-and-error across multiple editing runs. Extensive experiments on several benchmark datasets demonstrate the effectiveness and robustness of our LGA approach across different LLM types and various knowledge editing methods.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡å±‚æ¢¯åº¦åˆ†ææœ‰æ•ˆè¯†åˆ«å’Œåˆ©ç”¨å›ºå®šçš„é»„é‡‘å±‚æ¥æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç ”ç©¶æ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šæŸ¥è¯¢çš„çŸ¥è¯†æ›´æ–°èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¯¹å…¶ä»–è¾“å…¥çš„è¡Œä¸ºä¸å˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å±‚æ¢¯åº¦åˆ†ææ–¹æ³•ï¼Œé€šè¿‡æ¢¯åº¦å½’å› é«˜æ•ˆè¯†åˆ«é»„é‡‘å±‚ï¼Œé¿å…å¤šæ¬¡ç¼–è¾‘å®éªŒçš„è¯•é”™è¿‡ç¨‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœéªŒè¯äº†é»„é‡‘å±‚çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œå¹¶è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¸åŒç±»å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹åŠå„ç§çŸ¥è¯†ç¼–è¾‘æ–¹æ³•ä¸­çš„é€‚ç”¨æ€§ã€‚

**å…³é”®è¯**ï¼šçŸ¥è¯†ç¼–è¾‘, LLM, å±‚æ¢¯åº¦åˆ†æ, é»„é‡‘å±‚, å‚æ•°æ›´æ–°, æ ·æœ¬ä¼˜åŒ–, æ•°æ®é›†æ³›åŒ–, å®éªŒè¯„ä¼°

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.20207v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.20207v1.pdf)

---

## [13. LLMs Can Learn to Reason Via Off-Policy RL](https://arxiv.org/abs/2602.19362v1)

**ä½œè€…**ï¼šDaniel Ritter, Owen Oertell, Bradley Guo ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement learning (RL) approaches for Large Language Models (LLMs) frequently use on-policy algorithms, such as PPO or GRPO. However, policy lag from distributed training architectures and differences between the training and inference policies break this assumption, making the data off-policy by design. To rectify this, prior work has focused on making this off-policy data appear more on-policy, either via importance sampling (IS), or by more closely aligning the training and inference policies by explicitly modifying the inference engine. In this work, we embrace off-policyness and propose a novel off-policy RL algorithm that does not require these modifications: Optimal Advantage-based Policy Optimization with Lagged Inference policy (OAPL). We show that OAPL outperforms GRPO with importance sampling on competition math benchmarks, and can match the performance of a publicly available coding model, DeepCoder, on LiveCodeBench, while using 3x fewer generations during training. We further empirically demonstrate that models trained via OAPL have improved test time scaling under the Pass@k metric. OAPL allows for efficient, effective post-training even with lags of more than 400 gradient steps between the training and inference policies, 100x more off-policy than prior approaches.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºå¹¶éªŒè¯ä¸€ç§é€‚ç”¨äºLLMåˆ†å¸ƒå¼è®­ç»ƒä¸­å¤©ç„¶ç¦»ç­–ç•¥æ•°æ®çš„ç¦»ç­–ç•¥RLç®—æ³•OAPLï¼Œåœ¨æ•°å­¦ä¸ä»£ç åŸºå‡†ä¸Šä¼˜äº/å¯åŒ¹æ•Œç°æœ‰æ–¹æ³•ä¸”è®­ç»ƒæ›´é«˜æ•ˆã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šLLMåè®­ç»ƒå¸¸ç”¨PPO/GRPOç­‰on-policyç®—æ³•ï¼Œä½†åˆ†å¸ƒå¼è®­ç»ƒå¸¦æ¥çš„policy lagåŠè®­ç»ƒ/æ¨ç†ç­–ç•¥ä¸ä¸€è‡´ä½¿æ•°æ®â€œè®¾è®¡ä¸Šå°±ç¦»ç­–ç•¥â€ï¼Œä¼ ç»Ÿç”¨ISæˆ–æ”¹æ¨ç†å¼•æ“æ¥â€œä¼ªè£…æˆon-policyâ€æˆæœ¬é«˜ä¸”å—é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºOptimal Advantage-based Policy Optimization with Lagged Inference policyï¼ˆOAPLï¼‰ï¼Œç›´æ¥åœ¨å­˜åœ¨æ˜¾è‘—æ»åï¼ˆè®­ç»ƒä¸æ¨ç†ç­–ç•¥ç›¸éš”æ•°ç™¾æ­¥ï¼‰æ¡ä»¶ä¸‹è¿›è¡Œç¨³å®šçš„ä¼˜åŠ¿é©±åŠ¨ç­–ç•¥ä¼˜åŒ–ï¼Œæ— éœ€é‡è¦æ€§é‡‡æ ·æˆ–æ”¹åŠ¨æ¨ç†ç³»ç»Ÿã€‚

**ä¸»è¦ç»“è®º**ï¼šOAPLåœ¨ç«èµ›æ•°å­¦åŸºå‡†ä¸Šè¶…è¿‡å¸¦ISçš„GRPOï¼Œåœ¨LiveCodeBenchä¸Šä»¥çº¦3å€æ›´å°‘ç”Ÿæˆæ¬¡æ•°è¾¾åˆ°å¯ä¸DeepCoderåŒ¹æ•Œçš„è¡¨ç°ï¼Œå¹¶æå‡Pass@kä¸‹çš„æµ‹è¯•æ—¶æ‰©å±•æ€§ï¼›å³ä½¿æ»åè¶…è¿‡400ä¸ªæ¢¯åº¦æ­¥ã€æ¯”ä»¥å¾€æ–¹æ³•æ›´â€œç¦»ç­–ç•¥â€çº¦100å€ä»èƒ½æœ‰æ•ˆè®­ç»ƒã€‚

**å…³é”®è¯**ï¼šLLMå¼ºåŒ–å­¦ä¹ , ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ , ç­–ç•¥æ»å, åˆ†å¸ƒå¼è®­ç»ƒ, é‡è¦æ€§é‡‡æ ·, ä¼˜åŠ¿å‡½æ•°ä¼˜åŒ–, ç­–ç•¥ä¼˜åŒ–ç®—æ³•, åè®­ç»ƒ, æµ‹è¯•æ—¶æ‰©å±•, ä»£ç ç”Ÿæˆè¯„æµ‹

**è¯„åˆ†**ï¼š36

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19362v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19362v1.pdf)

---

## [14. Active perception and disentangled representations allow continual, episodic zero and few-shot learning](https://arxiv.org/abs/2602.19355v1)

**ä½œè€…**ï¼šDavid Rawlinson, Gideon Kowadlo  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Generalization is often regarded as an essential property of machine learning systems. However, perhaps not every component of a system needs to generalize. Training models for generalization typically produces entangled representations at the boundaries of entities or classes, which can lead to destructive interference when rapid, high-magnitude updates are required for continual or few-shot learning. Techniques for fast learning with non-interfering representations exist, but they generally fail to generalize. Here, we describe a Complementary Learning System (CLS) in which the fast learner entirely foregoes generalization in exchange for continual zero-shot and few-shot learning. Unlike most CLS approaches, which use episodic memory primarily for replay and consolidation, our fast, disentangled learner operates as a parallel reasoning system. The fast learner can overcome observation variability and uncertainty by leveraging a conventional slow, statistical learner within an active perception system: A contextual bias provided by the fast learner induces the slow learner to encode novel stimuli in familiar, generalized terms, enabling zero-shot and few-shot learning. This architecture demonstrates that fast, context-driven reasoning can coexist with slow, structured generalization, providing a pathway for robust continual learning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§ç»“åˆä¸»åŠ¨æ„ŸçŸ¥çš„äº’è¡¥å­¦ä¹ ç³»ç»Ÿï¼ˆCLSï¼‰ï¼šè®©â€œå¿«é€Ÿå­¦ä¹ å™¨â€æ”¾å¼ƒæ³›åŒ–ã€ä¸“æ³¨äºå»çº ç¼ çš„æƒ…æ™¯æ¨ç†ï¼Œå¹¶å€ŸåŠ©â€œæ…¢é€Ÿå­¦ä¹ å™¨â€çš„ç»Ÿè®¡æ³›åŒ–æ¥å®ç°æŒç»­çš„é›¶æ ·æœ¬ä¸å°æ ·æœ¬å­¦ä¹ ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè¿½æ±‚å¼ºæ³›åŒ–å¾€å¾€ä¼šåœ¨ç±»åˆ«è¾¹ç•Œå½¢æˆçº ç¼ è¡¨å¾ï¼Œå¯¼è‡´åœ¨æŒç»­å­¦ä¹ æˆ–å°æ ·æœ¬å­¦ä¹ æ‰€éœ€çš„å¿«é€Ÿå¤§å¹…æ›´æ–°æ—¶å‘ç”Ÿç¾éš¾æ€§å¹²æ‰°ï¼›è€Œç°æœ‰ä¸å¹²æ‰°çš„å¿«é€Ÿå­¦ä¹ æ–¹æ³•åˆå¸¸ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºå¹¶è¡Œçš„å¿«æ…¢åŒç³»ç»Ÿï¼šå¿«å­¦ä¹ å™¨ç”¨å»çº ç¼ è¡¨å¾è¿›è¡Œæƒ…æ™¯é©±åŠ¨çš„å¿«é€Ÿæ¨ç†ï¼ˆä¸è¿½æ±‚æ³›åŒ–ï¼‰ï¼Œå¹¶åœ¨ä¸»åŠ¨æ„ŸçŸ¥æ¡†æ¶ä¸­æä¾›ä¸Šä¸‹æ–‡åç½®ï¼Œå¼•å¯¼æ…¢å­¦ä¹ å™¨ç”¨å·²å­¦åˆ°çš„æ³›åŒ–æ¦‚å¿µå¯¹æ–°åˆºæ¿€è¿›è¡Œç¼–ç ï¼Œä»è€Œåœ¨è§‚æµ‹å™ªå£°/ä¸ç¡®å®šæ€§ä¸‹å®ç°é›¶/å°æ ·æœ¬è¯†åˆ«ä¸å­¦ä¹ ã€‚

**ä¸»è¦ç»“è®º**ï¼šé€šè¿‡è®©å¿«é€Ÿç³»ç»Ÿä¸“æ³¨äºä¸å¹²æ‰°çš„æƒ…æ™¯æ¨ç†ã€æ…¢é€Ÿç³»ç»Ÿä¸“æ³¨äºç»“æ„åŒ–æ³›åŒ–ï¼Œå¹¶ç”¨ä¸»åŠ¨æ„ŸçŸ¥æŠŠä¸¤è€…è€¦åˆï¼Œå¯åœ¨åŒä¸€æ¶æ„ä¸­åŒæ—¶è·å¾—é²æ£’çš„æŒç»­å­¦ä¹ èƒ½åŠ›ä¸é›¶/å°æ ·æœ¬å­¦ä¹ è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šæŒç»­å­¦ä¹ , é›¶æ ·æœ¬å­¦ä¹ , å°æ ·æœ¬å­¦ä¹ , äº’è¡¥å­¦ä¹ ç³»ç»Ÿï¼ˆCLSï¼‰, æƒ…æ™¯è®°å¿†, è§£è€¦è¡¨ç¤º, è¡¨å¾çº ç¼ , ç¾éš¾æ€§å¹²æ‰°, ä¸»åŠ¨æ„ŸçŸ¥, å¿«æ…¢å­¦ä¹ å™¨, ä¸Šä¸‹æ–‡åç½®

**è¯„åˆ†**ï¼š26

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19355v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19355v1.pdf)

---

## [15. Smooth Gate Functions for Soft Advantage Policy Optimization](https://arxiv.org/abs/2602.19345v1)

**ä½œè€…**ï¼šEgor Denisov, Svetlana Glazyrina, Maksim Kryzhanovskiy ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-22

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Group Relative Policy Optimization (GRPO) has significantly advanced the training of large language models and enhanced their reasoning capabilities, while it remains susceptible to instability due to the use of hard clipping. Soft Adaptive Policy Optimization (SAPO) addresses this limitation by replacing clipping with a smooth sigmoid-based gate function, which leads to more stable updates. We have decided to push this theory further and investigate the impact of different gate functions on both training stability and final model performance. We formalize the key properties that admissible gates should satisfy and identify several families of such functions for empirical evaluation. This paper presents an analysis of our findings based on experiments conducted with the Qwen2.5-7B-Instruct model on mathematical reasoning tasks. These results provide practical guidance for designing smoother and more robust policy optimization objectives for large language model training.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç³»ç»Ÿæ¯”è¾ƒä¸åŒâ€œå¹³æ»‘é—¨æ§å‡½æ•°â€æ›¿ä»£ç¡¬è£å‰ªåœ¨ä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ä¸­çš„æ•ˆæœï¼Œä»¥æå‡å¤§æ¨¡å‹RLè®­ç»ƒçš„ç¨³å®šæ€§ä¸æœ€ç»ˆæ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šGRPOç­‰æ–¹æ³•ä¾èµ–ç¡¬clippingï¼Œå®¹æ˜“é€ æˆè®­ç»ƒä¸ç¨³å®šï¼›SAPOç”¨sigmoidé—¨æ§ç¼“è§£ä½†é—¨æ§å‡½æ•°é€‰æ‹©ç¼ºä¹ç³»ç»Ÿç ”ç©¶ï¼Œå› æ­¤éœ€è¦æ¢ç´¢æ›´åˆé€‚çš„å¹³æ»‘é—¨æ§è®¾è®¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå½¢å¼åŒ–å®šä¹‰â€œå¯ç”¨é—¨æ§å‡½æ•°â€åº”æ»¡è¶³çš„å…³é”®æ€§è´¨ï¼Œå¹¶æ„é€ å¤šç±»æ»¡è¶³æ¡ä»¶çš„é—¨æ§å‡½æ•°æ—ï¼›åœ¨Qwen2.5-7B-Instructçš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œå¯¹æ¯”å®éªŒï¼Œè¯„ä¼°è®­ç»ƒç¨³å®šæ€§ä¸æœ€ç»ˆè¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šç›¸è¾ƒç¡¬è£å‰ªï¼Œåˆé€‚çš„å¹³æ»‘é—¨æ§å¯å¸¦æ¥æ›´ç¨³å®šçš„ç­–ç•¥æ›´æ–°ï¼Œå¹¶åœ¨ä¸ç‰ºç‰²ç”šè‡³æå‡æ€§èƒ½çš„æƒ…å†µä¸‹æ”¹è¿›è®­ç»ƒé²æ£’æ€§ï¼›è®ºæ–‡ç»™å‡ºé—¨æ§å‡½æ•°è®¾è®¡çš„ç»éªŒæ€§æŒ‡å¯¼ã€‚

**å…³é”®è¯**ï¼šç­–ç•¥ä¼˜åŒ–ç›®æ ‡, è½¯é—¨æ§å‡½æ•°, ç¡¬å‰ªåˆ‡æ›¿ä»£, è®­ç»ƒç¨³å®šæ€§, ä¼˜åŠ¿å‡½æ•°ä¼°è®¡, å¤§è¯­è¨€æ¨¡å‹å¯¹é½è®­ç»ƒ, æ•°å­¦æ¨ç†è¯„æµ‹, é²æ£’æ›´æ–°

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.19345v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.19345v1.pdf)

---

