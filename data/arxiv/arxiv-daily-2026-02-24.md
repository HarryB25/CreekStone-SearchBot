# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-24

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.LG](#csLG) (8 ç¯‡)
- [cs.CV](#csCV) (7 ç¯‡)

---

## cs.CV

## [1. Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking](https://arxiv.org/abs/2602.21435v1)

**ä½œè€…**ï¼šShengqiong Wu, Bobo Li, Xinkai Wang ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºâ€œäº¤é”™åˆ†æ-èµ·è‰å¾ªç¯â€(AD-Loop)æ€ç»´èŒƒå¼ï¼Œè®©ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£ä¸ç”Ÿæˆä¹‹é—´åŠ¨æ€äº¤æ›¿æ¨ç†ï¼Œä»è€Œå®ç°æ›´å¼ºçš„ååŒè§£é¢˜èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰UVLMå¤šåœç•™åœ¨ç»“æ„ç»Ÿä¸€å±‚é¢ï¼Œç†è§£ä¸ç”Ÿæˆåœ¨è§£é¢˜æ—¶ç¼ºå°‘æ˜¾å¼äº’åŠ¨ï¼Œå¯¼è‡´ä¸¤ç§èƒ½åŠ›æ›´åƒå¹¶è¡ŒæŠ€èƒ½è€Œéç›¸äº’ä¿ƒè¿›çš„è¿‡ç¨‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®¾è®¡AD-Loopï¼Œåœ¨æ–‡æœ¬ä¸â€œè§†è§‰æ€è€ƒâ€ä¹‹é—´äº¤é”™è¿›è¡Œåˆ†æä¸è‰æ‹Ÿã€è¿­ä»£ä¿®æ­£ç†è§£ä¸è¾“å‡ºï¼›è®­ç»ƒä¸Šé‡‡ç”¨ä¸¤é˜¶æ®µï¼šå…ˆç”¨äº¤é”™æ€ç»´æ•°æ®åšç›‘ç£å­¦ä¹ åˆå§‹åŒ–äº¤æ›¿æ¨¡å¼ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ ä¿ƒä½¿æ¨¡å‹è‡ªé€‚åº”åœ°æ§åˆ¶äº¤æ›¿ä¸ç­–ç•¥ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šé¡¹ç†è§£ä¸ç”ŸæˆåŸºå‡†ä¸Šï¼ŒAD-Loopç¨³å®šæå‡æ€§èƒ½ï¼Œå¹¶å¯è¿ç§»åˆ°ä¸åŒUVLMæ¶æ„ï¼›å¯è§†åŒ–åˆ†æè¡¨æ˜éšå¼è§†è§‰æ€è€ƒæœ‰æ•ˆæ”¯æ’‘äº†è¿™ç§ç†è§£-ç”Ÿæˆçš„ååŒä¼˜åŒ–ã€‚

**å…³é”®è¯**ï¼šç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹, äº¤é”™åˆ†æ-èµ·è‰å¾ªç¯, äº¤é”™æ€ç»´é“¾, æ–‡æœ¬-è§†è§‰æ€ç»´èåˆ, ä¸¤é˜¶æ®µè®­ç»ƒ, ç›‘ç£å¾®è°ƒ, å¼ºåŒ–å­¦ä¹ , éšå¼è§†è§‰æ€ç»´, è·¨æ¶æ„è¿ç§»

**è¯„åˆ†**ï¼š37

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21435v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21435v1.pdf)

---

## [2. PSF-Med: Measuring and Explaining Paraphrase Sensitivity in Medical Vision Language Models](https://arxiv.org/abs/2602.21428v1)

**ä½œè€…**ï¼šBinesh Sadanandan, Vahid Behzadan  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Medical Vision Language Models (VLMs) can change their answers when clinicians rephrase the same question, which raises deployment risks. We introduce Paraphrase Sensitivity Failure (PSF)-Med, a benchmark of 19,748 chest Xray questions paired with about 92,000 meaningpreserving paraphrases across MIMIC-CXR and PadChest. Across six medical VLMs, we measure yes/no flips for the same image and find flip rates from 8% to 58%. However, low flip rate does not imply visual grounding: text-only baselines show that some models stay consistent even when the image is removed, suggesting they rely on language priors. To study mechanisms in one model, we apply GemmaScope 2 Sparse Autoencoders (SAEs) to MedGemma 4B and analyze FlipBank, a curated set of 158 flip cases. We identify a sparse feature at layer 17 that correlates with prompt framing and predicts decision margin shifts. In causal patching, removing this feature's contribution recovers 45% of the yesminus-no logit margin on average and fully reverses 15% of flips. Acting on this finding, we show that clamping the identified feature at inference reduces flip rates by 31% relative with only a 1.3 percentage-point accuracy cost, while also decreasing text-prior reliance. These results suggest that flip rate alone is not enough; robustness evaluations should test both paraphrase stability and image reliance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPSF-Med ç³»ç»Ÿè¯„æµ‹åŒ»å­¦VLMå¯¹åŒä¹‰æ”¹å†™é—®é¢˜çš„æ•æ„Ÿæ€§ï¼Œå‘ç°å¤§é‡â€œç­”æ¡ˆç¿»è½¬â€ä¸”éƒ¨åˆ†ç¨³å®šæ€§æ¥è‡ªè¯­è¨€å…ˆéªŒè€Œéå›¾åƒç†è§£ï¼Œå¹¶æå‡ºåŸºäºç¨€ç–ç‰¹å¾å¹²é¢„çš„æ–¹æ³•æ˜¾è‘—é™ä½ç¿»è½¬ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¸´åºŠä¸­åŒä¸€é—®é¢˜çš„ä¸åŒè¡¨è¿°å¯èƒ½å¯¼è‡´åŒ»å­¦VLMè¾“å‡ºæ”¹å˜ï¼Œå¸¦æ¥éƒ¨ç½²é£é™©ï¼›åŒæ—¶ä»…çœ‹ä¸€è‡´æ€§æ— æ³•åˆ¤æ–­æ¨¡å‹æ˜¯å¦çœŸæ­£ä¾èµ–å½±åƒè¯æ®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºåŒ…å«19,748ä¸ªèƒ¸ç‰‡é—®é¢˜ä¸çº¦92,000ä¸ªè¯­ä¹‰ç­‰ä»·æ”¹å†™çš„PSF-MedåŸºå‡†ï¼Œåœ¨6ä¸ªåŒ»å­¦VLMä¸Šæµ‹é‡åŒå›¾åŒä¹‰é—®å¥çš„yes/noç¿»è½¬ç‡å¹¶ç”¨text-onlyåŸºçº¿æ£€éªŒâ€œå»å›¾åƒâ€ä¾èµ–ï¼›è¿›ä¸€æ­¥å¯¹MedGemma 4Bç”¨GemmaScope 2 SAEåˆ†æFlipBankç¿»è½¬æ ·ä¾‹ï¼Œå®šä½ä¸æç¤ºæ¡†æ¶ç›¸å…³çš„ç¨€ç–ç‰¹å¾å¹¶é€šè¿‡å› æœpatchingä¸æ¨ç†æ—¶clampè¿›è¡Œå¹²é¢„ã€‚

**ä¸»è¦ç»“è®º**ï¼šä¸åŒæ¨¡å‹ç¿»è½¬ç‡è¾¾8%~58%ï¼Œä¸”ä½ç¿»è½¬ç‡ä¸ä»£è¡¨è‰¯å¥½è§†è§‰å¯¹é½ï¼ˆå¯èƒ½åªæ˜¯è¯­è¨€å…ˆéªŒå¯¼è‡´çš„ç¨³å®šï¼‰ï¼›å¯¹å•ä¸ªå…³é”®ç¨€ç–ç‰¹å¾è¿›è¡Œå¹²é¢„å¯æ¢å¤logit marginå¹¶ä½¿ç¿»è½¬ç‡ç›¸å¯¹ä¸‹é™31%ï¼Œä»…å¸¦æ¥çº¦1.3ä¸ªç™¾åˆ†ç‚¹ç²¾åº¦æŸå¤±ï¼ŒåŒæ—¶é™ä½å¯¹æ–‡æœ¬å…ˆéªŒçš„ä¾èµ–ã€‚

**å…³é”®è¯**ï¼šåŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹, èƒ¸éƒ¨Xå…‰é—®ç­”, é‡Šä¹‰é²æ£’æ€§, æç¤ºæ”¹å†™æ•æ„Ÿæ€§, è§†è§‰ä¾èµ–è¯„æµ‹, è¯­è¨€å…ˆéªŒåç½®, åŸºå‡†æ•°æ®é›†, å›ç­”ç¿»è½¬ç‡, ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰, æ¨ç†æ—¶ç‰¹å¾é’³åˆ¶

**è¯„åˆ†**ï¼š39

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21428v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21428v1.pdf)

---

## [3. Automating Timed Up and Go Phase Segmentation and Gait Analysis via the tugturn Markerless 3D Pipeline](https://arxiv.org/abs/2602.21425v1)

**ä½œè€…**ï¼šAbel GonÃ§alves Chinaglia, Guilherme Manna Cesar, Paulo Roberto Pereira Santiago  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Instrumented Timed Up and Go (TUG) analysis can support clinical and research decision-making, but robust and reproducible markerless pipelines are still limited. We present \textit{tugturn.py}, a Python-based workflow for 3D markerless TUG processing that combines phase segmentation, gait-event detection, spatiotemporal metrics, intersegmental coordination, and dynamic stability analysis. The pipeline uses spatial thresholds to segment each trial into stand, first gait, turning, second gait, and sit phases, and applies a relative-distance strategy to detect heel-strike and toe-off events within valid gait windows. In addition to conventional kinematics, \textit{tugturn} provides Vector Coding outputs and Extrapolated Center of Mass (XCoM)-based metrics. The software is configured through TOML files and produces reproducible artifacts, including HTML reports, CSV tables, and quality-assurance visual outputs. A complete runnable example is provided with test data and command-line instructions. This manuscript describes the implementation, outputs, and reproducibility workflow of \textit{tugturn} as a focused software contribution for markerless biomechanical TUG analysis.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºå¹¶å¼€æºäº†ä¸€ä¸ªåä¸º tugturn.py çš„æ— æ ‡è®°3D TUGï¼ˆTimed Up and Goï¼‰åˆ†ææµæ°´çº¿ï¼Œå¯è‡ªåŠ¨å®Œæˆåˆ†æœŸã€æ­¥æ€äº‹ä»¶æ£€æµ‹ä¸å¤šç§ç”Ÿç‰©åŠ›å­¦æŒ‡æ ‡è¾“å‡ºï¼Œå¹¶å¼ºè°ƒå¯å¤ç°çš„æŠ¥å‘ŠåŒ–äº§å‡ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šTUG åœ¨ä¸´åºŠä¸ç ”ç©¶ä¸­å¸¸ç”¨ï¼Œä½†ç°æœ‰æ— æ ‡è®°ï¼ˆmarkerlessï¼‰å¤„ç†æµç¨‹åœ¨é²æ£’æ€§ã€æ ‡å‡†åŒ–ä¸å¯å¤ç°æ€§æ–¹é¢ä»ä¸è¶³ï¼Œé™åˆ¶äº†è·¨å®éªŒ/è·¨åœºæ™¯å¯¹æ¯”ä¸è½åœ°åº”ç”¨ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡ç©ºé—´é˜ˆå€¼å°†ä¸€æ¬¡TUGè¯•æ¬¡åˆ†å‰²ä¸ºèµ·ç«‹ã€ç¬¬ä¸€æ®µæ­¥è¡Œã€è½¬èº«ã€ç¬¬äºŒæ®µæ­¥è¡Œã€åä¸‹äº”ä¸ªé˜¶æ®µï¼Œå¹¶åœ¨æœ‰æ•ˆæ­¥è¡Œçª—å£å†…ç”¨ç›¸å¯¹è·ç¦»ç­–ç•¥æ£€æµ‹ heel-strike/toe-offï¼›åŒæ—¶è®¡ç®—æ—¶ç©ºå‚æ•°ã€çŸ¢é‡ç¼–ç ï¼ˆVector Codingï¼‰ååŒæŒ‡æ ‡ä¸åŸºäº XCoM çš„åŠ¨æ€ç¨³å®šæ€§æŒ‡æ ‡ï¼Œå¹¶ç”¨TOMLé…ç½®ä¸è‡ªåŠ¨ç”ŸæˆHTML/CSV/è´¨æ£€å¯è§†åŒ–ç¡®ä¿å¤ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼štugturn æä¾›äº†ä¸€ä¸ªè¦†ç›–åˆ†æœŸã€äº‹ä»¶æ£€æµ‹ã€æŒ‡æ ‡è®¡ç®—ä¸å¯å¤ç°æŠ¥å‘Šçš„ç«¯åˆ°ç«¯æ— æ ‡è®°TUGè½¯ä»¶å®ç°ï¼Œé™ä½äº†è§„èŒƒåŒ–ç”Ÿç‰©åŠ›å­¦TUGåˆ†æçš„é—¨æ§›å¹¶ä¾¿äºå¤ç”¨ä¸å®¡è®¡ã€‚

**å…³é”®è¯**ï¼šæ— æ ‡è®°åŠ¨ä½œæ•æ‰, ä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡, é˜¶æ®µåˆ†å‰², æ­¥æ€äº‹ä»¶æ£€æµ‹, æ—¶ç©ºæ­¥æ€æŒ‡æ ‡, å…³èŠ‚é—´åè°ƒ, åŠ¨æ€ç¨³å®šæ€§, å¤–æ¨è´¨å¿ƒï¼ˆXCoMï¼‰, å¯å¤ç°ç”Ÿç‰©åŠ›å­¦æµæ°´çº¿

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21425v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21425v1.pdf)

---

## [4. ECHOSAT: Estimating Canopy Height Over Space And Time](https://arxiv.org/abs/2602.21421v1)

**ä½œè€…**ï¼šJan Pauls, Karsten SchrÃ¶dter, Sven Ligensa ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Forest monitoring is critical for climate change mitigation. However, existing global tree height maps provide only static snapshots and do not capture temporal forest dynamics, which are essential for accurate carbon accounting. We introduce ECHOSAT, a global and temporally consistent tree height map at 10 m resolution spanning multiple years. To this end, we resort to multi-sensor satellite data to train a specialized vision transformer model, which performs pixel-level temporal regression. A self-supervised growth loss regularizes the predictions to follow growth curves that are in line with natural tree development, including gradual height increases over time, but also abrupt declines due to forest loss events such as fires. Our experimental evaluation shows that our model improves state-of-the-art accuracies in the context of single-year predictions. We also provide the first global-scale height map that accurately quantifies tree growth and disturbances over time. We expect ECHOSAT to advance global efforts in carbon monitoring and disturbance assessment. The maps can be accessed at https://github.com/ai4forest/echosat.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šECHOSAT åˆ©ç”¨å¤šä¼ æ„Ÿå™¨å«æ˜Ÿæ•°æ®ä¸ä¸“ç”¨è§†è§‰Transformerï¼Œç”Ÿæˆå…¨çƒ10måˆ†è¾¨ç‡ã€è·¨å¤šå¹´çš„æ—¶åºä¸€è‡´æ ‘é«˜åœ°å›¾ï¼Œèƒ½åŒæ—¶åˆ»ç”»ç”Ÿé•¿ä¸æ‰°åŠ¨ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å…¨çƒæ ‘é«˜äº§å“å¤šä¸ºå•å¹´ä»½é™æ€å¿«ç…§ï¼Œæ— æ³•åæ˜ æ£®æ—éšæ—¶é—´çš„ç”Ÿé•¿ä¸ç«ç¾/ç ä¼ç­‰çªå‘ä¸‹é™ï¼Œä»è€Œé™åˆ¶ç¢³æ ¸ç®—ä¸æ‰°åŠ¨è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šèåˆå¤šæºå«æ˜Ÿè§‚æµ‹è®­ç»ƒä¸€ä¸ªè¿›è¡Œåƒç´ çº§æ—¶é—´å›å½’çš„Vision Transformerï¼Œå¹¶å¼•å…¥è‡ªç›‘ç£â€œç”Ÿé•¿æŸå¤±â€çº¦æŸé¢„æµ‹ç¬¦åˆè‡ªç„¶ç”Ÿé•¿æ›²çº¿ï¼ˆç¼“æ…¢å¢é«˜ï¼‰ä¸”å…è®¸æ£®æ—æŸå¤±äº‹ä»¶å¯¼è‡´çš„çªé™ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å•å¹´ä»½æ ‘é«˜é¢„æµ‹ç²¾åº¦ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶é¦–æ¬¡æä¾›èƒ½å¤Ÿåœ¨å…¨çƒå°ºåº¦ä¸Šå¯é é‡åŒ–æ ‘é«˜å¢é•¿ä¸æ‰°åŠ¨çš„å¤šå¹´åºåˆ—åœ°å›¾ï¼Œæ”¯æŒç¢³ç›‘æµ‹ä¸ç¾å®³/æ‰°åŠ¨è¯„ä¼°ã€‚

**å…³é”®è¯**ï¼šæ—å† é«˜åº¦ä¼°è®¡, å…¨çƒæ ‘é«˜åˆ¶å›¾, æ—¶åºæ£®æ—åŠ¨æ€, å¤šä¼ æ„Ÿå™¨é¥æ„Ÿèåˆ, 10ç±³åˆ†è¾¨ç‡, åƒç´ çº§æ—¶é—´å›å½’, è‡ªç›‘ç£å­¦ä¹ æŸå¤±, æ ‘æœ¨ç”Ÿé•¿æ›²çº¿çº¦æŸ, æ£®æ—æ‰°åŠ¨æ£€æµ‹, ç«ç¾è‡´æŸç›‘æµ‹, ç¢³æ ¸ç®—ç›‘æµ‹

**è¯„åˆ†**ï¼š37

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21421v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21421v1.pdf)

---

## [5. WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions](https://arxiv.org/abs/2602.21416v1)

**ä½œè€…**ï¼šMarco Terral, Haotian Zhang, Tianyang Zhang ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºé¢å‘çœŸå®åœºæ™¯çš„SVGæå–ä»»åŠ¡ä¸WildSVGåŸºå‡†ï¼Œç³»ç»Ÿè¯„æµ‹å‘ç°ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒå™ªå£°ä¸‹çš„SVGç”Ÿæˆä»æ˜æ˜¾ä¸å¯é ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ¨¡å‹åœ¨å¹²å‡€æ¸²æŸ“æˆ–çº¯æ–‡æœ¬æ¡ä»¶ä¸‹èƒ½ç”ŸæˆSVGï¼Œä½†åœ¨çœŸå®ç…§ç‰‡ä¸­å—å™ªå£°ã€é®æŒ¡ä¸åŸŸåç§»å½±å“è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼›åŒæ—¶ç¼ºä¹èƒ½åæ˜ çœŸå®å›°éš¾çš„ç»Ÿä¸€è¯„æµ‹åŸºå‡†ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºWildSVG Benchmarkï¼šåŒ…å«çœŸå®å›¾åƒå…¬å¸LogoåŠå…¶SVGæ ‡æ³¨çš„Natural WildSVGï¼Œä»¥åŠå°†å¤æ‚SVGæ¸²æŸ“åˆæˆåˆ°çœŸå®åœºæ™¯ä»¥æ¨¡æ‹Ÿå›°éš¾æ¡ä»¶çš„Synthetic WildSVGï¼›åœ¨è¯¥åŸºå‡†ä¸Šå¯¹å¤šç§SOTAå¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå¯¹æ¯”è¯„æµ‹ï¼Œå¹¶æ¢ç´¢è¿­ä»£å¼ç²¾ç‚¼ç”Ÿæˆç­–ç•¥ã€‚

**ä¸»è¦ç»“è®º**ï¼šåŸºå‡†æµ‹è¯•æ˜¾ç¤ºå½“å‰æ–¹æ³•è·ç¦»çœŸå®å¯ç”¨çš„å¯é SVGæå–ä»æœ‰å·®è·ï¼Œä½†è¿­ä»£ç²¾ç‚¼ç­‰ç­–ç•¥å±•ç°å‡ºæ”¹è¿›æ½œåŠ›ï¼Œä¸”æ¨¡å‹èƒ½åŠ›æ•´ä½“å‘ˆæŒç»­æå‡è¶‹åŠ¿ã€‚

**å…³é”®è¯**ï¼šå›¾åƒåˆ°çŸ¢é‡å›¾, çœŸå®åœºæ™¯é²æ£’æ€§, å™ªå£°ä¸é®æŒ¡, åŸŸåç§», å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹, åŸºå‡†æµ‹è¯•æ•°æ®é›†, åˆæˆæ•°æ®ç”Ÿæˆ, è¿­ä»£å¼ç²¾ç‚¼

**è¯„åˆ†**ï¼š32

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21416v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21416v1.pdf)

---

## [6. Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation](https://arxiv.org/abs/2602.21406v1)

**ä½œè€…**ï¼šAsim Unmesh, Kaki Ramesh, Mayank Patel ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºå¹¶ç³»ç»Ÿè¯„ä¼°ä¸€ç§åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„å¼€æ”¾è¯è¡¨é›¶æ ·æœ¬æ—¶åºåŠ¨ä½œåˆ†å‰²ï¼ˆOVTASï¼‰è®­ç»ƒå…æ–¹æ¡ˆï¼Œåœ¨æ— éœ€ä»»åŠ¡ç›‘ç£ä¸‹å–å¾—æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿæ—¶åºåŠ¨ä½œåˆ†å‰²ä¾èµ–å°é—­æ ‡ç­¾ä¸æ ‡æ³¨æ•°æ®ï¼Œä½†ç°å®åŠ¨ä½œç±»åˆ«ç©ºé—´å·¨å¤§ä¸”åˆ†è§£æ–¹å¼å¤šæ ·ï¼Œéš¾ä»¥è¦†ç›–ï¼›å› æ­¤éœ€è¦èƒ½åœ¨å¼€æ”¾è¯è¡¨ã€é›¶æ ·æœ¬æ¡ä»¶ä¸‹æ³›åŒ–çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé‡‡ç”¨â€œå…ˆåˆ†ç±»ååˆ†å‰²â€çš„è®­ç»ƒå…æµæ°´çº¿ï¼šç”¨FAESè®¡ç®—å¸§ç‰¹å¾ä¸å€™é€‰åŠ¨ä½œæ–‡æœ¬åµŒå…¥çš„ç›¸ä¼¼åº¦å®Œæˆå¸§çº§åŒ¹é…ï¼Œå†ç”¨SMTSåŸºäºç›¸ä¼¼åº¦çŸ©é˜µæ–½åŠ æ—¶é—´ä¸€è‡´æ€§ä»¥å¾—åˆ°ç¨³å®šçš„åŠ¨ä½œæ®µè¾¹ç•Œï¼›å¹¶åœ¨14ä¸ªVLMä¸Šåšç³»ç»Ÿå¯¹æ¯”åˆ†æã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜è¯¥OVTASåœ¨æ ‡å‡†åŸºå‡†ä¸Šæ— éœ€é¢å¤–è®­ç»ƒå³å¯è·å¾—å¼ºç»“æœï¼Œä¸åŒVLMå¯¹æ—¶åºåˆ†å‰²é€‚é…æ€§å­˜åœ¨å·®å¼‚ï¼Œæ•´ä½“éªŒè¯äº†VLMåœ¨å¼€æ”¾è¯è¡¨ç»“æ„åŒ–æ—¶åºç†è§£ä¸­çš„æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šæ—¶é—´åŠ¨ä½œåˆ†å‰², å¼€æ”¾è¯è¡¨, é›¶æ ·æœ¬å­¦ä¹ , è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰, åˆ†å‰²-åˆ†ç±»èŒƒå¼, å¸§-åŠ¨ä½œåµŒå…¥ç›¸ä¼¼åº¦ï¼ˆFAESï¼‰, ç›¸ä¼¼åº¦çŸ©é˜µæ—¶é—´åˆ†å‰²ï¼ˆSMTSï¼‰, æ—¶é—´ä¸€è‡´æ€§çº¦æŸ, å¼€æ”¾è¯è¡¨åŠ¨ä½œåˆ†å‰²è¯„æµ‹

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21406v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21406v1.pdf)

---

## [7. FlowFixer: Towards Detail-Preserving Subject-Driven Generation](https://arxiv.org/abs/2602.21402v1)

**ä½œè€…**ï¼šJinyoung Jun, Won-Dong Jang, Wenbin Ouyang ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present FlowFixer, a refinement framework for subject-driven generation (SDG) that restores fine details lost during generation caused by changes in scale and perspective of a subject. FlowFixer proposes direct image-to-image translation from visual references, avoiding ambiguities in language prompts. To enable image-to-image training, we introduce a one-step denoising scheme to generate self-supervised training data, which automatically removes high-frequency details while preserving global structure, effectively simulating real-world SDG errors. We further propose a keypoint matching-based metric to properly assess fidelity in details beyond semantic similarities usually measured by CLIP or DINO. Experimental results demonstrate that FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFlowFixer æå‡ºä¸€ç§é¢å‘ä¸»ä½“é©±åŠ¨ç”Ÿæˆçš„ç»†èŠ‚ä¿®å¤æ¡†æ¶ï¼Œé€šè¿‡å‚è€ƒå›¾åƒçš„ç«¯åˆ°ç«¯ç¿»è¯‘æ¢å¤å› å°ºåº¦/è§†è§’å˜åŒ–è€Œä¸¢å¤±çš„ä¸»ä½“é«˜é¢‘ç»†èŠ‚ï¼Œå¹¶å¼•å…¥æ›´åˆé€‚çš„ç»†èŠ‚ä¿çœŸè¯„æµ‹æŒ‡æ ‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ä¸»ä½“é©±åŠ¨ç”Ÿæˆåœ¨ä¸»ä½“å‘ç”Ÿå°ºåº¦ä¸è§†è§’å˜åŒ–æ—¶å®¹æ˜“ä¸¢å¤±çº¹ç†ç­‰ç»†èŠ‚ï¼Œè€Œä¾èµ–æ–‡æœ¬æç¤ºä¼šå¸¦æ¥æè¿°æ­§ä¹‰ï¼Œä¸”å¸¸ç”¨ CLIP/DINO ç›¸ä¼¼åº¦éš¾ä»¥çœŸå®åæ˜ ç»†èŠ‚ä¿çœŸåº¦ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé‡‡ç”¨â€œå‚è€ƒå›¾åƒâ†’ç”Ÿæˆç»“æœâ€çš„ç›´æ¥å›¾åƒåˆ°å›¾åƒç²¾ä¿®ä»¥é¿å…è¯­è¨€æ­§ä¹‰ï¼›æå‡ºä¸€æ­¥å»å™ªçš„è‡ªç›‘ç£æ•°æ®åˆæˆæ–¹æ¡ˆï¼Œè‡ªåŠ¨æŠ¹å»é«˜é¢‘ç»†èŠ‚ä½†ä¿ç•™å…¨å±€ç»“æ„ä»¥æ¨¡æ‹ŸçœŸå® SDG è¯¯å·®ï¼›å¹¶è®¾è®¡åŸºäºå…³é”®ç‚¹åŒ¹é…çš„ç»†èŠ‚ä¿çœŸåº¦é‡ç”¨äºè¯„ä¼°ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å®šæ€§ä¸å®šé‡å®éªŒä¸­ï¼ŒFlowFixer ç›¸æ¯”ç°æœ‰ SOTA ä¸»ä½“é©±åŠ¨ç”Ÿæˆæ–¹æ³•èƒ½æ›´å¥½æ¢å¤ä¸»ä½“ç»†èŠ‚å¹¶å–å¾—æ›´é«˜çš„ç»†èŠ‚ä¿çœŸæŒ‡æ ‡è¡¨ç°ï¼Œä»è€Œæ ‘ç«‹é«˜ä¿çœŸ SDG çš„æ–°åŸºå‡†ã€‚

**å…³é”®è¯**ï¼šä¸»ä½“é©±åŠ¨ç”Ÿæˆ, ç»†èŠ‚ä¿æŒç”Ÿæˆ, å›¾åƒåˆ°å›¾åƒç¿»è¯‘, è§†è§‰å‚è€ƒæ¡ä»¶ç”Ÿæˆ, è‡ªç›‘ç£æ•°æ®ç”Ÿæˆ, ä¸€æ­¥å»å™ª, é«˜é¢‘ç»†èŠ‚æ¢å¤, å°ºåº¦ä¸è§†è§’é²æ£’æ€§, å…³é”®ç‚¹åŒ¹é…æŒ‡æ ‡, ç»†èŠ‚ä¿çœŸè¯„æµ‹, CLIP/DINOè¯­ä¹‰ç›¸ä¼¼åº¦å±€é™

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21402v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21402v1.pdf)

---

## cs.LG

## [8. MINAR: Mechanistic Interpretability for Neural Algorithmic Reasoning](https://arxiv.org/abs/2602.21442v1)

**ä½œè€…**ï¼šJesse He, Helen Jenne, Max Vargas ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The recent field of neural algorithmic reasoning (NAR) studies the ability of graph neural networks (GNNs) to emulate classical algorithms like Bellman-Ford, a phenomenon known as algorithmic alignment. At the same time, recent advances in large language models (LLMs) have spawned the study of mechanistic interpretability, which aims to identify granular model components like circuits that perform specific computations. In this work, we introduce Mechanistic Interpretability for Neural Algorithmic Reasoning (MINAR), an efficient circuit discovery toolbox that adapts attribution patching methods from mechanistic interpretability to the GNN setting. We show through two case studies that MINAR recovers faithful neuron-level circuits from GNNs trained on algorithmic tasks. Our study sheds new light on the process of circuit formation and pruning during training, as well as giving new insight into how GNNs trained to perform multiple tasks in parallel reuse circuit components for related tasks. Our code is available at https://github.com/pnnl/MINAR.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMINARå°†æœºåˆ¶å¯è§£é‡Šæ€§ä¸­çš„å½’å› è¡¥ä¸ï¼ˆattribution patchingï¼‰æ–¹æ³•è¿ç§»åˆ°GNNçš„ç¥ç»ç®—æ³•æ¨ç†ä¸­ï¼Œé«˜æ•ˆå‘ç°èƒ½å®ç°ç®—æ³•è®¡ç®—çš„ç¥ç»å…ƒçº§ç”µè·¯ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç¥ç»ç®—æ³•æ¨ç†ç ”ç©¶è¡¨æ˜GNNèƒ½å¯¹é½ç»å…¸ç®—æ³•ï¼Œä½†å…¶å†…éƒ¨å¦‚ä½•å½¢æˆâ€œç®—æ³•ç”µè·¯â€ä»ä¸æ¸…æ™°ï¼›æœºåˆ¶å¯è§£é‡Šæ€§æä¾›äº†å®šä½å…·ä½“è®¡ç®—å›è·¯çš„æ€è·¯ï¼Œä½†ç¼ºå°‘é¢å‘GNN/NARçš„å®ç”¨å·¥å…·ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºMINARç”µè·¯å‘ç°å·¥å…·ç®±ï¼Œå°†å½’å› è¡¥ä¸ç­‰ç”µè·¯å®šä½æŠ€æœ¯é€‚é…åˆ°GNNç»“æ„ä¸Šï¼Œå¹¶åœ¨ç®—æ³•ä»»åŠ¡è®­ç»ƒçš„GNNä¸­è¿›è¡Œç¥ç»å…ƒçº§ç”µè·¯æå–ä¸éªŒè¯ï¼›é€šè¿‡ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç”µè·¯å½¢æˆ/å‰ªæï¼Œä»¥åŠå¤šä»»åŠ¡å¹¶è¡Œè®­ç»ƒæ—¶ç”µè·¯ç»„ä»¶çš„å¤ç”¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šMINARèƒ½å¤Ÿä»NAR-GNNä¸­æ¢å¤â€œå¿ å®â€çš„ç¥ç»å…ƒçº§ç”µè·¯ï¼Œæ­ç¤ºç”µè·¯åœ¨è®­ç»ƒä¸­é€æ­¥å½¢æˆå¹¶è¢«å‰ªæçš„åŠ¨æ€è¿‡ç¨‹ï¼›åŒæ—¶å‘ç°å¤šä»»åŠ¡è®¾ç½®ä¸‹ç›¸å…³ä»»åŠ¡ä¼šå…±äº«ä¸å¤ç”¨éƒ¨åˆ†ç”µè·¯ç»„ä»¶ï¼Œä»è€Œæä¾›å¯¹ç®—æ³•å¯¹é½æœºç†çš„æ›´ç»†ç²’åº¦è§£é‡Šã€‚

**å…³é”®è¯**ï¼šç¥ç»ç®—æ³•æ¨ç†, å›¾ç¥ç»ç½‘ç»œ, ç®—æ³•å¯¹é½, æœºç†å¯è§£é‡Šæ€§, å½’å› è¡¥ä¸, ç¥ç»å…ƒçº§ç”µè·¯, ç»å…¸ç®—æ³•æ¨¡æ‹Ÿ, è®­ç»ƒå‰ªæ

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21442v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21442v1.pdf)

---

## [9. Provably Safe Generative Sampling with Constricting Barrier Functions](https://arxiv.org/abs/2602.21429v1)

**ä½œè€…**ï¼šDarshan Gadginmath, Ahmed Allibhoy, Fabio Pasqualetti  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, eess.SY, math.OC  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Flow-based generative models, such as diffusion models and flow matching models, have achieved remarkable success in learning complex data distributions. However, a critical gap remains for their deployment in safety-critical domains: the lack of formal guarantees that generated samples will satisfy hard constraints. We address this by proposing a safety filtering framework that acts as an online shield for any pre-trained generative model. Our key insight is to cooperate with the generative process rather than override it. We define a constricting safety tube that is relaxed at the initial noise distribution and progressively tightens to the target safe set at the final data distribution, mirroring the coarse-to-fine structure of the generative process itself. By characterizing this tube via Control Barrier Functions (CBFs), we synthesize a feedback control input through a convex Quadratic Program (QP) at each sampling step. As the tube is loosest when noise is high and intervention is cheapest in terms of control energy, most constraint enforcement occurs when it least disrupts the model's learned structure. We prove that this mechanism guarantees safe sampling while minimizing the distributional shift from the original model at each sampling step, as quantified by the KL divergence. Our framework applies to any pre-trained flow-based generative scheme requiring no retraining or architectural modifications. We validate the approach across constrained image generation, physically-consistent trajectory sampling, and safe robotic manipulation policies, achieving 100% constraint satisfaction while preserving semantic fidelity.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§æ— éœ€é‡è®­ç»ƒçš„â€œå®‰å…¨è¿‡æ»¤/æŠ¤ç›¾â€æ¡†æ¶ï¼Œç”¨æ§åˆ¶å±éšœå‡½æ•°åœ¨ç”Ÿæˆé‡‡æ ·è¿‡ç¨‹ä¸­é€æ­¥æ”¶ç´§çº¦æŸï¼Œç†è®ºä¸Šä¿è¯ç”Ÿæˆæ ·æœ¬æ»¡è¶³ç¡¬çº¦æŸä¸”å°½é‡ä¸åç¦»åŸæ¨¡å‹åˆ†å¸ƒã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ‰©æ•£/æµå¼ç”Ÿæˆæ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œä½†åœ¨å®‰å…¨å…³é”®åœºæ™¯ç¼ºä¹â€œç”Ÿæˆç»“æœå¿…æ»¡è¶³ç¡¬çº¦æŸâ€çš„å½¢å¼åŒ–ä¿è¯ï¼Œé™åˆ¶äº†å®é™…éƒ¨ç½²ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„é€ ä»åˆå§‹å™ªå£°åˆ°æœ€ç»ˆå®‰å…¨é›†åˆé€æ­¥æ”¶ç´§çš„â€œå®‰å…¨ç®¡é“â€ï¼Œç”¨æ§åˆ¶å±éšœå‡½æ•°ï¼ˆCBFï¼‰è¡¨å¾å¹¶åœ¨æ¯ä¸ªé‡‡æ ·æ­¥é€šè¿‡å‡¸äºŒæ¬¡è§„åˆ’ï¼ˆQPï¼‰æ±‚è§£æœ€å°å¹²é¢„çš„åé¦ˆæ§åˆ¶è¾“å…¥ï¼›åŒæ—¶è¯æ˜è¯¥å¹²é¢„åœ¨æ¯æ­¥ä»¥KLæ•£åº¦æ„ä¹‰ä¸‹æœ€å°åŒ–å¯¹åŸç”Ÿæˆåˆ†å¸ƒçš„åç§»å¹¶ä¿è¯å®‰å…¨æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ–¹æ³•å¯ä½œä¸ºä»»æ„é¢„è®­ç»ƒæµå¼ç”Ÿæˆæ¨¡å‹çš„åœ¨çº¿å®‰å…¨ç›¾ç‰Œï¼Œå®ç°100%çº¦æŸæ»¡è¶³ï¼Œå¹¶åœ¨å—é™å›¾åƒç”Ÿæˆã€ç‰©ç†ä¸€è‡´è½¨è¿¹é‡‡æ ·ä¸å®‰å…¨æœºå™¨äººæ“ä½œç­–ç•¥ç­‰ä»»åŠ¡ä¸­ä¿æŒè¾ƒé«˜è¯­ä¹‰/ä»»åŠ¡ä¿çœŸåº¦ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆæ¨¡å‹, å®‰å…¨è¿‡æ»¤, æ§åˆ¶å±éšœå‡½æ•°, çº¦æŸé‡‡æ ·, åˆ†å¸ƒè½¬ç§», å›¾åƒç”Ÿæˆ, è½¨è¿¹é‡‡æ ·, æœºå™¨äººæ“æ§

**è¯„åˆ†**ï¼š42

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21429v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21429v1.pdf)

---

## [10. Proximal-IMH: Proximal Posterior Proposals for Independent Metropolis-Hastings with Approximate Operators](https://arxiv.org/abs/2602.21426v1)

**ä½œè€…**ï¼šYouguang Chen, George Biros  
**åˆ†ç±»**ï¼šcs.LG, stat.CO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We consider the problem of sampling from a posterior distribution arising in Bayesian inverse problems in science, engineering, and imaging. Our method belongs to the family of independence Metropolis-Hastings (IMH) sampling algorithms, which are common in Bayesian inference. Relying on the existence of an approximate posterior distribution that is cheaper to sample from but may have significant bias, we introduce Proximal-IMH, a scheme that removes this bias by correcting samples from the approximate posterior through an auxiliary optimization problem. This yields a local adjustment that trades off adherence to the exact model against stability around the approximate reference point. For idealized settings, we prove that the proximal correction tightens the match between approximate and exact posteriors, thereby improving acceptance rates and mixing. The method applies to both linear and nonlinear input-output operators and is particularly suitable for inverse problems where exact posterior sampling is too expensive. We present numerical experiments including multimodal and data-driven priors with nonlinear input-output operators. The results show that Proximal-IMH reliably outperforms existing IMH variants.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šProximal-IMH é€šè¿‡å¯¹â€œä¾¿å®œä½†æœ‰åâ€çš„è¿‘ä¼¼åéªŒæ ·æœ¬åšä¸€æ¬¡è¿‘ç«¯ä¼˜åŒ–æ ¡æ­£ï¼Œåœ¨ä¿æŒç‹¬ç«‹MHæ¡†æ¶ä¸‹æ˜¾è‘—æé«˜å¯¹çœŸå®åéªŒçš„è´´åˆåº¦ä¸é‡‡æ ·æ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè´å¶æ–¯é€†é—®é¢˜çš„ç²¾ç¡®åéªŒé‡‡æ ·å¸¸å› å‰å‘ç®—å­æ˜‚è´µè€Œéš¾ä»¥è¿›è¡Œï¼Œè€Œç›´æ¥ç”¨å¯å¿«é€Ÿé‡‡æ ·çš„è¿‘ä¼¼åéªŒåˆä¼šå¼•å…¥æ˜æ˜¾åå·®å¹¶å¯¼è‡´IMHæ¥å—ç‡/æ··åˆå˜å·®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥è¿‘ä¼¼åéªŒä¸ºç‹¬ç«‹æè®®åˆ†å¸ƒï¼Œå¹¶åœ¨æ¯æ¬¡æè®®åæ±‚è§£ä¸€ä¸ªè¾…åŠ©â€œè¿‘ç«¯/æ­£åˆ™åŒ–â€çš„ä¼˜åŒ–é—®é¢˜ï¼Œå¯¹æ ·æœ¬åšå±€éƒ¨è°ƒæ•´ï¼Œåœ¨è´´è¿‘ç²¾ç¡®æ¨¡å‹ä¸å›´ç»•è¿‘ä¼¼å‚è€ƒç‚¹ä¿æŒç¨³å®šä¹‹é—´æŠ˜ä¸­ï¼›ç†è®ºä¸Šè¯æ˜è¯¥æ ¡æ­£å¯æ”¶ç´§è¿‘ä¼¼ä¸ç²¾ç¡®åéªŒçš„å·®å¼‚ï¼Œä»è€Œæ”¹å–„æ¥å—ç‡ä¸æ··åˆã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ç†æƒ³åŒ–è®¾å®šä¸‹ç»™å‡ºæ¥å—ç‡ä¸æ··åˆæ”¹å–„çš„ç†è®ºä¿è¯ï¼Œå¹¶åœ¨åŒ…å«å¤šå³°åˆ†å¸ƒã€æ•°æ®é©±åŠ¨å…ˆéªŒåŠéçº¿æ€§ç®—å­çš„æ•°å€¼å®éªŒä¸­ç¨³å®šä¼˜äºç°æœ‰IMHå˜ä½“ï¼Œé€‚ç”¨äºçº¿æ€§ä¸éçº¿æ€§é€†é—®é¢˜çš„é«˜æˆæœ¬åéªŒé‡‡æ ·ã€‚

**å…³é”®è¯**ï¼šåéªŒåˆ†å¸ƒ, è´å¶æ–¯é€†é—®é¢˜, è¿‘ä¼¼åéªŒ, ä¼˜åŒ–é—®é¢˜, æ¥å—ç‡, æ··åˆæ€§, éçº¿æ€§è¾“å…¥è¾“å‡º, æ•°å€¼å®éªŒ

**è¯„åˆ†**ï¼š17

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21426v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21426v1.pdf)

---

## [11. On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation](https://arxiv.org/abs/2602.21424v1)

**ä½œè€…**ï¼šAlexander Galozy  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect to internal information under fixed observations. This induces a probe-relative notion of $Îµ$-behavioural equivalence and a within-policy behavioural distance that quantifies probe sensitivity. We establish three structural results. First, the set of policies exhibiting non-trivial behavioural dependency is not closed under convex aggregation. Second, behavioural distance contracts under convex combination. Third, we prove a sufficient local condition under which gradient ascent on a skewed mixture objective decreases behavioural distance when a dominant-mode gradient aligns with the direction of steepest contraction. Minimal bandit and partially observable gridworld experiments provide controlled witnesses of these mechanisms. In the examined settings, behavioural distance decreases under convex aggregation and under continued optimisation with skewed latent priors, and in these experiments it precedes degradation under latent prior shift. These results identify structural conditions under which probe-conditioned behavioural separation is not preserved under common policy transformations.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šåœ¨éƒ¨åˆ†å¯è§‚æµ‹RLä¸­ï¼Œç­–ç•¥å¯¹å†…éƒ¨ä¿¡æ¯ï¼ˆè®°å¿†/æ½œå˜é‡æ¨æ–­ï¼‰çš„â€œè¡Œä¸ºä¾èµ–â€åœ¨å¸¸è§çš„ç­–ç•¥å˜æ¢ï¼ˆå‡¸ç»„åˆä¸ç‰¹å®šä¼˜åŒ–è¿‡ç¨‹ï¼‰ä¸‹å¯èƒ½ä¸è¢«ç»“æ„æ€§ä¿æŒï¼Œå¹¶å¾€å¾€è¡¨ç°ä¸ºå¯æ¢æµ‹çš„è¡Œä¸ºå·®å¼‚è¢«å‹ç¼©ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°å®ä¸­çš„POMDPæ™ºèƒ½ä½“å¸¸ä¾èµ–å†…éƒ¨çŠ¶æ€åšå†³ç­–ï¼Œä½†å®è·µé‡Œç»å¸¸å¯¹ç­–ç•¥åšé›†æˆ/æ··åˆæˆ–ç»§ç»­ä¼˜åŒ–ï¼›ä½œè€…å¸Œæœ›å¼„æ¸…è¿™äº›å˜æ¢æ˜¯å¦ä¼šç ´åï¼ˆæˆ–å‰Šå¼±ï¼‰è¿™ç§â€œåŸºäºå†…éƒ¨ä¿¡æ¯çš„å¯åŒºåˆ†è¡Œä¸ºâ€ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå½¢å¼åŒ–â€œè¡Œä¸ºä¾èµ–â€ä¸ºåœ¨å›ºå®šè§‚æµ‹ä¸‹åŠ¨ä½œåˆ†å¸ƒéšå†…éƒ¨ä¿¡æ¯å˜åŒ–ï¼Œå¹¶æ®æ­¤å®šä¹‰æ¢é’ˆç›¸å¯¹çš„Îµ-è¡Œä¸ºç­‰ä»·ä¸ç­–ç•¥å†…çš„è¡Œä¸ºè·ç¦»ï¼ˆè¡¡é‡æ¢é’ˆæ•æ„Ÿåº¦ï¼‰ï¼›éšåç»™å‡ºå…³äºå‡¸èšåˆä¸æ¢¯åº¦ä¸Šå‡ï¼ˆåç½®æ··åˆç›®æ ‡ï¼‰çš„ä¸‰ä¸ªç»“æ„æ€§å®šç†ï¼Œå¹¶ç”¨æœ€å°banditä¸éƒ¨åˆ†å¯è§‚æµ‹gridworldåšå¯¹ç…§å®éªŒéªŒè¯æœºåˆ¶ã€‚

**ä¸»è¦ç»“è®º**ï¼š(1) å…·æœ‰éå¹³å‡¡è¡Œä¸ºä¾èµ–çš„ç­–ç•¥é›†åˆå¯¹å‡¸èšåˆä¸å°é—­ï¼›(2) è¡Œä¸ºè·ç¦»åœ¨å‡¸ç»„åˆä¸‹æ”¶ç¼©ï¼›(3) åœ¨æ»¡è¶³å±€éƒ¨æ¡ä»¶æ—¶ï¼Œé’ˆå¯¹åç½®æ··åˆç›®æ ‡çš„æ¢¯åº¦ä¸Šå‡ä¼šè¿›ä¸€æ­¥é™ä½è¡Œä¸ºè·ç¦»ï¼›å®éªŒä¸­è§‚å¯Ÿåˆ°è¡Œä¸ºè·ç¦»éšèšåˆ/ç»§ç»­ä¼˜åŒ–ä¸‹é™ï¼Œä¸”å…¶ä¸‹é™å¾€å¾€å…ˆäºæ½œåœ¨å…ˆéªŒç§»ä½ä¸‹çš„æ€§èƒ½é€€åŒ–ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒ, ç­–ç•¥å˜æ¢, è¡Œä¸ºä¾èµ–, è¡Œä¸ºç­‰ä»·ï¼ˆÎµï¼‰, è¡Œä¸ºè·ç¦», æ¢é’ˆæ•æ„Ÿæ€§, å‡¸ç»„åˆç­–ç•¥, æ¢¯åº¦ä¸Šå‡, æ··åˆç›®æ ‡, æ½œåœ¨å…ˆéªŒåç§», éƒ¨åˆ†å¯è§‚æµ‹ç½‘æ ¼ä¸–ç•Œ

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21424v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21424v1.pdf)

---

## [12. Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning](https://arxiv.org/abs/2602.21420v1)

**ä½œè€…**ï¼šYuanda Xu, Hejian Sang, Zhengze Zhou ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§ä¸å¯¹ç§°çš„ç½®ä¿¡åº¦æƒ©ç½šæ–¹æ³•ï¼Œä»¥æ”¹è¿›å¼ºåŒ–å­¦ä¹ ä¸­çš„é”™è¯¯ä¿®æ­£ï¼Œå¢å¼ºç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¤šæ ·æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨æé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œè¿‡äºç»Ÿä¸€çš„é”™è¯¯æƒ©ç½šç­–ç•¥å¯¼è‡´æ¨¡å‹æ¨ç†è¾¹ç•Œæ”¶çª„å’Œç”Ÿæˆå¤šæ ·æ€§ä¸‹é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºäº†ä¸å¯¹ç§°ç½®ä¿¡åº¦é”™è¯¯æƒ©ç½šï¼ˆACEï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒèŠ‚è´Ÿä¼˜åŠ¿æ¥è§£å†³è¿‡äºè‡ªä¿¡çš„é”™è¯¯é—®é¢˜ã€‚

**ä¸»è¦ç»“è®º**ï¼šACEæ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºå‡ºä¸ç°æœ‰æ–¹æ³•çš„è‰¯å¥½å…¼å®¹æ€§ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´æ€§æé«˜äº†æ€§èƒ½ã€‚

**å…³é”®è¯**ï¼šå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰, å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼ºåŒ–å­¦ä¹ , è¿‡åº¦è‡ªä¿¡é”™è¯¯, éå¯¹ç§°ç½®ä¿¡åº¦æƒ©ç½šï¼ˆACEï¼‰, ç½®ä¿¡åº¦åç§»åº¦é‡, è´Ÿä¼˜åŠ¿è°ƒåˆ¶, é€‰æ‹©æ€§æ­£åˆ™åŒ–, ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–, ç”Ÿæˆå¤šæ ·æ€§é€€åŒ–

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21420v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21420v1.pdf)

---

## [13. Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting](https://arxiv.org/abs/2602.21415v1)

**ä½œè€…**ï¼šSunki Hong, Jisoo Lee, Yuanyuan Shi  
**åˆ†ç±»**ï¼šcs.LG, eess.SY  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Selecting the right deep learning model for power grid forecasting is challenging, as performance heavily depends on the data available to the operator. This paper presents a comprehensive benchmark of five modern neural architectures: two state space models (PowerMamba, S-Mamba), two Transformers (iTransformer, PatchTST), and a traditional LSTM. We evaluate these models on hourly electricity demand across six diverse US power grids for forecast windows between 24 and 168 hours. To ensure a fair comparison, we adapt each model with specialized temporal processing and a modular layer that cleanly integrates weather covariates. Our results reveal that there is no single best model for all situations. When forecasting using only historical load, PatchTST and the state space models provide the highest accuracy. However, when explicit weather data is added to the inputs, the rankings reverse: iTransformer improves its accuracy three times more efficiently than PatchTST. By controlling for model size, we confirm that this advantage stems from the architecture's inherent ability to mix information across different variables. Extending our evaluation to solar generation, wind power, and wholesale prices further demonstrates that model rankings depend on the forecast task: PatchTST excels on highly rhythmic signals like solar, while state space models are better suited for the chaotic fluctuations of wind and price. Ultimately, this benchmark provides grid operators with actionable guidelines for selecting the optimal forecasting architecture based on their specific data environments.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè¯¥è®ºæ–‡ç³»ç»ŸåŸºå‡†æµ‹è¯•äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ã€Transformerä¸LSTMåœ¨ç¾å›½å¤šç”µç½‘å¤šä»»åŠ¡é¢„æµ‹ä¸­çš„è¡¨ç°ï¼Œå‘ç°â€œæœ€ä¼˜æ¨¡å‹â€å–å†³äºæ˜¯å¦æœ‰å¤©æ°”ç­‰å¤–ç”Ÿå˜é‡ä»¥åŠå…·ä½“é¢„æµ‹å¯¹è±¡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç”µç½‘è´Ÿè·/æ–°èƒ½æº/ä»·æ ¼é¢„æµ‹ä¸­æ¨¡å‹é€‰æ‹©é«˜åº¦ä¾èµ–å¯ç”¨æ•°æ®ï¼ˆä»…å†å²åºåˆ— vs. å¸¦å¤©æ°”åå˜é‡ï¼‰ï¼Œç¼ºå°‘è·¨åŒºåŸŸã€è·¨ä»»åŠ¡ã€è·¨æ¶æ„çš„å…¬å¹³å¯¹æ¯”æ¥æŒ‡å¯¼è¿è¥æ–¹é€‰å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨6ä¸ªç¾å›½ç”µç½‘çš„å°æ—¶çº§ç”¨ç”µéœ€æ±‚ä¸Šï¼Œå¯¹PowerMambaã€S-Mambaã€iTransformerã€PatchTSTå’ŒLSTMè¿›è¡Œ24â€“168å°æ—¶æ»šåŠ¨é¢„æµ‹åŸºå‡†ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€çš„æ—¶é—´å¤„ç†ä¸å¯æ’æ‹”æ¨¡å—å…¬å¹³æ•´åˆå¤©æ°”åå˜é‡ï¼›åŒæ—¶æ‰©å±•åˆ°å…‰ä¼ã€é£ç”µä¸æ‰¹å‘ç”µä»·ä»»åŠ¡ï¼Œå¹¶æ§åˆ¶æ¨¡å‹è§„æ¨¡åˆ†ææ¶æ„å·®å¼‚æ¥æºã€‚

**ä¸»è¦ç»“è®º**ï¼šä»…ç”¨å†å²è´Ÿè·æ—¶PatchTSTä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ç²¾åº¦æœ€ä½³ï¼›åŠ å…¥æ˜¾å¼å¤©æ°”åiTransformerçš„æ”¶ç›Šæ˜¾è‘—æ›´å¤§ä¸”æ›´é«˜æ•ˆï¼Œä¼˜åŠ¿æ¥è‡ªæ›´å¼ºçš„è·¨å˜é‡ä¿¡æ¯æ··åˆèƒ½åŠ›ï¼›ä¸åŒä»»åŠ¡å‘ˆç°ä¸åŒèµ¢å®¶ï¼ˆPatchTSTé€‚åˆå¼ºèŠ‚å¾‹å¦‚å…‰ä¼ï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹æ›´é€‚åˆé£ç”µ/ä»·æ ¼ç­‰é«˜å™ªå£°æ³¢åŠ¨ï¼‰ï¼Œå› æ­¤åº”æŒ‰æ•°æ®ç¯å¢ƒä¸ä»»åŠ¡ç‰¹æ€§é€‰æ¨¡å‹ã€‚

**å…³é”®è¯**ï¼šç”µåŠ›ç³»ç»Ÿé¢„æµ‹, ç”µç½‘è´Ÿè·é¢„æµ‹, æ—¶é—´åºåˆ—é¢„æµ‹, å¤šå˜é‡é¢„æµ‹, å¤–ç”Ÿå˜é‡ï¼ˆå¤©æ°”ï¼‰, é¢„æµ‹åŒºé—´ï¼ˆ24-168å°æ—¶ï¼‰, æ¨¡å‹åŸºå‡†è¯„æµ‹, çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰, å¯å†ç”Ÿèƒ½æºå‘ç”µé¢„æµ‹, ç”µåŠ›å¸‚åœºä»·æ ¼é¢„æµ‹

**è¯„åˆ†**ï¼š17

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21415v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21415v1.pdf)

---

## [14. Generative Bayesian Computation as a Scalable Alternative to Gaussian Process Surrogates](https://arxiv.org/abs/2602.21408v1)

**ä½œè€…**ï¼šNick Polson, Vadim Sokolov  
**åˆ†ç±»**ï¼šcs.LG, stat.AP, stat.CO, stat.ME, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Gaussian process (GP) surrogates are the default tool for emulating expensive computer experiments, but cubic cost, stationarity assumptions, and Gaussian predictive distributions limit their reach. We propose Generative Bayesian Computation (GBC) via Implicit Quantile Networks (IQNs) as a surrogate framework that targets all three limitations. GBC learns the full conditional quantile function from input--output pairs; at test time, a single forward pass per quantile level produces draws from the predictive distribution.   Across fourteen benchmarks we compare GBC to four GP-based methods. GBC improves CRPS by 11--26\% on piecewise jump-process benchmarks, by 14\% on a ten-dimensional Friedman function, and scales linearly to 90,000 training points where dense-covariance GPs are infeasible. A boundary-augmented variant matches or outperforms Modular Jump GPs on two-dimensional jump datasets (up to 46\% CRPS improvement). In active learning, a randomized-prior IQN ensemble achieves nearly three times lower RMSE than deep GP active learning on Rocket LGBB. Overall, GBC records a favorable point estimate in 12 of 14 comparisons. GPs retain an edge on smooth surfaces where their smoothness prior provides effective regularization.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºç”¨åŸºäºéšå¼åˆ†ä½æ•°ç½‘ç»œï¼ˆIQNï¼‰çš„ç”Ÿæˆå¼è´å¶æ–¯è®¡ç®—ï¼ˆGBCï¼‰æ›¿ä»£é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä»£ç†æ¨¡å‹ï¼Œä»¥æ›´å¥½å¤„ç†éå¹³ç¨³/è·³å˜ç°è±¡å¹¶å®ç°å¤§è§„æ¨¡çº¿æ€§æ‰©å±•çš„é¢„æµ‹åˆ†å¸ƒå»ºæ¨¡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»ŸGPä»£ç†æ¨¡å‹å­˜åœ¨è®­ç»ƒ/æ¨æ–­ç«‹æ–¹å¤æ‚åº¦ã€å¸¸è§å¹³ç¨³æ€§å‡è®¾ä¸é€‚ç”¨äºè·³å˜æˆ–åˆ†æ®µå‡½æ•°ã€ä»¥åŠé«˜æ–¯é¢„æµ‹åˆ†å¸ƒè¡¨è¾¾èƒ½åŠ›å—é™ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†åœ¨å¤æ‚ä¸å¤§æ•°æ®ä»¿çœŸä¸­çš„é€‚ç”¨æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šGBCé€šè¿‡IQNç›´æ¥å­¦ä¹ æ¡ä»¶åˆ†ä½æ•°å‡½æ•°ï¼ˆç»™å®šè¾“å…¥è¾“å‡ºå¯¹ï¼‰ï¼Œæµ‹è¯•æ—¶å¯¹ä¸åŒåˆ†ä½æ°´å¹³åšä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯ç”Ÿæˆé¢„æµ‹åˆ†å¸ƒæ ·æœ¬ï¼›å¹¶å¼•å…¥è¾¹ç•Œå¢å¼ºå˜ä½“ä¸éšæœºå…ˆéªŒçš„IQNé›†æˆç”¨äºæå‡è·³å˜å»ºæ¨¡ä¸ä¸»åŠ¨å­¦ä¹ è¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨14ä¸ªåŸºå‡†ä¸Šï¼ŒGBCç›¸å¯¹å¤šç§GPæ–¹æ³•åœ¨å¤šæ•°ä»»åŠ¡ä¸­å–å¾—æ›´ä¼˜æˆ–å¯æ¯”çš„CRPS/RMSEï¼Œå°¤å…¶åœ¨è·³å˜/åˆ†æ®µä¸é«˜ç»´ä»»åŠ¡ä¸Šæå‡æ˜æ˜¾ï¼Œä¸”å¯æ‰©å±•åˆ°9ä¸‡è®­ç»ƒæ ·æœ¬ï¼›ä½†åœ¨å¹³æ»‘è¡¨é¢ä¸ŠGPå‡­å€Ÿå¹³æ»‘å…ˆéªŒä»å…·æœ‰ä¸€å®šä¼˜åŠ¿ã€‚

**å…³é”®è¯**ï¼šä»£ç†å»ºæ¨¡, é«˜æ–¯è¿‡ç¨‹å›å½’, ç”Ÿæˆå¼è´å¶æ–¯è®¡ç®—, éšå¼åˆ†ä½æ•°ç½‘ç»œ, æ¡ä»¶åˆ†ä½æ•°å‡½æ•°, é¢„æµ‹åˆ†å¸ƒé‡‡æ ·, è·³è·ƒè¿‡ç¨‹å»ºæ¨¡, éå¹³ç¨³è¿‡ç¨‹, å¤§è§„æ¨¡è®­ç»ƒæ‰©å±•, ä¸»åŠ¨å­¦ä¹ , è¿ç»­ç§©æ¦‚ç‡å¾—åˆ†

**è¯„åˆ†**ï¼š21

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21408v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21408v1.pdf)

---

## [15. FedVG: Gradient-Guided Aggregation for Enhanced Federated Learning](https://arxiv.org/abs/2602.21399v1)

**ä½œè€…**ï¼šAlina Devkota, Jacob Thrasher, Donald Adjeroh ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-24

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Federated Learning (FL) enables collaborative model training across multiple clients without sharing their private data. However, data heterogeneity across clients leads to client drift, which degrades the overall generalization performance of the model. This effect is further compounded by overemphasis on poorly performing clients. To address this problem, we propose FedVG, a novel gradient-based federated aggregation framework that leverages a global validation set to guide the optimization process. Such a global validation set can be established using readily available public datasets, ensuring accessibility and consistency across clients without compromising privacy. In contrast to conventional approaches that prioritize client dataset volume, FedVG assesses the generalization ability of client models by measuring the magnitude of validation gradients across layers. Specifically, we compute layerwise gradient norms to derive a client-specific score that reflects how much each client needs to adjust for improved generalization on the global validation set, thereby enabling more informed and adaptive federated aggregation. Extensive experiments on both natural and medical image benchmarking datasets, across diverse model architectures, demonstrate that FedVG consistently improves performance, particularly in highly heterogeneous settings. Moreover, FedVG is modular and can be seamlessly integrated with various state-of-the-art FL algorithms, often further improving their results. Our code is available at https://github.com/alinadevkota/FedVG.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFedVGé€šè¿‡å…¨å±€éªŒè¯é›†çš„æ¢¯åº¦ä¿¡æ¯ä¸ºå®¢æˆ·ç«¯åˆ†é…è‡ªé€‚åº”èšåˆæƒé‡ï¼Œä»è€Œç¼“è§£è”é‚¦å­¦ä¹ ä¸­çš„éIIDå¯¼è‡´çš„å®¢æˆ·ç«¯æ¼‚ç§»å¹¶æå‡æ³›åŒ–æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»ŸFLèšåˆå¸¸æŒ‰æ•°æ®é‡åŠ æƒï¼Œæ˜“è¿‡åº¦å¼ºè°ƒè¡¨ç°å·®æˆ–åˆ†å¸ƒåçš„å®¢æˆ·ç«¯ï¼Œåœ¨æ•°æ®å¼‚è´¨æ€§å¼ºæ—¶å¼•å‘å®¢æˆ·ç«¯æ¼‚ç§»å¹¶æŸå®³å…¨å±€æ¨¡å‹æ³›åŒ–ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¼•å…¥å¯ç”±å…¬å…±æ•°æ®æ„å»ºçš„å…¨å±€éªŒè¯é›†ï¼Œè®¡ç®—å„å®¢æˆ·ç«¯æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„åˆ†å±‚æ¢¯åº¦èŒƒæ•°ï¼Œå¹¶æ®æ­¤å½¢æˆåæ˜ â€œä¸ºæå‡æ³›åŒ–éœ€è¦è°ƒæ•´å¤šå°‘â€çš„å®¢æˆ·ç«¯å¾—åˆ†ï¼Œç”¨è¯¥å¾—åˆ†æŒ‡å¯¼èšåˆæƒé‡ï¼›è¯¥æ¡†æ¶å¯æ¨¡å—åŒ–åµŒå…¥å¤šç§ç°æœ‰FLç®—æ³•ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨è‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒç­‰å¤šæ•°æ®é›†ã€ä¸åŒæ¨¡å‹æ¶æ„ä¸é«˜å¼‚è´¨æ€§è®¾å®šä¸‹ï¼ŒFedVGç¨³å®šä¼˜äºåŸºçº¿å¹¶å¸¸èƒ½è¿›ä¸€æ­¥æå‡SOTAè”é‚¦å­¦ä¹ æ–¹æ³•çš„æ•ˆæœï¼ŒåŒæ—¶ä¸ä¾èµ–å…±äº«ç§æœ‰æ•°æ®ã€‚

**å…³é”®è¯**ï¼šè”é‚¦å­¦ä¹ , å®¢æˆ·ç«¯æ¼‚ç§», æ¢¯åº¦å¼•å¯¼èšåˆ, è‡ªé€‚åº”å®¢æˆ·ç«¯åŠ æƒ, å…¨å±€éªŒè¯é›†, å…¬å…±æ•°æ®é›†, å±‚çº§æ¢¯åº¦èŒƒæ•°, æ³›åŒ–è¯„ä¼°, å¼‚æ„å®¢æˆ·ç«¯é²æ£’æ€§, åŒ»å­¦å½±åƒåŸºå‡†

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.21399v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.21399v1.pdf)

---

