# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-27

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (5 ç¯‡)
- [cs.LG](#csLG) (7 ç¯‡)
- [cs.AI](#csAI) (3 ç¯‡)

---

## cs.AI

## [1. Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330v1)

**ä½œè€…**ï¼šKunihiro Miyazaki, Takanobu Kawahara, Stephen Roberts ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, q-fin.TR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§å°†æŠ•èµ„åˆ†ææ‹†è§£ä¸ºç»†ç²’åº¦äº¤æ˜“ä»»åŠ¡çš„å¤šæ™ºèƒ½ä½“LLMäº¤æ˜“ç³»ç»Ÿï¼Œåœ¨é˜²æ³„æ¼å›æµ‹çš„æ—¥æœ¬è‚¡å¸‚æ•°æ®ä¸Šæ˜¾è‘—æå‡é£é™©è°ƒæ•´åæ”¶ç›Šã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ™ºèƒ½ä½“äº¤æ˜“ç³»ç»Ÿå¤šç”¨ç²—ç²’åº¦è§’è‰²/æŒ‡ä»¤é©±åŠ¨ï¼Œå¿½è§†çœŸå®æŠ•ç ”æµç¨‹ç»†èŠ‚ï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½ä¸‹é™ä¸”å†³ç­–è¿‡ç¨‹ä¸é€æ˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®¾è®¡å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒæŠŠæŠ•ç ”æµç¨‹æ˜¾å¼åˆ†è§£ä¸ºå¤šä¸ªç»†ç²’åº¦åˆ†æä¸å†³ç­–å­ä»»åŠ¡ï¼Œå¹¶åœ¨åŒ…å«ä»·æ ¼ã€è´¢æŠ¥ã€æ–°é—»ä¸å®è§‚ä¿¡æ¯çš„æ—¥æœ¬è‚¡ç¥¨æ•°æ®ä¸Šè¿›è¡Œæ³„æ¼æ§åˆ¶å›æµ‹ï¼›åŒæ—¶åˆ†æä¸­é—´äº§ç‰©ä¸ä¸‹æ¸¸åå¥½çš„å¯¹é½ï¼Œå¹¶ç»“åˆæ ‡å‡†ç»„åˆä¼˜åŒ–åˆ©ç”¨ä¸æŒ‡æ•°ä½ç›¸å…³æ€§å’Œè¾“å‡ºæ–¹å·®æ„å»ºæŠ•èµ„ç»„åˆã€‚

**ä¸»è¦ç»“è®º**ï¼šç»†ç²’åº¦ä»»åŠ¡åˆ†è§£ç›¸è¾ƒç²—ç²’åº¦è®¾è®¡èƒ½æ˜¾è‘—æ”¹å–„é£é™©è°ƒæ•´åå›æŠ¥ï¼›ç³»ç»Ÿè¡¨ç°çš„å…³é”®é©±åŠ¨å› ç´ ä¹‹ä¸€æ˜¯å„é˜¶æ®µåˆ†æè¾“å‡ºä¸æœ€ç»ˆå†³ç­–åå¥½çš„å¯¹é½ï¼Œè¿›ä¸€æ­¥é€šè¿‡ç»„åˆä¼˜åŒ–å¯è·å¾—æ›´ä¼˜æ•´ä½“è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šå¤šæ™ºèƒ½ä½“ LLM, é‡åŒ–äº¤æ˜“, ç»†ç²’åº¦ä»»åŠ¡åˆ†è§£, æŠ•èµ„åˆ†æå·¥ä½œæµ, æ³„æ¼æ§åˆ¶å›æµ‹, æ—¥æœ¬è‚¡å¸‚æ•°æ®, é£é™©è°ƒæ•´æ”¶ç›Š, åå¥½å¯¹é½, ä¸­é—´è¾“å‡ºå¯è§£é‡Šæ€§, æŠ•èµ„ç»„åˆä¼˜åŒ–, ä½ç›¸å…³æ€§åˆ†æ•£

**è¯„åˆ†**ï¼š37

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23330v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23330v1.pdf)

---

## [2. LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329v1)

**ä½œè€…**ï¼šChen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus ç­‰ 19 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL, cs.CR, cs.CY, cs.HC  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šç ”ç©¶å‘ç°ï¼šåœ¨å¤šé¡¹ä¸ç”Ÿç‰©å®‰å…¨ç›¸å…³çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œç»™æ–°æ‰‹æä¾›LLMæ˜¾è‘—æå‡å…¶è¡¨ç°ï¼ˆå‡†ç¡®ç‡çº¦ä¸ºä»…ç”¨äº’è”ç½‘çš„4.16å€ï¼‰ï¼Œå¹¶æš´éœ²å‡ºæ½œåœ¨åŒç”¨é€”é£é™©ä¸è¯„æµ‹ç¼ºå£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå°½ç®¡LLMåœ¨ç”Ÿç‰©å­¦åŸºå‡†ä¸Šè¡¨ç°è¶Šæ¥è¶Šå¥½ï¼Œä½†ä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦çœŸæ­£â€œæŠ¬å‡â€ç¼ºä¹è®­ç»ƒçš„ç”¨æˆ·åˆ°è¶…è¶Šä»…é äº’è”ç½‘çš„æ°´å¹³ï¼›è¿™ç›´æ¥å…³ç³»åˆ°ç§‘ç ”åŠ é€Ÿä¸ç”Ÿç‰©å®‰å…¨åŒç”¨é€”é£é™©è¯„ä¼°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¼€å±•å¤šæ¨¡å‹ã€å¤šåŸºå‡†çš„äººç±»â€œupliftâ€å¯¹ç…§å®éªŒï¼šæ¯”è¾ƒæ–°æ‰‹åœ¨â€œå¯ç”¨LLMâ€ä¸â€œä»…äº’è”ç½‘â€æ¡ä»¶ä¸‹å®Œæˆ8ç»„ç”Ÿç‰©å®‰å…¨ç›¸å…³ä»»åŠ¡çš„è¡¨ç°ï¼Œå¹¶åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šä¸ä»…äº’è”ç½‘æ¡ä»¶ä¸‹çš„ä¸“å®¶åŸºçº¿å¯¹æ¯”ã€‚

**ä¸»è¦ç»“è®º**ï¼šLLMæ˜¾è‘—æå‡æ–°æ‰‹å‡†ç¡®ç‡ï¼ˆ4.16å€ï¼Œä¸”éƒ¨åˆ†ä»»åŠ¡æ–°æ‰‹+LLMè¶…è¿‡ä¸“å®¶åŸºçº¿ï¼‰ï¼Œä½†çº¯LLMå¾€å¾€åˆä¼˜äºâ€œäºº+LLMâ€ï¼Œè¯´æ˜ç”¨æˆ·æœªèƒ½å……åˆ†å‘æŒ¥æ¨¡å‹èƒ½åŠ›ï¼›å¤šæ•°å‚ä¸è€…ä¹Ÿè¡¨ç¤ºè¾ƒæ˜“è·å¾—åŒç”¨é€”ç›¸å…³ä¿¡æ¯ï¼Œæç¤ºéœ€æŒç»­ã€äº¤äº’å¼çš„upliftè¯„ä¼°ä¸æ›´å¼ºé˜²æŠ¤ã€‚

**å…³é”®è¯**ï¼šæ–°æ‰‹èƒ½åŠ›æå‡, äººæœºåä½œè¯„æµ‹, LLMè¾…åŠ©ä»»åŠ¡, ç”Ÿç‰©å®‰å…¨, è®¡ç®—ç”Ÿç‰©å­¦ä»»åŠ¡, åŸºå‡†æµ‹è¯•, ä¸“å®¶åŸºçº¿å¯¹æ¯”, å®‰å…¨é˜²æŠ¤ç»•è¿‡, äº¤äº’å¼è¯„ä¼°

**è¯„åˆ†**ï¼š16

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23329v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23329v1.pdf)

---

## [3. Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315v1)

**ä½œè€…**ï¼šSha Hu  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºåˆ©ç”¨è¾“å…¥çš„ä¸å˜å˜æ¢ç”Ÿæˆå¤šæ ·æœ¬å¹¶å¯¹é¢„æµ‹ç»“æœé‡é‡‡æ ·èšåˆï¼Œä»¥é™ä½è®¤çŸ¥ï¼ˆepistemicï¼‰ä¸ç¡®å®šæ€§å¹¶æå‡æ¨ç†ç²¾åº¦ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå³ä½¿è®­ç»ƒè‰¯å¥½çš„æ¨¡å‹åœ¨æ¨ç†æ—¶ä»ä¼šå› å¶ç„¶ï¼ˆaleatoricï¼‰ä¸è®¤çŸ¥ï¼ˆepistemicï¼‰ä¸ç¡®å®šæ€§äº§ç”Ÿé”™è¯¯ï¼Œä¸”æ¨¡å‹å¢å¤§å¹¶éå”¯ä¸€æç²¾è·¯å¾„ã€‚ä½œè€…è§‚å¯Ÿåˆ°å¯¹åŒä¸€è¾“å…¥åšä¸å˜å˜æ¢åå¾—åˆ°çš„å¤šæ¬¡æ¨ç†è¯¯å·®åœ¨è®¤çŸ¥ä¸ç¡®å®šæ€§ä¸Šå‘ˆéƒ¨åˆ†ç‹¬ç«‹æ€§ï¼Œå¯è¢«ç”¨æ¥â€œæŠµæ¶ˆâ€é”™è¯¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¯¹å•ä¸ªè¾“å…¥æ–½åŠ å¤šç§ä¿æŒè¯­ä¹‰/æ ‡ç­¾ä¸å˜çš„å˜æ¢ï¼ˆå¦‚å‡ ä½•ã€å¢å¼ºç­‰ï¼‰ï¼Œç”¨åŒä¸€å·²è®­ç»ƒæ¨¡å‹åˆ†åˆ«æ¨ç†å¾—åˆ°å¤šä»½è¾“å‡ºã€‚å†é€šè¿‡â€œresampling/èšåˆâ€ç­–ç•¥å°†è¿™äº›è¾“å‡ºæ•´åˆä¸ºæ›´å¯é çš„æœ€ç»ˆé¢„æµ‹ï¼Œä»è€Œå‡å°‘ç”±è®¤çŸ¥ä¸ç¡®å®šæ€§å¯¼è‡´çš„åå·®ä¸é”™è¯¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šå¤šå˜æ¢å¤šæ¬¡æ¨ç†å¹¶èšåˆå¯æœ‰æ•ˆé™ä½è®¤çŸ¥ä¸ç¡®å®šæ€§å¸¦æ¥çš„æ¨ç†é”™è¯¯ã€æå‡å‡†ç¡®ç‡ã€‚è¯¥ç­–ç•¥ä¸ºåœ¨ä¸æ˜¾è‘—å¢å¤§æ¨¡å‹è§„æ¨¡çš„æƒ…å†µä¸‹è·å–æ›´å¥½æ€§èƒ½æä¾›äº†æŠ˜ä¸­æ–¹æ¡ˆï¼ˆä»¥é¢å¤–æ¨ç†å¼€é”€æ¢å–ç²¾åº¦æå‡ï¼‰ã€‚

**å…³é”®è¯**ï¼šè®¤çŸ¥ä¸ç¡®å®šæ€§, å¶ç„¶ä¸ç¡®å®šæ€§, ä¸ç¡®å®šæ€§åˆ†è§£, ä¸å˜æ€§å˜æ¢, æµ‹è¯•æ—¶å¢å¼º, é‡é‡‡æ ·æ¨ç†, å¤šè§†è§’æ¨ç†, é¢„æµ‹é›†æˆ, è¾“å‡ºèšåˆ, æ¨ç†è¯¯å·®é™ä½, æ¨¡å‹è§„æ¨¡-æ€§èƒ½æƒè¡¡

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23315v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23315v1.pdf)

---

## cs.CV

## [4. MediX-R1: Open Ended Medical Reinforcement Learning](https://arxiv.org/abs/2602.23363v1)

**ä½œè€…**ï¼šSahal Shaji Mullappilly, Mohammed Irfan Kurpath, Omair Mohamed ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMediX-R1 æå‡ºé¢å‘åŒ»ç–—å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¼€æ”¾å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¤šä¿¡å·å¤åˆå¥–åŠ±ä¸åŸºäºLLMçš„è¯„æµ‹ï¼Œå®ç°è¶…è¶Šé€‰æ‹©é¢˜çš„ä¸´åºŠè‡ªç”±æ–‡æœ¬å›ç­”èƒ½åŠ›å¹¶åœ¨å¤šé¡¹åŸºå‡†ä¸Šæ˜¾è‘—æå‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŒ»ç–—LLM/VLMè®­ç»ƒä¸è¯„æµ‹å¤šä¾èµ–å¯éªŒè¯ç­”æ¡ˆæˆ–å¤šé€‰é¢˜ï¼Œéš¾ä»¥å¯¹å¼€æ”¾å¼ä¸´åºŠå›ç­”æä¾›ç¨³å®šã€ç»†ç²’åº¦çš„åé¦ˆä¸å¯é è¯„ä¼°ã€‚ä¸ºæå‡çœŸå®åœºæ™¯ä¸‹çš„è¯­ä¹‰æ­£ç¡®æ€§ä¸å¯è§£é‡Šæ¨ç†ï¼Œéœ€è¦é¢å‘è‡ªç”±æ–‡æœ¬çš„å¥–åŠ±è®¾è®¡ä¸è¯„æµ‹ä½“ç³»ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨è§†è§‰-è¯­è¨€éª¨å¹²æ¨¡å‹ä¸Šè¿›è¡ŒGroup Based RLå¾®è°ƒï¼Œè®¾è®¡å¤åˆå¥–åŠ±ï¼šLLMåˆ¤å®šçš„ä¸¥æ ¼è¯­ä¹‰æ­£ç¡®æ€§(YES/NO)å¥–åŠ±ã€åŒ»å­¦åµŒå…¥çš„è¯­ä¹‰ç›¸ä¼¼å¥–åŠ±ï¼Œä»¥åŠæ ¼å¼ä¸æ¨¡æ€è¯†åˆ«çš„è½»é‡å¥–åŠ±ä»¥çº¦æŸå¯è§£é‡Šæ¨ç†ä¸è¾“å…¥æ¨¡æ€ç†è§£ã€‚è¯„æµ‹ä¸Šæå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œç”¨Reference-based LLM-as-judgeæ›¿ä»£å­—ç¬¦ä¸²åŒ¹é…æŒ‡æ ‡ï¼Œè¦†ç›–æ–‡æœ¬ä¸å›¾æ–‡ä»»åŠ¡çš„è¯­ä¹‰æ­£ç¡®æ€§ã€æ¨ç†ä¸ä¸Šä¸‹æ–‡å¯¹é½ã€‚

**ä¸»è¦ç»“è®º**ï¼šä»…ç”¨çº¦51KæŒ‡ä»¤æ ·æœ¬ï¼ŒMediX-R1åœ¨æ ‡å‡†åŒ»ç–—æ–‡æœ¬LLMä¸å›¾æ–‡VLMåŸºå‡†ä¸Šè¶…è¿‡å¼ºå¼€æºåŸºçº¿ï¼Œå°¤å…¶åœ¨å¼€æ”¾å¼ä¸´åºŠä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆç»¼åˆå¥–åŠ±ä¿¡å·ä¸LLMè¯„æµ‹çš„å¼€æ”¾å¼RLæ˜¯æå‡åŒ»ç–—å¤šæ¨¡æ€æ¨¡å‹å¯é æ¨ç†çš„å¯è¡Œè·¯å¾„ã€‚

**å…³é”®è¯**ï¼šå¼€æ”¾å¼å¼ºåŒ–å­¦ä¹ , åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹, è¯­ä¹‰æ­£ç¡®æ€§, å¥–åŠ±æœºåˆ¶, è¯„ä¼°æ¡†æ¶, ä¸´åºŠä»»åŠ¡, æ¨¡å‹è®­ç»ƒ, æ•°æ®é›†

**è¯„åˆ†**ï¼š39

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23363v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23363v1.pdf)

---

## [5. VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale](https://arxiv.org/abs/2602.23361v1)

**ä½œè€…**ï¼šSven Elflein, Ruilong Li, SÃ©rgio Agostinho ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šVGG-T$^3$é€šè¿‡å°†å¯å˜é•¿åº¦çš„åœºæ™¯KVè¡¨ç¤ºåœ¨æµ‹è¯•æ—¶è’¸é¦ä¸ºå›ºå®šå¤§å°çš„MLPï¼Œå®ç°å¯¹å¤§è§„æ¨¡å¤šè§†å›¾ç¦»çº¿å‰é¦ˆ3Dé‡å»ºçš„çº¿æ€§æ‰©å±•ï¼Œå¹¶åœ¨é€Ÿåº¦ä¸ç²¾åº¦ä¸Šä¼˜äºç°æœ‰çº¿æ€§æ–¹æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ç¦»çº¿å‰é¦ˆ3Dé‡å»ºæ–¹æ³•çš„è®¡ç®—ä¸æ˜¾å­˜å¼€é”€éšè¾“å…¥å›¾åƒæ•°å‘ˆäºŒæ¬¡å¢é•¿ï¼Œéš¾ä»¥å¤„ç†ä¸Šåƒå¼ å›¾åƒçš„é›†åˆã€‚ç“¶é¢ˆæ¥æºäºç”¨softmaxæ³¨æ„åŠ›èšåˆæ—¶äº§ç”Ÿçš„å¯å˜é•¿åº¦KVç©ºé—´è¡¨ç¤ºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºVisual Geometry Grounded Test Time Trainingï¼šåœ¨æµ‹è¯•æ—¶å¯¹åœºæ™¯çš„KVå‡ ä½•è¡¨å¾è¿›è¡Œè®­ç»ƒ/è’¸é¦ï¼Œç”¨å›ºå®šå®¹é‡çš„MLPæ›¿ä»£å¯å˜é•¿åº¦KVå­˜å‚¨ï¼Œä»è€Œä¿æŒå…¨å±€èšåˆèƒ½åŠ›å¹¶å°†å¤æ‚åº¦é™ä¸ºéšè§†è§’æ•°çº¿æ€§å¢é•¿ã€‚æ¨¡å‹å¯ç”¨æœªè§å›¾åƒæŸ¥è¯¢è¯¥åœºæ™¯è¡¨ç¤ºä»¥æ”¯æŒè§†è§‰å®šä½ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ–¹æ³•åœ¨1kå›¾åƒé›†åˆä¸Š54ç§’å®Œæˆé‡å»ºï¼Œç›¸æ¯”ä¾èµ–softmaxæ³¨æ„åŠ›çš„åŸºçº¿åŠ é€Ÿ11.6Ã—ï¼›åŒæ—¶åœ¨ç‚¹å›¾é‡å»ºè¯¯å·®ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–çº¿æ€§æ—¶é—´æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¯¹æœªè§è§†å›¾çš„å®šä½/æŸ¥è¯¢èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šä¸‰ç»´é‡å»º, å¤šè§†å›¾é‡å»º, ç¦»çº¿å‰é¦ˆæ¨ç†, çº¿æ€§æ—¶é—´æ‰©å±•, æµ‹è¯•æ—¶è®­ç»ƒ, å…¨å±€åœºæ™¯èšåˆ, è½¯æœ€å¤§æ³¨æ„åŠ›, ç‚¹å›¾é‡å»ºè¯¯å·®, è§†è§‰å®šä½

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23361v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23361v1.pdf)

---

## [6. Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training](https://arxiv.org/abs/2602.23357v1)

**ä½œè€…**ï¼šAheli Saha, RenÃ© Schuster, Didier Stricker  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç ”ç©¶äº‹ä»¶ç›¸æœºçš„å†…åœ¨å‚æ•°å¦‚ä½•å½±å“äº‹ä»¶æµç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œå¹¶æå‡ºè”åˆåˆ†å¸ƒè®­ç»ƒä»¥æå‡æ¨¡å‹å¯¹ä¸åŒä¼ æ„Ÿå™¨è®¾ç½®çš„é²æ£’æ³›åŒ–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šäº‹ä»¶ç›¸æœºæ•°æ®å½¢æ€æ–°é¢–ä¸”å…¬å¼€æ•°æ®ä¸å‚æ•°è¦†ç›–ä¸è¶³ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“å¯¹ç‰¹å®šä¼ æ„Ÿå™¨/å‚æ•°â€œè¿‡æ‹Ÿåˆâ€ï¼Œç¼ºä¹è·¨è®¾å¤‡ä¸è·¨é…ç½®çš„å¯é æ€§ã€‚ä½œè€…å¸Œæœ›ç³»ç»Ÿåˆ†æå…³é”®å†…å‚å¯¹æ£€æµ‹æ•ˆæœçš„ä½œç”¨ï¼Œå¹¶è®©ä¸‹æ¸¸æ£€æµ‹æ¨¡å‹å…·å¤‡ä¼ æ„Ÿå™¨æ— å…³çš„ç¨³å¥æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¯¹äº‹ä»¶ç›¸æœºä¿¡å·çš„å…³é”®å†…åœ¨å‚æ•°è¿›è¡Œç»†è‡´åˆ†æä¸å¯¹æ¯”å®éªŒï¼Œé‡åŒ–å…¶å¯¹ç›®æ ‡æ£€æµ‹æ¨¡å‹è¡¨ç°çš„å½±å“ï¼›åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé€šè¿‡â€œè”åˆåˆ†å¸ƒè®­ç»ƒâ€ï¼ˆåœ¨è®­ç»ƒä¸­è¦†ç›–å¹¶æ··åˆå¤šç§å‚æ•°/æ•°æ®åˆ†å¸ƒï¼‰æ¥å­¦ä¹ å¯¹å‚æ•°å˜åŒ–ä¸æ•æ„Ÿçš„è¡¨å¾ï¼Œä»è€Œå®ç°è‡ªé€‚åº”æ„ŸçŸ¥ä¸ä¼ æ„Ÿå™¨æ³›åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šäº‹ä»¶ç›¸æœºçš„å†…å‚å˜åŒ–ä¼šæ˜¾è‘—å½±å“äº‹ä»¶æ•°æ®åˆ†å¸ƒä¸æ£€æµ‹æ€§èƒ½ï¼›é€šè¿‡è”åˆåˆ†å¸ƒè®­ç»ƒå¯æœ‰æ•ˆæå‡ç›®æ ‡æ£€æµ‹æ¨¡å‹å¯¹ä¸åŒä¼ æ„Ÿå™¨é…ç½®çš„æ³›åŒ–èƒ½åŠ›ä¸é²æ£’æ€§ï¼Œå‘ä¼ æ„Ÿå™¨æ— å…³çš„ä¸‹æ¸¸åº”ç”¨è¿ˆè¿›ã€‚

**å…³é”®è¯**ï¼šäº‹ä»¶ç›¸æœº, äº‹ä»¶è§†è§‰, äº‹ä»¶æµç›®æ ‡æ£€æµ‹, è‡ªé€‚åº”æ„ŸçŸ¥, ä¼ æ„Ÿå™¨æ³›åŒ–, ä¼ æ„Ÿå™¨æ— å…³é²æ£’æ€§, è”åˆåˆ†å¸ƒè®­ç»ƒ, å†…åœ¨å‚æ•°å»ºæ¨¡, å‚æ•°æ•æ„Ÿæ€§åˆ†æ, åŸŸæ³›åŒ–

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23357v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23357v1.pdf)

---

## [7. Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?](https://arxiv.org/abs/2602.23339v1)

**ä½œè€…**ï¼šTilemachos Aravanis, Vladan StojniÄ‡, Bill Psomas ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§æ£€ç´¢å¢å¼ºçš„æµ‹è¯•æ—¶é€‚é…æ–¹æ³•ï¼Œåœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­ç”¨å°‘é‡åƒç´ æ ‡æ³¨æ”¯æŒæ ·æœ¬æ˜¾è‘—ç¼©å°é›¶æ ·æœ¬ä¸å…¨ç›‘ç£çš„æ€§èƒ½å·®è·ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¼€æ”¾è¯æ±‡åˆ†å‰²ä¸»è¦ä¾èµ–VLMçš„å›¾åƒçº§ç²—ç›‘ç£ä¸”å—è‡ªç„¶è¯­è¨€è¯­ä¹‰æ­§ä¹‰å½±å“ï¼Œå¯¼è‡´åƒç´ çº§é¢„æµ‹ç²¾åº¦è½åäºå…¨ç›‘ç£æ–¹æ³•ã€‚ä½œè€…å¸Œæœ›ç”¨å°‘é‡åƒç´ æ ‡æ³¨æ ·æœ¬å¼¥è¡¥ç›‘ç£é¸¿æ²Ÿï¼ŒåŒæ—¶ä¿æŒå¼€æ”¾è¯æ±‡èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨few-shotè®¾å®šä¸‹ï¼Œå¼•å…¥å¸¦åƒç´ æ ‡æ³¨çš„æ”¯æŒé›†ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶è¿›è¡Œæ£€ç´¢å¢å¼ºï¼šä¸ºæ¯ä¸ªæŸ¥è¯¢å›¾åƒæ£€ç´¢ç›¸å…³æ”¯æŒæ ·æœ¬ï¼Œèåˆæ–‡æœ¬ä¸æ”¯æŒè§†è§‰ç‰¹å¾ï¼Œå­¦ä¹ ä¸€ä¸ªè½»é‡çš„â€œæŒ‰å›¾å®šåˆ¶â€åˆ†ç±»å™¨/é€‚é…å™¨ã€‚ä¸æ‰‹å·¥æ™šèåˆä¸åŒï¼Œè¯¥æ–¹æ³•è¿›è¡Œå¯å­¦ä¹ çš„é€æŸ¥è¯¢èåˆï¼Œä¸”æ”¯æŒé›†å¯æŒç»­æ‰©å±•å¹¶é€‚ç”¨äºä¸ªæ€§åŒ–/ç»†ç²’åº¦åˆ†å‰²ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜è¯¥æ£€ç´¢+æµ‹è¯•æ—¶é€‚é…ç­–ç•¥èƒ½æ˜¾è‘—æå‡OVSæ€§èƒ½ã€ç¼©å°ä¸å…¨ç›‘ç£åˆ†å‰²çš„å·®è·ï¼ŒåŒæ—¶ä»ä¿ç•™å¯¹ä»»æ„æ–‡æœ¬ç±»åˆ«çš„å¼€æ”¾è¯æ±‡æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šå¼€æ”¾è¯æ±‡åˆ†å‰², è§†è§‰è¯­è¨€æ¨¡å‹, é›¶æ ·æœ¬åˆ†å‰², å°æ ·æœ¬åˆ†å‰², æ£€ç´¢å¢å¼º, æµ‹è¯•æ—¶è‡ªé€‚åº”, è½»é‡åˆ†ç±»å™¨, è·¨æ¨¡æ€ç‰¹å¾èåˆ, é€æŸ¥è¯¢èåˆ, ä¸ªæ€§åŒ–åˆ†å‰²

**è¯„åˆ†**ï¼š28

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23339v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23339v1.pdf)

---

## [8. ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding](https://arxiv.org/abs/2602.23306v1)

**ä½œè€…**ï¼šYiran Guan, Sifan Tu, Dingkang Liang ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šThinkOmni é€šè¿‡æ— éœ€è®­ç»ƒçš„æ•°æ®æ— å…³æ¡†æ¶ï¼ŒæŠŠç°æˆå¤§æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›â€œè¿ç§»â€åˆ°å…¨æ¨¡æ€å¤§æ¨¡å‹ï¼ˆOLLMï¼‰çš„è§£ç è¿‡ç¨‹ä¸­ï¼Œä»è€Œæ˜¾è‘—æå‡å¤šæ¨¡æ€æ¨ç†è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ OLLM è™½å…·å¤‡å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†æ¨ç†æ·±åº¦ä¸åŠè¿‘æœŸ LRMï¼›è€Œé€šè¿‡å†è®­ç»ƒè¡¥å¼ºæ¨ç†é€šå¸¸éœ€è¦é«˜è´¨é‡æ•°æ®ã€ä»»åŠ¡é€‚é…ä¸é«˜ç®—åŠ›æˆæœ¬ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º LRM-as-a-Guideï¼šåœ¨è§£ç æ—¶ç”¨ç°æˆ LRM ç”Ÿæˆ/è¯„ä¼°æ¨ç†ä¿¡å·æ¥å¼•å¯¼ OLLM è¾“å‡ºï¼›å¹¶è®¾è®¡ Stepwise Contrastive Scalingï¼šæŒ‰æ¨ç†æ­¥éª¤è‡ªé€‚åº”èåˆâ€œæ„ŸçŸ¥ vs æ¨ç†â€ä¿¡å·ï¼Œé¿å…æ‰‹å·¥è°ƒè¶…å‚ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å…­ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šæ•´ä½“ç¨³å®šæå‡ï¼ˆå¦‚ MathVista 70.2ã€MMAU 75.5ï¼‰ï¼Œè¡¨æ˜æ— éœ€è®­ç»ƒä¹Ÿèƒ½æœ‰æ•ˆå¢å¼º OLLM çš„æ¨ç†èƒ½åŠ›ä¸”å…·å¤‡è¾ƒå¼ºé€šç”¨æ€§ä¸å¯è¿ç§»æ€§ã€‚

**å…³é”®è¯**ï¼šå…¨æ¨¡æ€æ¨ç†, å¤šæ¨¡æ€æ¨ç†, è®­ç»ƒæ— å…³æ¡†æ¶, æ•°æ®æ— å…³æ–¹æ³•, å¼•å¯¼è§£ç , æ¨ç†æ¨¡å‹è’¸é¦å¼æŒ‡å¯¼, é€æ­¥å¯¹æ¯”ç¼©æ”¾, æ„ŸçŸ¥-æ¨ç†æƒè¡¡

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23306v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23306v1.pdf)

---

## cs.LG

## [9. Model Agreement via Anchoring](https://arxiv.org/abs/2602.23360v1)

**ä½œè€…**ï¼šEric Eaton, Surbhi Goel, Marcel Hussing ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.   We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§â€œé”šå®šï¼ˆanchoringï¼‰â€åˆ†ææŠ€æœ¯ï¼Œåœ¨ä¸åè°ƒè®­ç»ƒçš„å‰æä¸‹è¯æ˜ç‹¬ç«‹è®­ç»ƒæ¨¡å‹çš„é¢„æµ‹åˆ†æ­§å¯éšè®­ç»ƒ/æ¨¡å‹è§„æ¨¡ç­‰è‡ªç„¶å‚æ•°è¶‹è¿‘äº0ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°å¤šç§å¸¸è§ç®—æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç‹¬ç«‹è®­ç»ƒçš„æ¨¡å‹å¾€å¾€å­˜åœ¨é¢„æµ‹ä¸ä¸€è‡´ï¼Œå½±å“ç¨³å®šæ€§ä¸å¯å¤ç°æ€§ï¼›ä½œè€…å¸Œæœ›ç”¨èƒ½é€‚é…ç°æœ‰è®­ç»ƒæµç¨‹çš„é€šç”¨åˆ†ææ¡†æ¶ï¼Œåˆ»ç”»å¹¶æ¨åŠ¨è¿™ç§â€œæ¨¡å‹åˆ†æ­§â€é™åˆ°æ¥è¿‘é›¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥å›å½’ä»»åŠ¡ä¸­ä¸¤æ¨¡å‹é¢„æµ‹å·®çš„æœŸæœ›å¹³æ–¹ï¼ˆç‹¬ç«‹æ ·æœ¬è®­ç»ƒã€æ— åè°ƒï¼‰ä½œä¸ºåˆ†æ­§åº¦é‡ï¼Œåœ¨è¯æ˜ä¸­å¼•å…¥â€œé”šå®šåˆ°ä¸¤æ¨¡å‹å¹³å‡å€¼â€çš„æŠ€å·§æ¥ä¸Šç•Œåˆ†æ­§ï¼›å¹¶å°†è¯¥æŠ€å·§åˆ†åˆ«ç”¨äºå †å é›†æˆã€æ¢¯åº¦æå‡ã€å¸¦æ¶æ„æœç´¢çš„ç¥ç»ç½‘ç»œè®­ç»ƒã€å›ºå®šæ·±åº¦å›å½’æ ‘è®­ç»ƒï¼ŒåŒæ—¶ä»ä¸€ç»´å¹³æ–¹æŸå¤±æ¨å¹¿åˆ°å¤šç»´ä¸ä»»æ„å¼ºå‡¸æŸå¤±ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»™å‡ºå››ç±»ç®—æ³•çš„åˆ†æ­§ä¸Šç•Œä¸æ”¶æ•›è¶‹åŠ¿ï¼šå †å é›†æˆéšæ¨¡å‹æ•°kå¢å¤§åˆ†æ­§è¶‹0ï¼Œæ¢¯åº¦æå‡éšè¿­ä»£kå¢å¤§è¶‹0ï¼Œæ¶æ„æœç´¢éšå€™é€‰æ¶æ„è§„æ¨¡nå¢å¤§è¶‹0ï¼Œå›å½’æ ‘éšæ·±åº¦då¢å¤§è¶‹0ï¼›ç»“æœå¯æ¨å¹¿åˆ°æ›´ä¸€èˆ¬çš„å¤šç»´å›å½’ä¸å¼ºå‡¸æŸå¤±è®¾ç½®ã€‚

**å…³é”®è¯**ï¼šæ¨¡å‹åˆ†æ­§, ç‹¬ç«‹è®­ç»ƒ, é¢„æµ‹å·®å¹³æ–¹æœŸæœ›, é”šå®šåˆ†æ, åˆ†æ­§ç•Œ, å †å é›†æˆ, æ¢¯åº¦æå‡, ç¥ç»æ¶æ„æœç´¢, å›å½’æ ‘æ·±åº¦, å¼ºå‡¸æŸå¤±, å¤šç»´å›å½’

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23360v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23360v1.pdf)

---

## [10. A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358v1)

**ä½œè€…**ï¼šElad Kimchi Shoshani, Leeyam Gabay, Yedid Hoshen  
**åˆ†ç±»**ï¼šcs.LG, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPLADAé€šè¿‡åœ¨å®¢æˆ·ç«¯é¢„ç½®çš„å¤§è§„æ¨¡æ— æ ‡æ³¨å‚è€ƒæ•°æ®é›†ä¸Šåªä¼ è¾“å°‘é‡å›¾åƒçš„ç±»åˆ«æ ‡ç­¾ï¼ˆå¹¶å‰ªææŒ‘é€‰æœ€ç›¸å…³æ ·æœ¬ï¼‰ï¼Œå®ç°ä»¥<1MBè½½è·é«˜æ•ˆä¼ é€’æ–°åˆ†ç±»ä»»åŠ¡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ•°æ®é›†æœåŠ¡å™¨å‘å¤šå®¢æˆ·ç«¯åˆ†å‘é«˜åˆ†è¾¨ç‡åŸå§‹æ•°æ®é€šä¿¡æˆæœ¬æé«˜ï¼Œè€Œç›´æ¥ä¸‹å‘é¢„è®­ç»ƒæ¨¡å‹åˆå› ç¡¬ä»¶/æ¡†æ¶å·®å¼‚å¸¸ä¸å¯è¡Œï¼›ç°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•éš¾ä»¥åœ¨é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹åšåˆ°è¶³å¤Ÿå°çš„æ–‡ä»¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå‡è®¾å®¢æˆ·ç«¯å·²æ‹¥æœ‰é€šç”¨å‚è€ƒæ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰ï¼ŒæœåŠ¡å™¨ä¸å‘é€åƒç´ è€Œä»…å‘é€â€œå“ªäº›å‚è€ƒå›¾åƒå¯¹åº”å“ªäº›ç±»åˆ«â€çš„ä¼ªæ ‡ç­¾é›†åˆï¼›ä¸ºç¼“è§£å‚è€ƒé›†ä¸ç›®æ ‡ä»»åŠ¡åˆ†å¸ƒä¸åŒ¹é…ï¼Œå¼•å…¥å‰ªææœºåˆ¶ç­›é€‰è¯­ä¹‰æœ€ç›¸å…³çš„å‚è€ƒå›¾åƒä»¥æå‡è®­ç»ƒæ•ˆç‡å¹¶è¿›ä¸€æ­¥å‹ç¼©ä¼ è¾“é‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨10ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šï¼ŒPLADAèƒ½åœ¨ä¸ä¼ è¾“å›¾åƒåƒç´ çš„æƒ…å†µä¸‹ä»¥å°äº1MBçš„é€šä¿¡è½½è·ä¿æŒè¾ƒé«˜åˆ†ç±»å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†é¢å‘æ•°æ®é›†æœåŠ¡çš„æä½æˆæœ¬ä»»åŠ¡ä¼ é€’æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šæ•°æ®é›†æœåŠ¡, ä¼ªæ ‡ç­¾, æ•°æ®å‹ç¼©, ä¼ è¾“æ•ˆç‡, ä»»åŠ¡çŸ¥è¯†è½¬ç§», å›¾åƒåˆ†ç±», æ•°æ®ä¿®å‰ª, é«˜åˆ†è¾¨ç‡æ•°æ®

**è¯„åˆ†**ï¼š37

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23358v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23358v1.pdf)

---

## [11. SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353v1)

**ä½œè€…**ï¼šSimon Roschmann, Paul Krzakala, Sonia Mazelet ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSOTAlignæå‡ºä¸€ç§åŠç›‘ç£å¯¹é½æ¡†æ¶ï¼Œç”¨å°‘é‡å›¾æ–‡é…å¯¹ä¸å¤§é‡éé…å¯¹æ•°æ®ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“åœ¨å†»ç»“çš„è§†è§‰/è¯­è¨€å•æ¨¡æ€ç¼–ç å™¨é—´å­¦åˆ°ç¨³å¥çš„è”åˆåµŒå…¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¯¹é½æ–¹æ³•å¤šä¾èµ–å¯¹æ¯”å­¦ä¹ ä¸æµ·é‡é…å¯¹æ ·æœ¬ï¼Œæˆæœ¬é«˜ä¸”éš¾è·å–ï¼›ä½œè€…å¸Œæœ›åœ¨é…å¯¹ç›‘ç£æå°‘çš„æƒ…å†µä¸‹ï¼Œä»èƒ½æœ‰æ•ˆå¯¹é½é¢„è®­ç»ƒçš„å•æ¨¡æ€æ¨¡å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä¸¤é˜¶æ®µï¼šå…ˆç”¨å°‘é‡é…å¯¹æ•°æ®è®­ç»ƒçº¿æ€§â€œæ•™å¸ˆâ€æ¢å¤ç²—ç•¥å…±äº«å‡ ä½•ï¼›å†åœ¨å¤§é‡éé…å¯¹å›¾åƒ/æ–‡æœ¬ä¸Šç”¨åŸºäºæœ€ä¼˜ä¼ è¾“çš„æ•£åº¦å¯¹é½å…³ç³»ç»“æ„ï¼Œä»è€Œç»†åŒ–è”åˆç©ºé—´ä¸”é¿å…å¯¹ç›®æ ‡ç©ºé—´è¿‡åº¦çº¦æŸã€‚

**ä¸»è¦ç»“è®º**ï¼šSOTAlignèƒ½æœ‰æ•ˆåˆ©ç”¨éé…å¯¹æ•°æ®ï¼Œåœ¨ä¸åŒæ•°æ®é›†ä¸ä¸åŒç¼–ç å™¨ç»„åˆä¸Šå­¦ä¹ åˆ°æ›´é²æ£’çš„è·¨æ¨¡æ€åµŒå…¥ï¼Œå¹¶æ˜¾è‘—ä¼˜äºçº¯ç›‘ç£ä¸ç°æœ‰åŠç›‘ç£å¯¹é½åŸºçº¿ã€‚

**å…³é”®è¯**ï¼šåŠç›‘ç£å¯¹é½, è§†è§‰-è¯­è¨€å¯¹é½, å•æ¨¡æ€ç¼–ç å™¨, å†»ç»“é¢„è®­ç»ƒæ¨¡å‹, è½»é‡å¯¹é½å±‚, æœ€ä¼˜ä¼ è¾“, æœ€ä¼˜ä¼ è¾“æ•£åº¦, éé…å¯¹æ•°æ®, å°‘é‡å›¾æ–‡é…å¯¹, è”åˆåµŒå…¥ç©ºé—´, å…±äº«å‡ ä½•ç»“æ„, çº¿æ€§æ•™å¸ˆæ¨¡å‹

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23353v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23353v1.pdf)

---

## [12. FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349v1)

**ä½œè€…**ï¼šJose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.   We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.   Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFlashOptim é€šè¿‡æ›´ç²¾ç¡®çš„ä¸»æƒé‡æ‹†åˆ†ä¸æ›´ä½è¯¯å·®çš„8-bitä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–ï¼Œå°†æ··åˆç²¾åº¦è®­ç»ƒçš„æ¯å‚æ•°æ˜¾å­˜å ç”¨é™ä½50%ä»¥ä¸Šä¸”ä¸æŸå¤±æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ ‡å‡†æ··åˆç²¾åº¦è®­ç»ƒé™¤å‚æ•°å¤–è¿˜éœ€å­˜æ¢¯åº¦ä¸ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¸¸å„4å­—èŠ‚ï¼‰ï¼Œå¯¼è‡´å¤§æ¨¡å‹è®­ç»ƒ/å¾®è°ƒåœ¨<100GBæ˜¾å­˜ä¸‹éš¾ä»¥è´Ÿæ‹…ï¼Œå¹¶ä¸”checkpointä½“ç§¯è¿‡å¤§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼šåŸºäºé‡åŒ–è¯¯å·®ç´§ç•Œæ”¹è¿›master weight splittingä»¥é™ä½ä¸»æƒé‡æ‹†åˆ†è¯¯å·®ï¼›è®¾è®¡compandingï¼ˆå‹æ‰©ï¼‰å‡½æ•°ä»¥æ˜¾è‘—é™ä½8-bitä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–è¯¯å·®ï¼Œå¹¶é…åˆ16-bitæ¢¯åº¦ä¸å¯é€‰gradient releaseã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨SGD/AdamW/Lionä¸Šï¼ŒFlashOptim å°†AdamWå†…å­˜ä»16B/paramé™è‡³7Bï¼ˆå¯ç”¨æ¢¯åº¦é‡Šæ”¾å¯è‡³5Bï¼‰ï¼Œcheckpointå‡åŠä»¥ä¸Šï¼Œå¹¶åœ¨å¤šé¡¹è§†è§‰ä¸è¯­è¨€åŸºå‡†ï¼ˆå«Llama-3.1-8Bå¾®è°ƒï¼‰ä¸Šæœªè§‚å¯Ÿåˆ°è´¨é‡ä¸‹é™ã€‚

**å…³é”®è¯**ï¼šæ··åˆç²¾åº¦è®­ç»ƒ, æ˜¾å­˜ä¼˜åŒ–, ä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–, 8-bité‡åŒ–, ä¸»æƒé‡æ‹†åˆ†, é‡åŒ–è¯¯å·®ç•Œ, å‹æ‰©å‡½æ•°, 16-bitæ¢¯åº¦, æ¢¯åº¦é‡Šæ”¾, æ¨¡å‹æ£€æŸ¥ç‚¹å‹ç¼©

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23349v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23349v1.pdf)

---

## [13. Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341v1)

**ä½œè€…**ï¼šAlkis Kalavasis, Anay Mehrotra, Manolis Zampetakis ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.DS, math.ST, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç ”ç©¶åœ¨ä»…è§‚æµ‹åˆ°â€œæ ·æœ¬æ‰€åœ¨åˆ†åŒºé›†åˆâ€(coarse data)çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ–¯å‡å€¼ä¼°è®¡ï¼Œç»™å‡ºäº†å‡¸åˆ†åŒºä¸‹å‡å€¼å¯è¯†åˆ«æ€§çš„å‡ ä½•åˆ»ç”»ï¼Œå¹¶åœ¨å¯è¯†åˆ«æ—¶æä¾›å¤šé¡¹å¼æ—¶é—´çš„é«˜æ•ˆä¼°è®¡ç®—æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨å–æ•´æµ‹é‡ã€ä¼ æ„Ÿå™¨åˆ†è¾¨ç‡æˆ–ç»æµç³»ç»Ÿæ»åç­‰åœºæ™¯ä¸­ï¼Œæ•°æ®å¸¸ä»¥é›†åˆ/åŒºé—´è€Œéç²¾ç¡®å€¼å‡ºç°ï¼Œå¯¼è‡´å‡å€¼å¯èƒ½ä¸å¯å”¯ä¸€æ¢å¤ï¼›å·²æœ‰å·¥ä½œè™½è¡¨æ˜å‡¸åˆ†åŒºä¸‹å¯åœ¨å¯è¯†åˆ«æ—¶æ ·æœ¬é«˜æ•ˆï¼Œä½†ç¼ºå°‘â€œä½•æ—¶å¯è¯†åˆ«â€ä»¥åŠâ€œèƒ½å¦è®¡ç®—é«˜æ•ˆâ€çš„å®Œæ•´ç­”æ¡ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ä»å‡¸å‡ ä½•ä¸ç»Ÿè®¡å¯è¯†åˆ«æ€§å‡ºå‘ï¼Œå»ºç«‹â€œè§‚æµ‹åˆ°çš„åˆ†åŒºæ ‡ç­¾åˆ†å¸ƒâ€ä¸å‡å€¼ä¹‹é—´çš„ä¸€ä¸€å¯¹åº”æ¡ä»¶ï¼ˆå¯¹å‡¸åˆ†åŒºç»™å‡ºå¿…è¦ä¸”å……åˆ†çš„ç»“æ„åˆ»ç”»ï¼‰ï¼Œå¹¶æ®æ­¤æ„é€ å¯åœ¨å¤šé¡¹å¼æ—¶é—´å†…æ±‚è§£çš„ä¼°è®¡æµç¨‹ï¼ˆåˆ©ç”¨å‡¸æ€§å°†ä¼°è®¡è½¬åŒ–ä¸ºå¯è®¡ç®—çš„ä¼˜åŒ–/æ¨æ–­é—®é¢˜ï¼‰ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»“æœå®Œæ•´å›ç­”äº†ä¸¤ä¸ªå¼€æ”¾é—®é¢˜ï¼šåœ¨å‡¸åˆ†åŒºä¸‹ç²¾ç¡®åˆ»ç”»äº†å‡å€¼å¯è¯†åˆ«å½“ä¸”ä»…å½“æ»¡è¶³ç‰¹å®šå‡ ä½•/ä¿¡æ¯æ¡ä»¶ï¼›ä¸”ä¸€æ—¦æ»¡è¶³è¯¥æ¡ä»¶ï¼Œå°±å­˜åœ¨è®¡ç®—ä¸Šé«˜æ•ˆã€å¹¶ä¿æŒè‰¯å¥½æ ·æœ¬å¤æ‚åº¦çš„å‡å€¼ä¼°è®¡æ–¹æ³•ï¼Œä»è€Œå°†â€œå¯è¯†åˆ«æ€§â€ä¸â€œå¯è®¡ç®—æ€§â€åœ¨è¯¥æ¨¡å‹ä¸­å¯¹é½ã€‚

**å…³é”®è¯**ï¼šç²—ç²’åº¦æ•°æ®, é›†åˆå€¼è§‚æµ‹, é«˜æ–¯å‡å€¼ä¼°è®¡, å¤šç»´é«˜æ–¯åˆ†å¸ƒ, å¯è¯†åˆ«æ€§, å‡¸åˆ’åˆ†, æ ·æœ¬å¤æ‚åº¦, ä¿¡æ¯é‡æ¡ä»¶, å¤šé¡¹å¼æ—¶é—´ç®—æ³•, NP-å›°éš¾æ€§

**è¯„åˆ†**ï¼š21

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23341v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23341v1.pdf)

---

## [14. Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336v1)

**ä½œè€…**ï¼šCamilo Gomez, Pengyang Wang, Liansheng Tang  
**åˆ†ç±»**ï¼šcs.LG, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§åŸºäºè¶…å•çº¯å½¢ï¼ˆhypersimplexï¼‰å¹³æ»‘æŠ•å½±çš„å¯å¾®â€œ0-1æŸå¤±â€è¿‘ä¼¼ç®—å­Soft-Binary-Argmaxï¼Œå¹¶åœ¨å¤§æ‰¹é‡è®­ç»ƒä¸‹æå‡åˆ†ç±»æ³›åŒ–æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼š0-1æŸå¤±ä¸åˆ†ç±»ç›®æ ‡æœ€ä¸€è‡´ä½†ä¸å¯å¾®ï¼Œéš¾ä»¥ç”¨äºç«¯åˆ°ç«¯æ¢¯åº¦ä¼˜åŒ–ï¼›åŒæ—¶å¤§batchè®­ç»ƒå¸¸å‡ºç°æ³›åŒ–ä¸‹é™ï¼Œéœ€è¦æ›´è´´åˆä»»åŠ¡çš„ç»“æ„åŒ–çº¦æŸæ¥ç¼©å°æ€§èƒ½å·®è·ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡å—çº¦æŸä¼˜åŒ–æ„é€ åˆ°n,kç»´è¶…å•çº¯å½¢çš„å¹³æ»‘ã€ä¿åºæŠ•å½±ï¼Œå®šä¹‰Soft-Binary-Argmaxä½œä¸ºå¯å¾®è¿‘ä¼¼çš„äºŒå€¼/argmaxé€‰æ‹©ç®—å­ï¼›æ¨å¯¼å…¶æ€§è´¨å¹¶ç»™å‡ºé«˜æ•ˆJacobianè®¡ç®—ï¼Œä½¿å…¶å¯åµŒå…¥äºŒåˆ†ç±»ä¸å¤šåˆ†ç±»ç½‘ç»œè¿›è¡Œåå‘ä¼ æ’­ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥ç®—å­åœ¨logitsä¸Šæ–½åŠ å‡ ä½•ä¸€è‡´æ€§çº¦æŸï¼Œå®éªŒæ˜¾ç¤ºèƒ½åœ¨å¤§æ‰¹é‡è®­ç»ƒæƒ…å½¢æ˜¾è‘—æ”¹å–„æ³›åŒ–è¡¨ç°ï¼Œå¹¶ç¼©å°å¤§batchç›¸è¾ƒå°batchçš„æ€§èƒ½å·®è·ã€‚

**å…³é”®è¯**ï¼šå¯å¾®é›¶ä¸€æŸå¤±, é›¶ä¸€æŸå¤±è¿‘ä¼¼, è¶…å•çº¯å½¢æŠ•å½±, ç»“æ„åŒ–ä¼˜åŒ–å±‚, çº¦æŸä¼˜åŒ–, é›…å¯æ¯”çŸ©é˜µè®¡ç®—, å¤§æ‰¹é‡è®­ç»ƒ, æ³›åŒ–æå‡, å‡ ä½•ä¸€è‡´æ€§çº¦æŸ, å¤šåˆ†ç±»å­¦ä¹ 

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23336v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23336v1.pdf)

---

## [15. ParamMem: Augmenting Language Agents with Parametric Reflective Memory](https://arxiv.org/abs/2602.23320v1)

**ä½œè€…**ï¼šTianjun Yao, Yongqiang Chen, Yujia Zheng ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.MA  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šParamMemå°†è·¨æ ·æœ¬çš„åæ€æ¨¡å¼å‚æ•°åŒ–å­˜å…¥æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¯æ§é‡‡æ ·ç”Ÿæˆæ›´å…·å¤šæ ·æ€§çš„åæ€ï¼Œä»è€Œæå‡åæ€å‹è¯­è¨€ä»£ç†åœ¨å¤šç±»æ¨ç†ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è‡ªæˆ‘åæ€å¸¸äº§ç”Ÿé‡å¤å†…å®¹ï¼Œé™åˆ¶æ¨ç†å¢ç›Šï¼›ä½œè€…å®è¯å‘ç°â€œåæ€å¤šæ ·æ€§â€ä¸ä»»åŠ¡æˆåŠŸç‡æ˜¾è‘—æ­£ç›¸å…³ï¼Œå› æ­¤éœ€è¦æ›´ä¸°å¯Œçš„åæ€ä¿¡å·æ¥æºä¸ç”Ÿæˆæœºåˆ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºParamMemå‚æ•°åŒ–è®°å¿†æ¨¡å—ï¼ŒæŠŠè·¨æ ·æœ¬åæ€è§„å¾‹ç¼–ç è¿›æ¨¡å‹å‚æ•°ï¼Œå¹¶ç”¨æ¸©åº¦é‡‡æ ·æ§åˆ¶åæ€å¤šæ ·æ€§ï¼›åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºParamAgentï¼Œå°†ParamMemä¸æƒ…æ™¯è®°å¿†ï¼ˆepisodicï¼‰åŠè·¨æ ·æœ¬è®°å¿†è”åˆï¼Œç”¨äºè¿­ä»£åæ€ä¸è§£é¢˜ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ã€å¤šè·³é—®ç­”ä¸Šç›¸å¯¹SOTAåŸºçº¿ç¨³å®šæå‡ï¼›åŒæ—¶ParamMemæ›´æ ·æœ¬é«˜æ•ˆã€æ”¯æŒè·¨æ¨¡å‹è§„æ¨¡çš„å¼±åˆ°å¼ºè¿ç§»ï¼Œå¹¶èƒ½åœ¨ä¸ä¾èµ–æ›´å¼ºå¤–éƒ¨æ¨¡å‹çš„æƒ…å†µä¸‹å®ç°è‡ªæˆ‘æ”¹è¿›ã€‚

**å…³é”®è¯**ï¼šåæ€å¤šæ ·æ€§, å‚æ•°åŒ–è®°å¿†, å‚æ•°åŒ–åæ€è®°å¿†, è·¨æ ·æœ¬è®°å¿†, æƒ…èŠ‚è®°å¿†, æ¸©åº¦é‡‡æ ·, æ ·æœ¬æ•ˆç‡, å¼±åˆ°å¼ºè¿ç§», å¤šè·³é—®ç­”

**è¯„åˆ†**ï¼š49

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23320v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23320v1.pdf)

---

