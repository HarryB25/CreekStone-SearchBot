# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-14

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (3 ç¯‡)
- [cs.LG](#csLG) (9 ç¯‡)
- [cs.AI](#csAI) (3 ç¯‡)

---

## cs.AI

## [1. CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use](https://arxiv.org/abs/2602.12268v1)

**ä½œè€…**ï¼šZhen Zhang, Kaiqiang Song, Xun Wang ç­‰ 14 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCM2æå‡ºä»¥â€œæ¸…å•å¼å¥–åŠ±â€æ›¿ä»£å¯éªŒè¯ç»“æœå¥–åŠ±ï¼Œåœ¨LLMæ¨¡æ‹Ÿå·¥å…·ç¯å¢ƒä¸­å¯¹å¤šè½®å¤šæ­¥éª¤å·¥å…·å‹ä»£ç†åšå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—ä¼˜äºSFTå¹¶è¾¾åˆ°/è¶…è¶ŠåŒè§„æ¨¡å¼€æºåŸºçº¿ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°å®å¤šè½®å·¥å…·ä½¿ç”¨ä»»åŠ¡ç¼ºä¹å¯éªŒè¯å¥–åŠ±ä¸”è¯„åˆ¤å¼€æ”¾ã€æ˜“ä¸ç¨³å®šï¼Œæ­å»ºå¯æ‰§è¡Œå·¥å…·ç¯å¢ƒæˆæœ¬é«˜ã€é™åˆ¶è§„æ¨¡ï¼›éœ€è¦ä¸€ç§å¯æ‰©å±•ä¸”ç¨³å®šçš„RLä¼˜åŒ–æ–¹å¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†æ¯è½®æ„å›¾åˆ†è§£ä¸ºç»†ç²’åº¦äºŒå…ƒæ ‡å‡†å¹¶è¦æ±‚è¯æ®ä¸ç»“æ„åŒ–å…ƒæ•°æ®æ”¯æ’‘ï¼Œç”¨ç¨€ç–å¥–åŠ±ä½†å¯†é›†è¯„ä¼°æ ‡å‡†æŠŠå¼€æ”¾å¼è¯„åˆ¤è½¬ä¸ºæ›´ç¨³çš„åˆ†ç±»å¼å†³ç­–ï¼›åœ¨LLMæ¨¡æ‹Ÿçš„å·¥å…·ç¯å¢ƒä¸­è®­ç»ƒï¼Œèµ·å§‹äº8BåŸºåº§ã€8kæ¡RLæ•°æ®ã€‚

**ä¸»è¦ç»“è®º**ï¼šCM2ç›¸è¾ƒSFTåœ¨tau^-Benchæå‡8åˆ†ã€BFCL-V4æå‡10åˆ†ã€ToolSandboxæå‡12åˆ†ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡åŒè§„æ¨¡å¼€æºåŸºçº¿ï¼ˆå«åˆ¤åˆ«æ¨¡å‹ï¼‰ï¼›ä¸ºæ— éœ€å¯éªŒè¯ç»“æœå¥–åŠ±çš„å¤šè½®å¤šæ­¥éª¤å·¥å…·ä»£ç†æä¾›äº†å¯æ‰©å±•çš„ä¼˜åŒ–èŒƒå¼ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, CM2, Reinforcement, Learning, Checklist, Rewards, Multi-Turn, Multi-Step

**è¯„åˆ†**ï¼š55

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12268v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12268v1.pdf)

---

## [2. Think like a Scientist: Physics-guided LLM Agent for Equation Discovery](https://arxiv.org/abs/2602.12259v1)

**ä½œè€…**ï¼šJianke Yang, Ohm Venkatachalam, Mohammad Kianezhad ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šKeplerAgentæ˜¯ä¸€ä¸ªéµå¾ªç§‘å­¦æ¨ç†æµç¨‹çš„ç‰©ç†å¼•å¯¼LLMä»£ç†ï¼Œå…ˆæå–ç‰©ç†æ€§è´¨å†çº¦æŸç¬¦å·å›å½’ï¼Œä»è€Œæ›´å‡†ç¡®ä¸”æ›´æŠ—å™ªåœ°å‘ç°è§£é‡Šç°è±¡çš„æ–¹ç¨‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰LLMå¤šç›´æ¥ä»æ•°æ®çŒœå…¬å¼ï¼Œæœªæ˜¾å¼å»ºæ¨¡ç§‘å­¦å®¶å¸¸ç”¨çš„å¤šæ­¥æ¨ç†ä¸ç‰©ç†å…ˆéªŒï¼ˆå¦‚å¯¹ç§°æ€§ï¼‰ï¼Œå¯¼è‡´å‡†ç¡®æ€§ä¸ç¨³å¥æ€§ä¸è¶³ï¼›éœ€è¦æŠŠè¿™äº›å…ˆéªŒèå…¥æ–¹ç¨‹å‘ç°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»£ç†æ¡†æ¶åè°ƒç‰©ç†å·¥å…·æå–ä¸­é—´ç»“æ„ï¼ˆå¦‚å¯¹ç§°æ€§ã€å®ˆæ’é‡ï¼‰ï¼Œå¹¶æ®æ­¤é…ç½®PySINDyä¸PySRçš„å‡½æ•°åº“ä¸ç»“æ„çº¦æŸï¼Œé€æ­¥æ”¶ç¼©å€™é€‰ç©ºé—´ä»¥è¿›è¡Œç¬¦å·å›å½’ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§ç‰©ç†æ–¹ç¨‹åŸºå‡†ä¸Šï¼ŒKeplerAgentçš„ç¬¦å·å‡†ç¡®ç‡æ˜¾è‘—æå‡ä¸”å¯¹å™ªå£°æ›´é²æ£’ï¼Œä¼˜äºçº¯LLMæ–¹æ³•å’Œä¼ ç»Ÿç¬¦å·å›å½’åŸºçº¿ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, Think, like, Scientist, Physics-guided, LLM, Agent, Equation

**è¯„åˆ†**ï¼š51

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12259v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12259v1.pdf)

---

## [3. "Sorry, I Didn't Catch That": How Speech Models Miss What Matters Most](https://arxiv.org/abs/2602.12249v1)

**ä½œè€…**ï¼šKaitlyn Zhou, Martijn Bartelds, Federico Bianchi ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL, cs.CY  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šå•†ä¸šè¯­éŸ³è¯†åˆ«åœ¨è¡—é“åç­‰çŸ­è€Œé«˜é£é™©è¯­å¥ä¸Šå¤§å¹…å¤±æ•ˆï¼ˆå¹³å‡é”™è¯¯ç‡44%ï¼‰ï¼Œä½†ç”¨å°‘é‡åˆæˆå¤šæ ·å‘éŸ³æ•°æ®å¾®è°ƒå¯æ˜¾è‘—é™ä½é”™è¯¯å¹¶å‡å°‘ä¸å…¬å¹³ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåŸºå‡†æµ‹è¯•çš„ä½WERæ©ç›–äº†çœŸå®åœºæ™¯ä¸­å¯¹å¯¼èˆªç­‰å…³é”®ä»»åŠ¡è‡³å…³é‡è¦çš„çŸ­è¯­å¥è½¬å†™å¤±è´¥ï¼Œä¸”è¿™äº›å¤±è´¥å¯¹éè‹±è¯­æ¯è¯­ä½¿ç”¨è€…ä¼¤å®³æ›´å¤§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ”¶é›†å¤šè¯­è¨€èƒŒæ™¯çš„ç¾å›½è¯´è¯äººæœ—è¯»ç¾å›½è¡—é“åï¼Œè¯„æµ‹æ¥è‡ªOpenAIã€Deepgramã€Googleã€Microsoftçš„15ä¸ªASRï¼›é‡åŒ–è½¬å†™é”™è¯¯åŠç”±æ­¤å¯¼è‡´çš„åœ°ç†è·¯ç”±åå·®ï¼›ç”¨å¼€æºTTSç”Ÿæˆå…·æœ‰å¤šæ ·å‘éŸ³çš„ä¸“ååˆæˆæ•°æ®ï¼Œç”¨ä¸è¶³1000æ¡æ ·æœ¬å¯¹æ¨¡å‹å¾®è°ƒå¹¶è¯„ä¼°å¢ç›Šã€‚

**ä¸»è¦ç»“è®º**ï¼šæ‰€æœ‰ç¾¤ä½“å‡å—è¯¯è½¬å†™å½±å“ï¼Œä½†éè‹±è¯­æ¯è¯­è€…çš„è·¯ç”±è·ç¦»è¯¯å·®çº¦ä¸ºè‹±è¯­æ¯è¯­è€…çš„ä¸¤å€ï¼›é€šè¿‡å°‘é‡åˆæˆæ•°æ®å¾®è°ƒä½¿éè‹±è¯­æ¯è¯­è€…çš„è¡—é“åè¯†åˆ«ç›¸å¯¹æå‡è¿‘60%ï¼Œå‡¸æ˜¾åŸºå‡†ä¸å®ç”¨å¯é æ€§é—´çš„é¸¿æ²Ÿå¹¶æä¾›ç®€å•å¯æ‰©å±•çš„ç¼“è§£è·¯å¾„ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, "Sorry, Didn't, Catch, That", How, Speech, Models

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12249v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12249v1.pdf)

---

## cs.CV

## [4. Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching](https://arxiv.org/abs/2602.12280v1)

**ä½œè€…**ï¼šHuai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºâ€œStroke of Surpriseâ€ï¼Œé€šè¿‡åºåˆ—æ„ŸçŸ¥çš„è”åˆä¼˜åŒ–è®©åŒä¸€çŸ¢é‡è‰å›¾åœ¨é€æ­¥æ·»åŠ ç¬”ç”»æ—¶ä»æ¦‚å¿µAå¹³æ»‘è½¬åŒ–ä¸ºæ¦‚å¿µBï¼Œå¹¶ç”¨Overlay Losså¢å¼ºç»“æ„æ•´åˆä¸å¹»è§‰æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿè§†è§‰é”™è§‰å¤šä¾èµ–ç©ºé—´æ“æ§ï¼Œéš¾ä»¥å®ç°éšç»˜åˆ¶è¿›ç¨‹æ”¹å˜è¯­ä¹‰çš„è‰å›¾ï¼›ä½œè€…å¸Œæœ›å‰ç¼€ç¬”ç”»æ—¢èƒ½æ„æˆå¯¹è±¡Aï¼Œåˆä¸ºåŠ å…¥å¢é‡ç¬”ç”»åç”Ÿæˆå¯¹è±¡Bæä¾›ç»“æ„åŸºç¡€ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé‡‡ç”¨åºåˆ—æ„ŸçŸ¥çš„è”åˆä¼˜åŒ–æ¡†æ¶ä¸åŒåˆ†æ”¯SDSï¼ŒåŒæ—¶ä¼˜åŒ–å‰ç¼€ä¸å¢é‡ç¬”ç”»ä»¥æ»¡è¶³ä¸¤é˜¶æ®µè¯­ä¹‰ï¼ŒåŠ¨æ€è°ƒæ•´å‰ç¼€ä»¥å‘ç°ä¸¤ç›®æ ‡çš„å…±äº«ç»“æ„å­ç©ºé—´ï¼›å¼•å…¥Overlay Lossé¼“åŠ±ç©ºé—´äº’è¡¥ã€é¿å…é®æŒ¡ï¼Œå®ç°ç»“æ„èåˆã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¯è¯†åˆ«åº¦ä¸å¹»è§‰å¼ºåº¦ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼ŒæˆåŠŸå°†è§†è§‰â€œå˜ä½â€ä»ç©ºé—´æ‹“å±•åˆ°æ—¶é—´ç»´åº¦çš„é€ç¬”ç»˜åˆ¶è¿‡ç¨‹ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, of, in, Stroke, Surprise, Progressive, Semantic, Illusions

**è¯„åˆ†**ï¼š22

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12280v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12280v1.pdf)

---

## [5. UniT: Unified Multimodal Chain-of-Thought Test-time Scaling](https://arxiv.org/abs/2602.12279v1)

**ä½œè€…**ï¼šLeon Liangyu Chen, Haoyu Ma, Zhipeng Fan ç­‰ 14 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šUniTæå‡ºé¢å‘ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„é“¾å¼æ€ç»´æµ‹è¯•æ—¶æ‰©å±•æ¡†æ¶ï¼Œä½¿æ¨¡å‹åœ¨å¤šè½®ä¸­åˆ†è§£ã€éªŒè¯ä¸ä¿®æ­£ï¼Œæ˜¾è‘—æå‡ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹å¤šä¸ºå•æ¬¡å‰å‘ã€ç¼ºä¹è¿­ä»£æ¨ç†ä¸è‡ªæˆ‘æ ¡éªŒï¼Œè€Œå¤æ‚ç©ºé—´å…³ç³»ä¸å¤šå¯¹è±¡/åŠ¨æ€æŒ‡ä»¤ä»»åŠ¡éœ€è¦åˆ†è§£ä¸çº é”™ï¼›è¯­è¨€æ¨¡å‹çš„TTSå·²éªŒè¯æœ‰æ•ˆä½†å°šæœªæ‰©å±•åˆ°å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç»“åˆä»£ç†å¼æ•°æ®åˆæˆã€ç»Ÿä¸€æ¨¡å‹è®­ç»ƒä¸çµæ´»æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ï¼Œä¿ƒå‘éªŒè¯ã€å­ç›®æ ‡åˆ†è§£å’Œå†…å®¹è®°å¿†ï¼›é‡‡ç”¨é¡ºåºCoTè¿­ä»£å¹¶è®­ç»ƒç”Ÿæˆä¸ç¼–è¾‘è½¨è¿¹ï¼Œåœ¨æµ‹è¯•æ—¶åˆ†é…æ›´å¤šè®¡ç®—ä»¥å®ç°æ¨ç†ã€æ ¡éªŒä¸ç²¾ç‚¼ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»Ÿä¸€æ¨¡å‹åœ¨ä»…è®­ç»ƒçŸ­æ¨ç†è½¨è¿¹ä¸‹å¯äºæµ‹è¯•æ—¶æ¨å¹¿åˆ°æ›´é•¿æ¨ç†é“¾ï¼›é¡ºåºé“¾å¼æ¨ç†è¾ƒå¹¶è¡Œé‡‡æ ·æ›´å¯æ‰©å±•ä¸”æ›´çœç®—ï¼›è®­ç»ƒç”Ÿæˆä¸ç¼–è¾‘è½¨è¿¹æ˜¾è‘—æå‡åˆ†å¸ƒå¤–è§†è§‰æ¨ç†ï¼Œç¡®ç«‹å¤šæ¨¡æ€TTSä¸ºæœ‰æ•ˆèŒƒå¼ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, æ‘˜è¦, UniT, Unified, Multimodal, Chain-of-Thought, Test-time, Scaling

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12279v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12279v1.pdf)

---

## [6. MonarchRT: Efficient Attention for Real-Time Video Generation](https://arxiv.org/abs/2602.12271v1)

**ä½œè€…**ï¼šKrish Agarwal, Zhuoming Chen, Cheng Luo ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMonarch-RTæå‡ºåŸºäºMonarchçŸ©é˜µçš„ç»“æ„åŒ–æ³¨æ„åŠ›å‚æ•°åŒ–ï¼Œåœ¨å°‘æ­¥è‡ªå›å½’è§†é¢‘æ‰©æ•£ä¸­ä»¥é«˜è¾¾95%ç¨€ç–åº¦ä¿æŒç”Ÿæˆè´¨é‡ï¼Œå¹¶é€šè¿‡è‡ªç ”é«˜æ•ˆå†…æ ¸å®ç°å•å¡16 FPSå®æ—¶è§†é¢‘ç”Ÿæˆã€‚

**ç ”ç©¶åŠ¨æœº**ï¼š3Dè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦åœ¨å°‘æ­¥è‡ªå›å½’å®æ—¶è§†é¢‘ç”Ÿæˆä¸­æˆä¸ºç“¶é¢ˆï¼Œè€Œè§†é¢‘æ³¨æ„åŠ›å‘ˆç°æ—¶ç©ºå‘¨æœŸæ€§+åŠ¨æ€ç¨€ç–+è‡´å¯†æ··åˆçš„å¤åˆç»“æ„ï¼Œä½¿ä¼ ç»Ÿç¨€ç–/Top-kè¿‘ä¼¼åœ¨è¯¥è®¾å®šä¸‹å¤±æ•ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåˆ©ç”¨MonarchçŸ©é˜µå¯¹æ³¨æ„åŠ›è¿›è¡Œå› å¼åˆ†è§£ï¼Œè®¾è®¡å¯¹é½å—ç»“æ„ä¸æ‰©å±•çš„tiled Monarchå‚æ•°åŒ–ä»¥åŒæ—¶è¡¨è¾¾å‘¨æœŸæ€§æ—¶ç©ºç»“æ„ã€åŠ¨æ€è¯­ä¹‰å¯¹åº”ä¸è‡´å¯†æ··åˆï¼›ç»“åˆå¾®è°ƒä¸Tritonè‡ªå®šä¹‰å†…æ ¸ï¼Œé™ä½å‚æ•°åŒ–å¼€é”€å¹¶æå‡æ¨ç†é€Ÿåº¦ã€‚

**ä¸»è¦ç»“è®º**ï¼šç›¸è¾ƒé¢å‘åŒå‘å¤šæ­¥æ‰©æ•£çš„ç¨€ç–åŸºçº¿ï¼ŒMonarch-RTåœ¨Self-Forcingä¸Šä»¥æœ€é«˜95%ç¨€ç–åº¦æ— è´¨é‡æŸå¤±ï¼Œå¹¶åœ¨RTX 5090/H100/B200ä¸Šè¾ƒFlashAttention-2/3/4å–å¾—1.4â€“11.8å€åŠ é€Ÿï¼Œé¦–æ¬¡å®ç°å•å¼ RTX 5090çš„16 FPSå®æ—¶è§†é¢‘ç”Ÿæˆã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, æ‘˜è¦, MonarchRT, Efficient, Attention, Real-Time, Video, Generation

**è¯„åˆ†**ï¼š33

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12271v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12271v1.pdf)

---

## cs.LG

## [7. Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage](https://arxiv.org/abs/2602.12274v1)

**ä½œè€…**ï¼šXin Ju, Jiachen Yao, Anima Anandkumar ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, physics.geo-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºFun-DDPSï¼Œå°†å‡½æ•°ç©ºé—´æ‰©æ•£å…ˆéªŒä¸å±€éƒ¨ç¥ç»ç®—å­ï¼ˆLNOï¼‰è§£è€¦ç»“åˆï¼Œå®ç°CCSå‰å‘ä¸åæ¼”ä¸­åœ¨ç¨€ç–è§‚æµ‹ä¸‹çš„ç‰©ç†ä¸€è‡´ç”Ÿæˆä¸é«˜æ•ˆæ•°æ®åŒåŒ–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šCCSåœ°ä¸‹æµåŠ¨åæ¼”ç—…æ€ä¸”è§‚æµ‹ç¨€ç–ï¼Œä¼ ç»Ÿç¡®å®šæ€§ä»£ç†åœ¨æç«¯ç¨€ç–ä¸‹å¤±æ•ˆï¼ŒäºŸéœ€æ—¢èƒ½è¡¥å…¨å‚æ•°ä¿¡æ¯åˆä¿æŒç‰©ç†ä¸€è‡´æ€§çš„ç”Ÿæˆå¼æ–¹æ³•ï¼Œå¹¶å¯¹å…¶åéªŒè¿›è¡Œä¸¥æ ¼éªŒè¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç”¨å•é€šé“å‡½æ•°ç©ºé—´æ‰©æ•£æ¨¡å‹å­¦ä¹ åœ°è´¨å‚æ•°å…ˆéªŒï¼Œå€ŸåŠ©å¯å¾®çš„å±€éƒ¨ç¥ç»ç®—å­æä¾›è·¨åœºæ¡ä»¶ä¸ç‰©ç†ä¸€è‡´çš„æ¢¯åº¦å¼•å¯¼ï¼›è§£è€¦è®¾è®¡ä½¿æ‰©æ•£å…ˆéªŒè¡¥å…¨ç¼ºå¤±å‚æ•°ï¼ŒLNOé«˜æ•ˆæ‰§è¡Œæ•°æ®åŒåŒ–ä¸æŒ‡å¯¼é‡‡æ ·ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ä»…25%è§‚æµ‹ä¸‹ï¼Œå‰å‘å»ºæ¨¡ç›¸å¯¹è¯¯å·®ä¸º7.7%ï¼Œæ˜¾è‘—ä¼˜äºæ ‡å‡†ä»£ç†çš„86.9%ï¼ˆçº¦11å€æå‡ï¼‰ã€‚åæ¼”ä¸­ç›¸å¯¹æ‹’ç»é‡‡æ ·åéªŒçš„JSæ•£åº¦<0.06ï¼Œæ ·æœ¬æ•ˆç‡æå‡4å€ï¼Œå¹¶ç”Ÿæˆæ— è”åˆçŠ¶æ€åŸºçº¿ï¼ˆFun-DPSï¼‰é«˜é¢‘ä¼ªå½±çš„ç‰©ç†ä¸€è‡´è§£ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, Function-Space, Decoupled, Diffusion, Forward, Inverse, Modeling, in

**è¯„åˆ†**ï¼š28

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12274v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12274v1.pdf)

---

## [8. Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data](https://arxiv.org/abs/2602.12267v1)

**ä½œè€…**ï¼šDuy Nguyen, Jiachen Yao, Jiayun Wang ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO's robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºFlow-Guided Neural Operatorï¼ˆFGNOï¼‰ï¼Œå°†ç®—å­å­¦ä¹ ä¸flow matchingç»“åˆï¼ŒæŠŠå™ªå£°/è…èš€å¼ºåº¦ä½œä¸ºè‡ªç›‘ç£è‡ªç”±åº¦ï¼Œè®­ç»ƒæ—¶å¤šå±‚æ¬¡å™ªå£°å­¦ä¹ ã€æ¨ç†ç”¨å¹²å‡€è¾“å…¥ï¼Œåœ¨å¤šé¡¹ç”Ÿç‰©åŒ»å­¦æ—¶åºä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶ŠåŸºçº¿ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ—¶åºSSLï¼ˆå¦‚MAEï¼‰ä¾èµ–å›ºå®šé®ç›–æ¯”ä¾‹ã€éš¾ä»¥é€‚é…å¤šæ—¶é—´å°ºåº¦ä¸ä»»åŠ¡éœ€æ±‚ï¼Œä¸”æ¨ç†å«å™ªé€ æˆéšæœºæ€§ä¸æ€§èƒ½æŸå¤±ã€‚éœ€è¦ä¸€ç§èƒ½è·¨å°ºåº¦æå–å±‚æ¬¡åŒ–è¡¨ç¤ºã€åœ¨å°æ ·æœ¬ä¸‹ä»ç¨³å¥ä¸”æ¨ç†ç¨³å®šçš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºFGNOåœ¨å‡½æ•°ç©ºé—´ä¸­å­¦ä¹ æ˜ å°„ï¼Œä½¿ç”¨STFTç»Ÿä¸€æ—¶é—´åˆ†è¾¨ç‡ï¼Œå¹¶ä»¥flow matchingæ³¨å…¥å¯æ§å™ªå£°ï¼›ä»ä¸åŒç½‘ç»œå±‚ä¸ä¸åŒflowæ—¶é—´èšåˆå¤šç²’åº¦ç‰¹å¾ã€‚è®­ç»ƒé˜¶æ®µç”¨å¸¦å™ªæ ·æœ¬ä¿ƒè¿›è¡¨ç¤ºå­¦ä¹ ï¼Œæ¨ç†é˜¶æ®µæ”¹ç”¨å¹²å‡€è¾“å…¥æå–è¡¨ç¤ºä»¥æ¶ˆé™¤éšæœºæ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šFGNOåœ¨BrainTreeBankã€DREAMTå’ŒSleepEDFä¸Šåˆ†åˆ«å®ç°æœ€é«˜35% AUROCæå‡ã€16% RMSEé™ä½åŠä½æ•°æ®åœºæ™¯ä¸‹>20%å‡†ç¡®ç‡ä¸å®F1æå‡ï¼Œå±•ç°å‡ºå¯¹æ•°æ®ç¨€ç¼ºçš„é²æ£’æ€§å’Œå¯¹å¤šæ ·æ—¶åºä»»åŠ¡çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, Self-Supervised, Learning, via, Flow-Guided, Neural, Operator, on

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12267v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12267v1.pdf)

---

## [9. Community Concealment from Unsupervised Graph Learning-Based Clustering](https://arxiv.org/abs/2602.12250v1)

**ä½œè€…**ï¼šDalyapraz Manatova, Pablo Moriano, L. Jean Camp  
**åˆ†ç±»**ï¼šcs.LG, cs.CR, cs.SI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§åœ¨æœ‰é™é¢„ç®—ä¸‹é€šè¿‡æ”¹è¾¹ä¸ç‰¹å¾ä¿®æ”¹æ¥é™ä½GNNæ— ç›‘ç£èšç±»å¯è¯†åˆ«æ€§ï¼Œä»è€ŒéšåŒ¿ç›®æ ‡ç¤¾åŒºçš„æ–¹æ³•ï¼Œå®éªŒè¯æ˜ä¼˜äºDICEå¹¶æå‡éšåŒ¿æ•ˆæœçº¦20-45%ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šGNNé©±åŠ¨çš„æ— ç›‘ç£ç¤¾åŒºæ£€æµ‹å¯èƒ½æš´éœ²ç¤¾ä¼šæˆ–åŸºç¡€è®¾æ–½ç½‘ç»œä¸­çš„æ•æ„Ÿç¾¤ä½“ç»“æ„ï¼ŒäºŸéœ€åœ¨ä¿ç•™æ•°æ®æ•ˆç”¨çš„å‰æä¸‹å®ç°ç¾¤ä½“çº§éšç§é˜²æŠ¤ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåˆ†æå¹¶é‡åŒ–å½±å“éšåŒ¿çš„ä¸¤å¤§å› ç´ â€”â€”ç¤¾åŒºè¾¹ç•Œè¿é€šæ€§ä¸ä¸é‚»è¿‘ç¤¾åŒºçš„ç‰¹å¾ç›¸ä¼¼åº¦ï¼›æ®æ­¤åœ¨æ‰°åŠ¨é¢„ç®—å†…é€‰æ‹©æ€§é‡è¿è¾¹å¹¶ä¿®æ”¹èŠ‚ç‚¹ç‰¹å¾ï¼Œå‰Šå¼±GNNæ¶ˆæ¯ä¼ é€’æ‰€ä¾èµ–çš„å¯åŒºåˆ†æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ‰€æç­–ç•¥åœ¨åˆæˆä¸çœŸå®ç½‘ç»œä¸Šå‡æ˜¾è‘—æå‡ç¤¾åŒºéšåŒ¿ï¼ˆç›¸å¯¹æå‡çº¦20-45%ï¼‰ï¼Œä¼˜äºDICEï¼Œè¡¨æ˜å¯è¡Œçš„GNNç¤¾åŒºå­¦ä¹ å¯¹æŠ—æ–¹æ¡ˆå¹¶æ­ç¤ºå›¾å­¦ä¹ å†…åœ¨çš„ç¾¤ä½“éšç§é£é™©ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, æ‘˜è¦, Community, Concealment, Unsupervised, Graph, Learning-Based, Clustering

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12250v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12250v1.pdf)

---

## [10. ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction](https://arxiv.org/abs/2602.12247v1)

**ä½œè€…**ï¼šNick Ferguson, Josh Pennington, Narek Beghian ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºExtractBenchï¼Œä¸€ä¸ªç”¨äºPDFåˆ°JSONå¤æ‚ç»“æ„åŒ–æŠ½å–çš„å¼€æºåŸºå‡†ä¸å¯æ‰§è¡Œè¯„ä¼°æ–¹æ³•ï¼Œæ˜¾ç¤ºå‰æ²¿LLMåœ¨ä¼ä¸šçº§å®½æ¨¡å¼ä¸‹å¯é æ€§æ˜æ˜¾ä¸è¶³ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç¼ºä¹è¦†ç›–ä¼ä¸šçº§æ¨¡å¼å¹¿åº¦çš„ç«¯åˆ°ç«¯PDFâ†’JSONåŸºå‡†ï¼Œä»¥åŠèƒ½åˆ»ç”»åµŒå¥—æŠ½å–ä¸­å¤šæ ·æ­£ç¡®æ€§æ ‡å‡†ï¼ˆç²¾ç¡®åŒ¹é…ã€æ•°é‡å®¹å·®ã€è¯­ä¹‰ç­‰ä»·ã€æ•°ç»„å¯¹é½ã€åŒºåˆ†æ¼æŠ¥ä¸å¹»è§‰ï¼‰çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯¼è‡´è¿›å±•å—é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºåŒ…å«35ä»½PDFã€é…å¥—JSON Schemaä¸äººå·¥é‡‘æ ‡çš„ExtractBenchï¼ˆå…±12,867ä¸ªå¯è¯„ä¼°å­—æ®µï¼‰ã€‚å°†Schemaè§†ä¸ºå¯æ‰§è¡Œè§„èŒƒï¼šæ¯ä¸ªå­—æ®µå£°æ˜å…¶è¯„åˆ†åº¦é‡ï¼Œè¦†ç›–æ ‡è¯†ç¬¦ç²¾ç¡®åŒ¹é…ã€æ•°é‡å®¹å·®ã€åç§°è¯­ä¹‰ç­‰ä»·ã€æ•°ç»„å¯¹é½åŠæ¼æŠ¥/å¹»è§‰è¯†åˆ«ï¼Œå¹¶æä¾›å¤šæ¨¡å‹åŸºçº¿è¯„æµ‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šå‰æ²¿æ¨¡å‹ï¼ˆGPT-5/5.2ã€Gemini-3 Flash/Proã€Claude 4.5 Opus/Sonnetï¼‰åœ¨ç°å®å¤æ‚Schemaä¸Šä¸å¯é ï¼Œæ€§èƒ½éšSchemaæ‰©å±•æ€¥å‰§ä¸‹é™ï¼›åœ¨369å­—æ®µçš„è´¢åŠ¡æŠ¥å‘ŠSchemaä¸Šæ‰€æœ‰æ¨¡å‹å‡äº§ç”Ÿ0%æœ‰æ•ˆè¾“å‡ºã€‚ExtractBenchæä¾›ç»Ÿä¸€æ•°æ®ä¸ä¸¥è°¨è¯„ä¼°æ¡†æ¶ï¼Œä¿ƒè¿›è¯¥æ–¹å‘çš„å¯é æ€§ç ”ç©¶ä¸ç³»ç»Ÿæ”¹è¿›ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, ExtractBench, Benchmark, Evaluation, Methodology, Complex, Structured, Extraction

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12247v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12247v1.pdf)

---

## [11. Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces](https://arxiv.org/abs/2602.12245v1)

**ä½œè€…**ï¼šAnthony Kobanda, Waris Radji  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡å°†JEPAçš„â€œå†…åœ¨ï¼ˆæœ€å°è¡ŒåŠ¨ï¼‰èƒ½é‡â€ä¸QRLçš„å®šå‘ä»£ä»·åˆ°è¾¾å‡½æ•°å»ºç«‹ç­‰ä»·è”ç³»ï¼Œè¯æ˜å…¶åœ¨æ½œç©ºé—´ä¸­è¯±å¯¼æ‹Ÿåº¦é‡ï¼Œå¹¶å¼ºè°ƒéå¯¹ç§°èƒ½é‡æ›´é€‚é…ä¸€æ–¹å‘å¯è¾¾æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¯¹ç§°å…¼å®¹èƒ½é‡éš¾ä»¥è¡¨è¾¾å•å‘å¯è¾¾æ€§ä¸æ–¹å‘æ€§åŠ¨æ€ï¼Œä½œè€…å¸Œæœ›ç”¨ç»Ÿä¸€çš„èƒ½é‡-è·ç¦»è§†è§’æŠŠJEPAçš„è¡¨ç¤ºå­¦ä¹ ä¸QRLçš„ç›®æ ‡é©±åŠ¨æ§åˆ¶å¯¹é½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå®šä¹‰å†…åœ¨èƒ½é‡ä¸ºä¸¤çŠ¶æ€é—´å¯è¡Œè½¨è¿¹ä¸Šç´¯è®¡å±€éƒ¨åŠªåŠ›çš„ä¸‹ç¡®ç•Œï¼Œåœ¨æ¸©å’Œçš„é—­åˆä¸å¯åŠ æ€§æ¡ä»¶ä¸‹è¯æ˜å…¶ä¸ºæ‹Ÿåº¦é‡ï¼›åŒæ—¶è¯æ˜æœ€ä¼˜cost-to-goå…·æœ‰ç›¸åŒå†…åœ¨å½¢å¼ï¼Œå¹¶å°†JEPAè®­ç»ƒç›®æ ‡å¯¹å‡†è¯¥èƒ½é‡ç±»ã€‚

**ä¸»è¦ç»“è®º**ï¼šç”¨å†…åœ¨èƒ½é‡è®­ç»ƒçš„JEPAä¼šåœ¨æ½œç©ºé—´è¯±å¯¼æ‹Ÿåº¦é‡ï¼Œä¸ç›®æ ‡è¾¾æˆæ§åˆ¶çš„ä»·å€¼å‡½æ•°ä¸€è‡´ï¼›å¯¹ç§°æœ‰é™èƒ½é‡ä¸å•å‘å¯è¾¾æ€§ç»“æ„ä¸åŒ¹é…ï¼Œæ–¹å‘æ€§ä»»åŠ¡åº”é‡‡ç”¨éå¯¹ç§°ï¼ˆæ‹Ÿåº¦é‡ï¼‰èƒ½é‡ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, Intrinsic-Energy, Joint, Embedding, Predictive, Architectures, Induce, Quasimetric

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12245v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12245v1.pdf)

---

## [12. Olmix: A Framework for Data Mixing Throughout LM Development](https://arxiv.org/abs/2602.12237v1)

**ä½œè€…**ï¼šMayee F. Chen, Tyler Murray, David Heineman ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šOlmixæå‡ºä¸€ä¸ªç”¨äºè¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ•°æ®æ··åˆæ¡†æ¶ï¼Œé€šè¿‡ç³»ç»ŸåŒ–è®¾è®¡ä¸â€œæ··åˆé‡ç”¨â€æœºåˆ¶åœ¨åŸŸé›†åˆæ¼”å˜ä¸­ç»´æŒæ€§èƒ½å¹¶æ˜¾è‘—èŠ‚çœè®¡ç®—ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ··åˆæ–¹æ³•ç¼ºä¹å¯¹è®¾è®¡é€‰æ‹©ä¸æ•°æ®çº¦æŸçš„ç³»ç»Ÿç†è§£ï¼Œä¸”å¸¸å‡è®¾åŸŸé›†åˆå›ºå®šï¼›ç°å®å¼€å‘ä¸­æ•°æ®é›†ä¼šå¢åˆ ã€åˆ†åŒºä¸ä¿®è®¢ï¼ŒäºŸéœ€èƒ½éšåŸŸæ¼”å˜é«˜æ•ˆæ›´æ–°æ··åˆæ¯”çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè¿›è¡Œå…¨é¢å®è¯ç ”ç©¶ä»¥æ¢³ç†æ··åˆæ–¹æ³•çš„é…ç½®ç©ºé—´ä¸æœ‰æ•ˆè®¾è®¡é€‰æ‹©ï¼›æå‡ºâ€œæ··åˆé‡ç”¨â€æœºåˆ¶ï¼Œå¤ç”¨æ—¢æœ‰æ¯”ä¾‹ã€ä»…å¯¹å—å½±å“åŸŸé‡ç®—ï¼Œå¹¶åœ¨äº”æ¬¡è´´è¿‘çœŸå®çš„åŸŸé›†åˆæ›´æ–°åºåˆ—ä¸Šè¯„æµ‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ··åˆé‡ç”¨åœ¨ä¿æŒä¸æ¯æ¬¡å®Œå…¨é‡ç®—ç›¸å½“æ€§èƒ½çš„åŒæ—¶å‡å°‘74%è®¡ç®—ï¼Œå¹¶è¾ƒæ— æ··åˆè®­ç»ƒåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæå‡11.6%ï¼›è¯¥æ¡†æ¶ä¸ºå®ç”¨åœºæ™¯ä¸‹å¼ºæ•°æ®æ··åˆæ–¹æ³•çš„è®¾è®¡ä¸è¿­ä»£æä¾›ä¾æ®ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, LM, Olmix, Framework, Data, Mixing, Throughout, Development

**è¯„åˆ†**ï¼š22

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12237v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12237v1.pdf)

---

## [13. Categorical Flow Maps](https://arxiv.org/abs/2602.12233v1)

**ä½œè€…**ï¼šDaan Roos, Oscar Davis, Floor Eijkelboom ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡º Categorical Flow Mapsï¼Œç”¨è¿ç»­æµåŒ¹é…ä¸è‡ªè’¸é¦åŠ é€Ÿç±»åˆ«æ•°æ®çš„å°‘æ­¥ï¼ˆç”šè‡³å•æ­¥ï¼‰ç”Ÿæˆï¼Œå¹¶åœ¨å›¾åƒã€åˆ†å­å›¾å’Œæ–‡æœ¬ä¸Šè¾¾æˆSOTAã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç¦»æ•£/ç±»åˆ«æ•°æ®ç”Ÿæˆç¼ºä¹å¯ç”¨äºè’¸é¦ä¸åŠ é€Ÿæ¨ç†çš„è¿ç»­è½¨è¿¹ï¼Œä¸”éœ€è¦åœ¨æ¦‚ç‡å•çº¯å½¢ä¸Šè‡ªç„¶çº¦æŸè¾“å‡ºå¹¶å¤ç”¨æ‰©æ•£/æµæ¨¡å‹ä¸­çš„å¼•å¯¼ä¸é‡åŠ æƒä»¥æå‡ä¸‹æ¸¸ç›®æ ‡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå®šä¹‰æœå‘å•çº¯å½¢çš„æµæ˜ å°„ï¼Œå°†æ¦‚ç‡è´¨é‡è¿è¾“åˆ°é¢„æµ‹ç»ˆç‚¹ï¼Œå®ç°å—çº¦æŸçš„å‚æ•°åŒ–ï¼›è®­ç»ƒç»“åˆç°æœ‰è‡ªè’¸é¦æŠ€æœ¯å¹¶æå‡ºç»ˆç‚¹ä¸€è‡´æ€§ç›®æ ‡ï¼Œè¿ç»­è¡¨è¿°ä½¿å¾—æµ‹è¯•æ—¶å¯ç›´æ¥åº”ç”¨å¼•å¯¼ä¸é‡åŠ æƒä»¥æ§åˆ¶é‡‡æ ·ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å›¾åƒã€åˆ†å­å›¾ä¸æ–‡æœ¬ä»»åŠ¡ä¸Šå–å¾—æœ€ä¼˜çš„å°‘æ­¥ç”Ÿæˆç»“æœï¼Œå•æ­¥äº¦å…·å¼ºæ€§èƒ½ï¼›æ–¹æ³•å…¼å…·é€Ÿåº¦ä¸å¯æ§æ€§ï¼Œä¸ºç±»åˆ«æ•°æ®çš„åŠ é€Ÿç”Ÿæˆæä¾›é€šç”¨æ–¹æ¡ˆã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, æ‘˜è¦, We, Categorical, Flow, Maps, introduce, flow-matching

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12233v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12233v1.pdf)

---

## [14. Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser](https://arxiv.org/abs/2602.12229v1)

**ä½œè€…**ï¼šZijing Ou, Jacob Si, Junyi Zhu ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºVMPOï¼Œå°†æ‰©æ•£å¯¹é½ä»KLä¼˜åŒ–è½¬ä¸ºæœ€å°åŒ–å¯¹æ•°é‡è¦æ€§æƒé‡çš„æ–¹å·®ï¼Œåœ¨on-policyé‡‡æ ·ä¸‹ä¸KLå¯¹é½æ¢¯åº¦ä¸€è‡´ï¼Œå¹¶ä¸ºå¥–åŠ±å€¾æ–œé‡‡æ ·æä¾›ç»Ÿä¸€ä¸”æ›´çµæ´»çš„è§†è§’ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä»SMCè§†è§’çœ‹ï¼Œå¥–åŠ±å¼•å¯¼å½¢æˆé‡è¦æ€§æƒé‡ï¼Œç›´æ¥é™ä½æƒé‡æ–¹å·®å¯æ›´å¥½é€¼è¿‘ç›®æ ‡åˆ†å¸ƒå¹¶å¯èƒ½å¸¦æ¥æ›´ç¨³å®šçš„ä¼˜åŒ–ï¼›å¸Œæœ›ç”¨ç»Ÿä¸€æ¡†æ¶ç†è§£å¹¶æ‹“å±•æ‰©æ•£å¯¹é½æ–¹æ³•ï¼Œæ‘†è„±å¯¹KLçš„ä¾èµ–ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæŠŠæ‰©æ•£å¯¹é½å»ºæ¨¡ä¸ºæ²¿å»å™ªè½¨è¿¹çš„SMCè¿‡ç¨‹ï¼Œä»¥å¥–åŠ±å€¾æ–œåˆ†å¸ƒä¸ºç›®æ ‡ï¼Œæå‡ºæœ€å°åŒ–logé‡è¦æ€§æƒé‡æ–¹å·®çš„VMPOï¼›è¯æ˜è¯¥ç›®æ ‡åœ¨ç›®æ ‡åˆ†å¸ƒå¤„å–å¾—æœ€å°å€¼ï¼Œä¸”åœ¨on-policyé‡‡æ ·æ—¶å…¶æ¢¯åº¦ç­‰åŒäºæ ‡å‡†KLå¯¹é½ï¼Œå¹¶é€šè¿‡ä¸åŒæ½œèƒ½/æ–¹å·®ç­–ç•¥å¤ç°å¹¶æ‹“å±•æ—¢æœ‰æ–¹æ³•ã€‚

**ä¸»è¦ç»“è®º**ï¼šVMPOä¸ºæ‰©æ•£å¯¹é½æä¾›äº†æœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–å™¨å’Œç»Ÿä¸€ç†è®ºè§†è§’ï¼Œæ—¢èƒ½è§£é‡Šå¹¶æ¶µç›–ç°æœ‰æ–¹æ³•ï¼ŒåˆæŒ‡å‘è¶…è¶ŠKLçš„æ–°çš„è®¾è®¡æ–¹å‘ï¼›å…¶ä¸å¥–åŠ±å€¾æ–œç›®æ ‡ä¸€è‡´ä¸”åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¸KLæ¢¯åº¦ç­‰ä»·ã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, Diffusion, KL, as, Alignment, Beyond, Variance, Minimisation

**è¯„åˆ†**ï¼š19

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12229v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12229v1.pdf)

---

## [15. Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222v1)

**ä½œè€…**ï¼šMiaosen Zhang, Yishan Liu, Shuxia Lin ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-12

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºåˆ†å¸ƒåˆ¤åˆ«ç†è®ºï¼ˆDDTï¼‰åŠä¸¤é¡¹æŠ€æœ¯ï¼ˆIDFTä¸Hinted Decodingï¼‰ï¼Œå®ç°è¿‘ä¼¼â€œon-policyâ€çš„SFTï¼Œåœ¨ä¿æŒSFTé«˜æ•ˆæ€§çš„åŒæ—¶å®ç°æ¥è¿‘DPO/SimPOçš„æ³›åŒ–è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»ŸSFTå°½ç®¡é«˜æ•ˆï¼Œä½†å› ç¼ºä¹on-policyæ•°æ®è€Œåœ¨æ³›åŒ–ä¸Šè½åäºRLï¼›ä¸ºé™ä½RLæˆæœ¬å¹¶å¼¥è¡¥æ³›åŒ–å·®è·ï¼Œéœ€è®©SFTåœ¨ä¸å¼•å…¥RLå¤æ‚åº¦çš„å‰æä¸‹æ›´è´´è¿‘æ¨¡å‹è¯±å¯¼åˆ†å¸ƒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šDDTç”¨äºè§£é‡Šä¸é‡åŒ–è®­ç»ƒæ•°æ®ä¸æ¨¡å‹è¯±å¯¼åˆ†å¸ƒçš„å¯¹é½åº¦ï¼›æ®æ­¤æå‡ºæŸå¤±å±‚é¢çš„IDFTä»¥æå‡æ³›åŒ–ï¼Œä»¥åŠæ•°æ®å±‚é¢çš„Hinted Decodingä»¥é‡æ•´è¯­æ–™åˆ†å¸ƒï¼Œä»è€Œå°†äºŒè€…æ•´åˆåˆ°æ ‡å‡†SFTæµç¨‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜è¯¥æ¡†æ¶åœ¨æ³›åŒ–æ€§èƒ½ä¸Šå¯åª²ç¾ç¦»çº¿RLç®—æ³•ï¼ˆå¦‚DPOã€SimPOï¼‰ï¼ŒåŒæ—¶ä¿æŒSFTçš„è®¡ç®—æ•ˆç‡ï¼Œä¸ºRLä¸å¯è¡Œçš„åœºæ™¯æä¾›åˆ‡å®å¯ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚

**å…³é”®è¯**ï¼šæ ‡é¢˜, Towards, On-Policy, SFT, Distribution, Discriminant, Theory, its

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.12222v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.12222v1.pdf)

---

