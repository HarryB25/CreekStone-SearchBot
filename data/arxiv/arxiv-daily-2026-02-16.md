# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-16

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.AI](#csAI) (3 ç¯‡)
- [cs.LG](#csLG) (8 ç¯‡)
- [cs.CL](#csCL) (1 ç¯‡)
- [cs.CV](#csCV) (3 ç¯‡)

---

## cs.AI

## [1. Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248v1)

**ä½œè€…**ï¼šPavel Koptev, Vishnu Kumar, Konstantin Malkov ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, math.OC, q-fin.MF  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ä¸ªç»“åˆâ€œæ— æ³„æ¼â€çš„ä¸¤é˜¶æ®µXGBoostã€KANä¸é›†æˆæ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨ç”Ÿäº§çº§äº¤æ˜“å­—æ®µå®æ—¶é¢„æµ‹ä¹°æ–¹-ä¾›åº”å•†ç»´åº¦çš„å‘ç¥¨ç¨€é‡Šï¼ˆå®é™…å›æ¬¾ä½äºæ ¸å‡†é‡‘é¢ï¼‰ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå‘ç¥¨ç¨€é‡Šä¼šå¸¦æ¥æ˜¾è‘—çš„éä¿¡ç”¨é£é™©ä¸åˆ©æ¶¦æŸå¤±ï¼Œè€Œä¼ ç»Ÿä¾èµ–ä¹°æ–¹ä¸å¯æ’¤é”€ä»˜æ¬¾æ‰¿è¯ºï¼ˆIPUï¼‰çš„æ–¹å¼ä¼šæŠ¬é«˜å‡†å…¥é—¨æ§›ã€é˜»ç¢ä¾›åº”é“¾é‡‘èåœ¨éæŠ•èµ„çº§ä¹°æ–¹ä¸­çš„æ¨å¹¿ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåŸºäºè¦†ç›–9ä¸ªå…³é”®äº¤æ˜“å­—æ®µçš„çœŸå®ç”Ÿäº§æ•°æ®ï¼Œæ„å»ºå¯ä¸ç°æœ‰ç¡®å®šæ€§è§„åˆ™äº’è¡¥çš„æœºå™¨å­¦ä¹ é¢„æµ‹ä½“ç³»ï¼Œé‡‡ç”¨â€œLeakage Freeâ€ä¸¤é˜¶æ®µå»ºæ¨¡æ€è·¯å¹¶å¯¹XGBoostã€KANåŠå…¶é›†æˆæ–¹æ¡ˆè¿›è¡Œå¯¹æ¯”è¯„ä¼°ï¼Œä»¥å®ç°æŒ‰ä¹°æ–¹-ä¾›åº”å•†å¯¹çš„å®æ—¶ç¨€é‡Šé¢„æµ‹ä¸åŠ¨æ€é¢åº¦æ”¯æŒã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜æ•°æ®é©±åŠ¨çš„ä¸¤é˜¶æ®µXGBoost/KANä¸é›†æˆæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹å‘ç¥¨ç¨€é‡Šï¼Œå¯ä½œä¸ºç¡®å®šæ€§ç®—æ³•çš„è¡¥å……ç”¨äºå®æ—¶åŠ¨æ€ä¿¡ç”¨é™é¢ï¼Œä»è€Œé™ä½å¯¹IPUçš„ä¾èµ–å¹¶æå‡ä¾›åº”é“¾é‡‘èçš„å¯ç”¨æ€§ã€‚

**å…³é”®è¯**ï¼šä¾›åº”é“¾é‡‘è, å‘ç¥¨ç¨€é‡Šé¢„æµ‹, æ”¯ä»˜ç¨€é‡Šé£é™©, ä¹°æ–¹-ä¾›åº”å•†å¯¹å»ºæ¨¡, å®æ—¶é£é™©è¯„åˆ†, ç‰¹å¾æ³„æ¼é˜²æŠ¤, é›†æˆå­¦ä¹ , è§„åˆ™æ¨¡å‹ä¸æœºå™¨å­¦ä¹ èåˆ

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15248v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15248v1.pdf)

---

## [2. Secure and Energy-Efficient Wireless Agentic AI Networks](https://arxiv.org/abs/2602.15212v1)

**ä½œè€…**ï¼šYuanyan Song, Kezhi Wang, Xinmian Xu  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In this paper, we introduce a secure wireless agentic AI network comprising one supervisor AI agent and multiple other AI agents to provision quality of service (QoS) for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes. Specifically, the supervisor AI agent can dynamically assign other AI agents to participate in cooperative reasoning, while the unselected AI agents act as friendly jammers to degrade the eavesdropper's interception performance. To extend the service duration of AI agents, an energy minimization problem is formulated that jointly optimizes AI agent selection, base station (BS) beamforming, and AI agent transmission power, subject to latency and reasoning accuracy constraints. To address the formulated problem, we propose two resource allocation schemes, ASC and LAW, which first decompose it into three sub-problems. Specifically, ASC optimizes each sub-problem iteratively using the proposed alternating direction method of multipliers (ADMM)-based algorithm, semi-definite relaxation (SDR), and successive convex approximation (SCA), while LAW tackles each sub-problem using the proposed large language model (LLM) optimizer within an agentic workflow. The experimental results show that the proposed solutions can reduce network energy consumption by up to 59.1% compared to other benchmark schemes. Furthermore, the proposed schemes are validated using a practical agentic AI system based on Qwen, demonstrating satisfactory reasoning accuracy across various public benchmarks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§å®‰å…¨ä¸”èŠ‚èƒ½çš„æ— çº¿Agentic AIç½‘ç»œï¼Œé€šè¿‡â€œåä½œæ¨ç†+å‹å†›å¹²æ‰°â€ä¿éšœéšç§ä¸QoSï¼Œå¹¶è”åˆä¼˜åŒ–èµ„æºåˆ†é…ä»¥æ˜¾è‘—é™ä½èƒ½è€—ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ— çº¿å¤šæ™ºèƒ½ä½“åä½œæ¨ç†åœ¨æå‡ç”¨æˆ·ä»»åŠ¡QoSçš„åŒæ—¶é¢ä¸´çªƒå¬å¨èƒä¸ç»ˆç«¯èƒ½é‡å—é™é—®é¢˜ï¼Œéœ€è¦åœ¨ä¿å¯†æ€§ã€æ—¶å»¶/å‡†ç¡®ç‡çº¦æŸä¸èƒ½è€—ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºå«ç›‘ç£Agentä¸å¤šåä½œAgentçš„ç½‘ç»œï¼šè¢«é€‰ä¸­Agentå‚ä¸åä½œæ¨ç†ï¼Œæœªé€‰ä¸­Agentä½œä¸ºå‹å†›å¹²æ‰°å™¨æŠ‘åˆ¶çªƒå¬ï¼›å»ºç«‹è”åˆä¼˜åŒ–é—®é¢˜ï¼ˆAgenté€‰æ‹©ã€BSæ³¢æŸæˆå½¢ã€Agentå‘å°„åŠŸç‡ï¼‰å¹¶æå‡ºASCï¼ˆADMM+SDR+SCAè¿­ä»£åˆ†è§£æ±‚è§£ï¼‰ä¸LAWï¼ˆåœ¨Agentic workflowä¸­ç”¨LLMä¼˜åŒ–å™¨æ±‚è§£å„å­é—®é¢˜ï¼‰ä¸¤ç§æ–¹æ¡ˆã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ASC/LAWç›¸æ¯”åŸºçº¿æœ€é«˜å¯é™ä½59.1%ç½‘ç»œèƒ½è€—ï¼Œå¹¶åœ¨åŸºäºQwençš„çœŸå®Agenticç³»ç»ŸéªŒè¯ä¸­ä¿æŒå¤šé¡¹å…¬å¼€åŸºå‡†ä¸Šä»¤äººæ»¡æ„çš„æ¨ç†å‡†ç¡®ç‡ä¸çº¦æŸæ»¡è¶³ã€‚

**å…³é”®è¯**ï¼šæ— çº¿å¤šæ™ºèƒ½ä½“ç½‘ç»œ, å®‰å…¨æ¨ç†, ç‰©ç†å±‚å®‰å…¨, å‹å¥½å¹²æ‰°, ååŒæ¨ç†, èµ„æºåˆ†é…ä¼˜åŒ–, èƒ½è€—æœ€å°åŒ–, æ³¢æŸæˆå½¢, åŠŸç‡æ§åˆ¶

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15212v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15212v1.pdf)

---

## [3. Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173v1)

**ä½œè€…**ï¼šLuise Ge, Yongyan Zhang, Yevgeniy Vorobeychik  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æ¯”è¾ƒäº†æ¨ç†å‹ä¸å¯¹è¯å‹LLMåœ¨ä¸ç¡®å®šé£é™©å†³ç­–ä¸­çš„å·®å¼‚ï¼Œå‘ç°æ¨ç†å‹æ›´æ¥è¿‘ç†æ€§æœŸæœ›æ”¶ç›Šæœ€å¤§åŒ–ï¼Œè€Œå¯¹è¯å‹æ›´å—å‘ˆç°æ–¹å¼ä¸è§£é‡Šå½±å“ä¸”å­˜åœ¨æ˜¾è‘—â€œæè¿°-ç»éªŒâ€å·®è·ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šLLMæ­£è¢«ç”¨äºå†³ç­–æ”¯æŒä¸ä»£ç†å¼å·¥ä½œæµï¼Œä½†å…¶åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„å†³ç­–è§„å¾‹ä¸åå·®ï¼ˆå¦‚æ¡†æ¶æ•ˆåº”ã€é¡ºåºæ•ˆåº”ï¼‰å°šç¼ºä¹ç³»ç»Ÿç†è§£ã€‚ä½œè€…å¸Œæœ›æ˜ç¡®ä¸åŒç±»å‹LLMåœ¨é£é™©é€‰æ‹©ä¸Šçš„è¡Œä¸ºç‰¹å¾ï¼Œå¹¶ä¸äººç±»ä¸ç†æ€§åŸºçº¿å¯¹ç…§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨20ä¸ªå‰æ²¿ä¸å¼€æºLLMä¸Šï¼Œæ²¿ä¸¤ç»´åº¦æ“æ§å®éªŒï¼šå‰æ™¯å‘ˆç°ï¼ˆæ˜¾å¼æè¿°vsåŸºäºç»éªŒå†å²ï¼‰ä¸æ˜¯å¦è¦æ±‚å†³ç­–è§£é‡Šï¼Œå¹¶åŠ å…¥å‰æ™¯é¡ºåºä¸å¾—å¤±æ¡†æ¶ç­‰å› ç´ ï¼›åŒæ—¶è¿›è¡ŒåŒ¹é…çš„äººç±»å—è¯•å®éªŒï¼Œå¹¶ç”¨æœŸæœ›æ”¶ç›Šæœ€å¤§åŒ–æ¨¡å‹ä½œç†æ€§å‚ç…§ã€‚

**ä¸»è¦ç»“è®º**ï¼šLLMå‘ˆç°ä¸¤ç±»èšç±»ï¼šæ¨ç†æ¨¡å‹å¯¹é¡ºåºã€æ¡†æ¶ä¸è§£é‡Šä¸æ•æ„Ÿï¼Œä¸”åœ¨æ˜¾å¼ä¸ç»éªŒå‘ˆç°ä¸‹è¡¨ç°ä¸€è‡´ã€æ›´åŠ ç†æ€§ï¼›å¯¹è¯æ¨¡å‹æ›´ä¸ç†æ€§ä½†ç•¥æ›´åƒäººç±»ï¼Œæ˜¾è‘—å—é¡ºåº/æ¡†æ¶/è§£é‡Šå½±å“å¹¶å‡ºç°å¤§çš„æè¿°-å†å²å·®è·ï¼Œå¼€æºæ¨¡å‹å¯¹æ¯”è¡¨æ˜æ•°å­¦æ¨ç†è®­ç»ƒå¯èƒ½æ˜¯åŒºåˆ†ä¸¤ç±»çš„å…³é”®å› ç´ ã€‚

**å…³é”®è¯**ï¼šLLMå†³ç­–ä¸ç¡®å®šæ€§, é£é™©é€‰æ‹©, å‰æ™¯ç†è®º, æè¿°-ç»éªŒå·®è·, æ¡†æ¶æ•ˆåº”, é€‰é¡¹é¡ºåºæ•ˆåº”, è§£é‡Šå¯¹å†³ç­–å½±å“, æ¨ç†æ¨¡å‹, æ•°å­¦æ¨ç†è®­ç»ƒ, äººç±»å¯¹ç…§å®éªŒ, æœŸæœ›æ”¶ç›Šæœ€å¤§åŒ–

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15173v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15173v1.pdf)

---

## cs.CL

## [4. AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking](https://arxiv.org/abs/2602.15190v1)

**ä½œè€…**ï¼šHerbert Ullrich, Jan Drchal  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§â€œåŒæ£€ç´¢å™¨â€RAGäº‹å®æ ¸æŸ¥ç³»ç»Ÿï¼Œå°†æ–‡æœ¬ç›¸ä¼¼æ£€ç´¢ä¸åå‘å›¾ç‰‡æœç´¢ç»“åˆï¼Œå¹¶ç”¨ä¸€æ¬¡å¤šæ¨¡æ€LLMè°ƒç”¨å®Œæˆåˆ¤å®šï¼Œæˆæœ¬ä½ä¸”æ•ˆæœååˆ—å‰èŒ…ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å›¾æ–‡äº‹å®æ ¸æŸ¥éœ€è¦åŒæ—¶è¦†ç›–æ–‡æœ¬è¯æ®ä¸å›¾åƒæº¯æºï¼Œä½†å¸¸è§æ–¹æ¡ˆè¦ä¹ˆæˆæœ¬é«˜ã€è¦ä¹ˆæµæ°´çº¿å¤æ‚éš¾å¤ç°ã€‚ä½œè€…å¸Œæœ›ç”¨æ›´ç®€å•ã€ä½æˆæœ¬ã€æ¨¡å—åŒ–çš„æ–¹æ¡ˆè·å¾—æœ‰ç«äº‰åŠ›çš„å…±äº«ä»»åŠ¡æˆç»©ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç³»ç»Ÿç”±ä¸‰ä¸ªè§£è€¦æ¨¡å—ç»„æˆï¼šæ–‡æœ¬ç«¯ç”¨å‘é‡ç›¸ä¼¼æ£€ç´¢å¬å›å€™é€‰è¯æ®ï¼Œå›¾åƒç«¯é€šè¿‡APIåå‘å›¾ç‰‡æœç´¢è·å–ç›¸å…³æ¥æºï¼Œå†æŠŠä¸¤è·¯è¯æ®äº¤ç»™GPT5.1è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆå¼æ ¸æŸ¥ï¼ˆæ¯æ¬¡æ ¸æŸ¥ä»…ä¸€æ¬¡å¤šæ¨¡æ€LLMè°ƒç”¨ï¼‰ã€‚åŒæ—¶å…¬å¼€ä»£ç ã€æç¤ºè¯ã€å‘é‡åº“ï¼Œå¹¶åˆ†æè¿è¡Œæˆæœ¬ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•åœ¨AVerImaTeCå…±äº«ä»»åŠ¡ä¸­è·å¾—ç¬¬3åï¼Œè¯æ˜â€œæ–‡æœ¬æ£€ç´¢+åå‘å›¾æœ+å•æ¬¡LLMç”Ÿæˆâ€çš„ç®€åŒ–RAGæ¡†æ¶å³å¯å–å¾—å¼ºåŸºçº¿è¡¨ç°ã€‚ç³»ç»Ÿå¹³å‡æˆæœ¬çº¦$0.013/æ¡ä¸”æ˜“å¤ç°ï¼Œå¯ä½œä¸ºåç»­æ”¹è¿›ä¸æ‰©å±•çš„èµ·ç‚¹ã€‚

**å…³é”®è¯**ï¼šå›¾æ–‡äº‹å®æ ¸æŸ¥, åŒæ£€ç´¢å™¨, åå‘å›¾åƒæœç´¢, æ–‡æœ¬ç›¸ä¼¼åº¦æ£€ç´¢, å›¾åƒæ£€ç´¢, å¤šæ¨¡æ€LLMæ¨ç†, æ¨ç†æˆæœ¬ä¼˜åŒ–, å…±äº«ä»»åŠ¡è¯„æµ‹

**è¯„åˆ†**ï¼š33

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15190v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15190v1.pdf)

---

## cs.CV

## [5. Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181v1)

**ä½œè€…**ï¼šYunxiao Zhang, William Stone, Suryansh Kumar  
**åˆ†ç±»**ï¼šcs.CV, cs.LG, cs.RO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§é¢å‘ä½“è‚²ä¸èˆå°è¡¨æ¼”çš„â€œæ—¶é—´å¯å½’æ¡£â€ç›¸æœºè™šæ‹ŸåŒ–ç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œåœ¨åŠ¨æ€å¿«é€Ÿè¿åŠ¨åœºæ™¯ä¸­å®ç°æ›´æ—¶ç©ºä¸€è‡´ã€é€¼çœŸçš„æ–°è§†è§’åˆæˆï¼Œå¹¶æ”¯æŒå›çœ‹ä»»æ„å†å²æ—¶åˆ»é‡æ¸²æŸ“ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŠ¨æ€3DGS/4DGSç­‰æ–¹æ³•ä¾èµ–é«˜è´¨é‡SfMç‚¹äº‘ä¸”éš¾ä»¥åº”å¯¹å¤§å¹…éåˆšä½“å¿«é€Ÿè¿åŠ¨ä¸å¤šä¸»ä½“ç‹¬ç«‹è¿åŠ¨ï¼Œå¯¼è‡´è·Ÿè¸ªå‡è®¾è¢«ç ´åã€æ¸²æŸ“ä¸ç¨³å®šï¼›åŒæ—¶ç¼ºä¹å¯¹è¿‡å»æ—¶é—´ç‚¹çš„å¯æ£€ç´¢é‡æ”¾ï¼ˆtime-archivalï¼‰èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå›åˆ°ç¥ç»ä½“æ¸²æŸ“æ¡†æ¶ï¼Œå°†æ¯ä¸ªæ—¶é—´ç‚¹çš„åŠ¨æ€åœºæ™¯å»ºæ¨¡ä¸ºè·¨å¤šè·¯åŒæ­¥ç›¸æœºè§†å›¾çš„åˆšä½“å˜æ¢ç»„åˆï¼Œå¹¶åœ¨æ­¤è¡¨ç¤ºä¸Šè¿›è¡Œç¥ç»è¡¨ç¤ºå­¦ä¹ ï¼Œä»¥æå‡æµ‹è¯•æ—¶çš„æ–°è§†è§’æ¸²æŸ“è´¨é‡ä¸æ—¶ç©ºä¸€è‡´æ€§ï¼›åŒæ—¶å°†ä¸åŒæ—¶é—´å®ä¾‹æ˜¾å¼çº³å…¥è¡¨ç¤ºä»¥å®ç°å¯å½’æ¡£æŸ¥è¯¢ä¸å›æ”¾æ¸²æŸ“ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•åœ¨å¿«èŠ‚å¥ä½“è‚²/æ¼”å‡ºç­‰å¤æ‚åŠ¨æ€åœºæ™¯ä¸‹ï¼Œç›¸æ¯”åŸºäºé«˜æ–¯æº…å°„çš„åŠ¨æ€æ–¹æ¡ˆæ›´ç¨³å¥ä¸”å…·æ›´é«˜è§†è§‰è´¨é‡ï¼Œå¹¶é¦–æ¬¡ï¼ˆç›¸å¯¹æ—¢æœ‰ç¥ç»æ¸²æŸ“/æ–°è§†è§’åˆæˆå·¥ä½œï¼‰æä¾›å¯å›åˆ°ä»»æ„å†å²æ—¶åˆ»è¿›è¡Œæ–°è§†è§’é‡å»ºçš„time-archivalèƒ½åŠ›ï¼Œé€‚ç”¨äºè½¬æ’­å›æ”¾ä¸åˆ†æå½’æ¡£ã€‚

**å…³é”®è¯**ï¼šç›¸æœºè™šæ‹ŸåŒ–, æ–°è§†è§’åˆæˆ, ç¥ç»ä½“æ¸²æŸ“, åŠ¨æ€åœºæ™¯æ¸²æŸ“, æ—¶ç©ºä¸€è‡´æ€§, æ—¶é—´å½’æ¡£æ¸²æŸ“, ä½“è‚²èµ›äº‹è½¬æ’­, å¤šè§†è§’åŒæ­¥ç›¸æœº, åˆšä½“å˜æ¢å»ºæ¨¡, ä¸‰ç»´é«˜æ–¯æ³¼æº…, éåˆšä½“å¿«é€Ÿè¿åŠ¨

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15181v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15181v1.pdf)

---

## [6. Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167v1)

**ä½œè€…**ï¼šXiaoyi Wen, Fei Jiang  
**åˆ†ç±»**ï¼šcs.CV, stat.AP, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§åˆ†å¸ƒå¼ï¼ˆdistributionalï¼‰æ·±åº¦å­¦ä¹ è¶…åˆ†è¾¨æ¡†æ¶ï¼Œä»¥æå‡4D Flow MRIåœ¨çœŸå®é‡‡é›†åŸŸåç§»ä¸‹çš„é‡å»ºé²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿè¶…åˆ†æ¨¡å‹ä¾èµ–â€œäººå·¥ä¸‹é‡‡æ ·-é«˜åˆ†â€é…å¯¹æ•°æ®è®­ç»ƒï¼Œä½†ä¸´åºŠä½åˆ†æ•°æ®çš„é€€åŒ–æœºåˆ¶ä¸ä¸‹é‡‡æ ·ä¸åŒï¼Œå¯¼è‡´åŸŸåç§»ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚4D Flow MRIå¯¹è¡€æµé€Ÿåº¦åŠå£é¢åº”åŠ›ç­‰æŒ‡æ ‡æ•æ„Ÿï¼Œéœ€åœ¨ä½è´¨è¾“å…¥ä¸‹ä»èƒ½å¯é è¶…åˆ†ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆç”¨é«˜åˆ†è¾¨CFDæ¨¡æ‹ŸåŠå…¶ä¸‹é‡‡æ ·æ•°æ®é¢„è®­ç»ƒæ¨¡å‹ï¼Œå†ç”¨å°‘é‡â€œé…å¯¹ä¸”åè°ƒ(harmonized)â€çš„4D Flow MRIâ€“CFDæ ·æœ¬å¾®è°ƒï¼›åŒæ—¶å¼•å…¥åˆ†å¸ƒå¼å­¦ä¹ /ä¼°è®¡æ¥å¯¹é½è®­ç»ƒä¸æµ‹è¯•åˆ†å¸ƒï¼Œå¹¶ç»™å‡ºåˆ†å¸ƒä¼°è®¡å™¨çš„ç†è®ºæ€§è´¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨çœŸå®æ•°æ®åº”ç”¨ä¸­ï¼Œè¯¥åˆ†å¸ƒå¼å­¦ä¹ æ¡†æ¶ç›¸è¾ƒä¼ ç»Ÿæ·±åº¦å­¦ä¹ è¶…åˆ†æ–¹æ³•æ˜¾è‘—æå‡äº†åŸŸåç§»ä¸‹çš„é‡å»ºæ•ˆæœä¸æ³›åŒ–é²æ£’æ€§ï¼Œè¯æ˜å…¶æ›´é€‚åˆä¸´åºŠçœŸå®é€€åŒ–åœºæ™¯çš„4D Flow MRIè¶…åˆ†ã€‚

**å…³é”®è¯**ï¼šè¶…åˆ†è¾¨ç‡é‡å»º, åŒ»å­¦å½±åƒè¶…åˆ†, åŸŸåç§», åŸŸæ³›åŒ–, åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ , åˆ†å¸ƒä¼°è®¡å™¨, é²æ£’æ€§å­¦ä¹ , è®¡ç®—æµä½“åŠ›å­¦æ¨¡æ‹Ÿï¼ˆCFDï¼‰, ä»¿çœŸåˆ°çœŸå®è¿ç§», å°æ ·æœ¬å¾®è°ƒ, è¡€æµåŠ¨åŠ›å­¦æŒ‡æ ‡

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15167v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15167v1.pdf)

---

## [7. Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154v1)

**ä½œè€…**ï¼šPraditha Alwis, Soumyadeep Chandra, Deepak Ravikumar ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­â€œç´¯è®¡æ ·æœ¬æŸå¤±ï¼ˆCSLï¼‰â€è½¨è¿¹æ¥è‡ªåŠ¨å‘ç°è§†é¢‘é€å¸§æ ‡æ³¨é”™è¯¯ï¼ˆé”™æ ‡ä¸æ—¶åºé”™ä¹±ï¼‰çš„é€šç”¨å®¡è®¡æ–¹æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šçœŸå®è§†é¢‘æ•°æ®å¸¸å«é”™æ ‡ä¸æ—¶åºä¸ä¸€è‡´ç­‰æ ‡æ³¨å™ªå£°ï¼Œå°¤å…¶åœ¨é˜¶æ®µ/æµç¨‹ç±»ä»»åŠ¡ä¸­ä¼šç ´åæ—¶é—´ä¸€è‡´æ€§å¹¶æ˜¾è‘—å½±å“æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚ç°æœ‰å‘ç°é”™è¯¯å¾€å¾€ä¾èµ–äººå·¥æˆ–é¢å¤–ç›‘ç£ï¼Œç¼ºä¹å¯æ³›åŒ–çš„è‡ªåŠ¨åŒ–æ‰‹æ®µã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨è®­ç»ƒè§†é¢‘åˆ†å‰²æ¨¡å‹æ—¶ä¿å­˜å„epochçš„checkpointï¼Œå¹¶ç”¨è¿™äº›checkpointå¯¹æ¯ä¸€å¸§è®¡ç®—è·¨epochå¹³å‡æŸå¤±å½¢æˆCSLï¼ˆæŸå¤±è½¨è¿¹æŒ‡çº¹ï¼‰ã€‚æŒç»­é«˜æŸå¤±æˆ–ä¸è§„åˆ™æŸå¤±è½¨è¿¹çš„å¸§è¢«åˆ¤ä¸ºéš¾ä»¥å­¦ä¹ æ ·æœ¬ï¼Œä»è€Œä½œä¸ºæ½œåœ¨é”™æ ‡æˆ–æ—¶é—´é”™ä½ï¼ˆdisorderingï¼‰å€™é€‰è¢«æ ‡è®°ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨EgoPERä¸Cholec80ä¸Šï¼ŒCSLèƒ½æœ‰æ•ˆå®šä½ç»†å¾®çš„é”™æ ‡ä¸å¸§é¡ºåºå¼‚å¸¸ï¼Œä¸”ä¸éœ€è¦é”™è¯¯æ ‡æ³¨çš„çœŸå€¼ç›‘ç£ã€å¯¹æ¨¡å‹ç›¸å¯¹æ— å…³ã€‚è¯¥æ–¹æ³•å¯ä½œä¸ºæ•°æ®é›†å®¡è®¡å·¥å…·æå‡è§†é¢‘æ•°æ®è´¨é‡ä¸è®­ç»ƒå¯é æ€§ã€‚

**å…³é”®è¯**ï¼šè§†é¢‘æ•°æ®é›†, æ³¨é‡Šé”™è¯¯, æŸå¤±è½¨è¿¹, ç´¯ç§¯æ ·æœ¬æŸå¤±, æ¨¡å‹æ— å…³, è¯¯æ ‡è®°, æ—¶é—´é”™ä½, æ•°æ®é›†å®¡è®¡, è®­ç»ƒå¯é æ€§, åŠ¨ä½œè¯†åˆ«

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15154v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15154v1.pdf)

---

## cs.LG

## [8. Size Transferability of Graph Transformers with Convolutional Positional Encodings](https://arxiv.org/abs/2602.15239v1)

**ä½œè€…**ï¼šJavier Porras-Valenzuela, Zhiyang Wang, Alejandro Ribeiro  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Transformers have achieved remarkable success across domains, motivating the rise of Graph Transformers (GTs) as attention-based architectures for graph-structured data. A key design choice in GTs is the use of Graph Neural Network (GNN)-based positional encodings to incorporate structural information. In this work, we study GTs through the lens of manifold limit models for graph sequences and establish a theoretical connection between GTs with GNN positional encodings and Manifold Neural Networks (MNNs). Building on transferability results for GNNs under manifold convergence, we show that GTs inherit transferability guarantees from their positional encodings. In particular, GTs trained on small graphs provably generalize to larger graphs under mild assumptions. We complement our theory with extensive experiments on standard graph benchmarks, demonstrating that GTs exhibit scalable behavior on par with GNNs. To further show the efficiency in a real-world scenario, we implement GTs for shortest path distance estimation over terrains to better illustrate the efficiency of the transferable GTs. Our results provide new insights into the understanding of GTs and suggest practical directions for efficient training of GTs in large-scale settings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ä»æµå½¢æé™æ¨¡å‹è§’åº¦å»ºç«‹å¸¦GNNå·ç§¯å¼ä½ç½®ç¼–ç çš„å›¾Transformerä¸æµå½¢ç¥ç»ç½‘ç»œçš„ç†è®ºè”ç³»ï¼Œè¯æ˜å…¶å¯ä»å°å›¾è®­ç»ƒè¿ç§»åˆ°å¤§å›¾æ¨ç†å¹¶åœ¨å®éªŒä¸­éªŒè¯å¯æ‰©å±•æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å›¾Transformerä¾èµ–ä½ç½®ç¼–ç æ³¨å…¥ç»“æ„ä¿¡æ¯ï¼Œä½†å…¶â€œè®­ç»ƒäºå°è§„æ¨¡å›¾ã€æ³›åŒ–åˆ°æ›´å¤§è§„æ¨¡å›¾â€çš„å°ºå¯¸å¯è¿ç§»æ€§ç¼ºä¹ç³»ç»Ÿç†è®ºè§£é‡Šä¸ä¿è¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåŸºäºå›¾åºåˆ—çš„æµå½¢æ”¶æ•›ä¸æµå½¢æé™æ¨¡å‹ï¼Œå°†ä½¿ç”¨GNN/å·ç§¯ä½ç½®ç¼–ç çš„å›¾Transformeråˆ»ç”»ä¸ºä¸æµå½¢ç¥ç»ç½‘ç»œç›¸å…³çš„å½¢å¼ï¼Œå¹¶åˆ©ç”¨GNNåœ¨æµå½¢æ”¶æ•›ä¸‹çš„å¯è¿ç§»æ€§ç»“æœï¼Œæ¨å‡ºå›¾Transformerçš„å¯è¿ç§»æ€§æ¥æºäºå…¶ä½ç½®ç¼–ç ï¼›åŒæ—¶åœ¨æ ‡å‡†åŸºå‡†ä¸åœ°å½¢æœ€çŸ­è·¯è·ç¦»ä¼°è®¡ä»»åŠ¡ä¸ŠåšéªŒè¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ¸©å’Œå‡è®¾ä¸‹ï¼Œå¸¦GNNå·ç§¯ä½ç½®ç¼–ç çš„å›¾Transformerå¯ç»§æ‰¿ä½ç½®ç¼–ç çš„ç†è®ºè¿ç§»ä¿è¯ï¼Œå› è€Œèƒ½ä»å°å›¾è®­ç»ƒæ¨å¹¿åˆ°å¤§å›¾ï¼›å®éªŒæ˜¾ç¤ºå…¶å¯æ‰©å±•æ€§ä¸GNNç›¸å½“ï¼Œå¹¶åœ¨çœŸå®åœºæ™¯ä¸­ä½“ç°å‡ºæ›´é«˜æ•ˆçš„å¤§è§„æ¨¡è®­ç»ƒä¸éƒ¨ç½²æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šå›¾å˜æ¢å™¨, å›¾ç¥ç»ç½‘ç»œ, ä½ç½®ç¼–ç , å¯è½¬ç§»æ€§, æµå½¢ç¥ç»ç½‘ç»œ, å›¾åºåˆ—, çŸ­è·¯å¾„ä¼°è®¡, å¤§è§„æ¨¡è®­ç»ƒ

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15239v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15239v1.pdf)

---

## [9. Closing the Distribution Gap in Adversarial Training for LLMs](https://arxiv.org/abs/2602.15238v2)

**ä½œè€…**ï¼šChengzhi Hu, Jonas Dornbusch, David LÃ¼dke ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºåˆ†å¸ƒå¼å¯¹æŠ—è®­ç»ƒï¼ˆDATï¼‰ï¼Œç”¨æ‰©æ•£å¼LLMè¿‘ä¼¼çœŸå®æç¤º-å›ç­”åˆ†å¸ƒæ¥ç”Ÿæˆé«˜ä¼¼ç„¶å¤šæ ·æ ·æœ¬ï¼Œä»è€Œæ˜¾è‘—æå‡LLMå¯¹ç®€å•æ”¹å†™/ç¿»è¯‘ç­‰åŒåˆ†å¸ƒæ”»å‡»çš„é²æ£’æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰LLMå¯¹æŠ—è®­ç»ƒä¸»è¦åœ¨è®­ç»ƒé›†ä¸Šæœ€å°åŒ–å¯¹æŠ—æŸå¤±ï¼Œä½†å¯¹æ•´ä½“æ•°æ®åˆ†å¸ƒè¦†ç›–ä¸è¶³ï¼Œå¯¼è‡´å¯¹çœ‹ä¼¼ç®€å•çš„åŒåˆ†å¸ƒå˜ä½“ï¼ˆå¦‚æ—¶æ€æ”¹å†™ã€è·¨è¯­è¨€ç¿»è¯‘ï¼‰ä»ç„¶è„†å¼±ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šDATåˆ©ç”¨æ‰©æ•£LLMå»ºæ¨¡å¹¶é‡‡æ ·æç¤ºä¸å›å¤çš„è”åˆåˆ†å¸ƒï¼Œç”Ÿæˆå¤šæ ·ä¸”é«˜æ¦‚ç‡çš„è®­ç»ƒæ ·æœ¬ä»¥è¡¥é½åˆ†å¸ƒè¦†ç›–ï¼›å†å°†åŸºäºè¯¥åˆ†å¸ƒçš„ä¼˜åŒ–ä¸æŒç»­å¯¹æŠ—è®­ç»ƒç»“åˆï¼Œå½¢æˆæ›´å¼ºçš„æ³›åŒ–é²æ£’æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šé€šè¿‡ç¼©å°â€œè®­ç»ƒé›†å¯¹æŠ—ä¼˜åŒ–â€ä¸â€œçœŸå®æ•°æ®åˆ†å¸ƒè¦†ç›–â€ä¹‹é—´çš„å·®è·ï¼ŒDATç›¸æ¯”ä»¥å¾€æ–¹æ³•å–å¾—æ›´é«˜çš„å¯¹æŠ—é²æ£’æ€§ï¼Œèƒ½æ›´å¥½æŠµå¾¡åŒåˆ†å¸ƒçš„ç®€å•æç¤ºå˜å½¢æ”»å‡»ã€‚

**å…³é”®è¯**ï¼šåˆ†å¸ƒå·®è·, åˆ†å¸ƒå¼å¯¹æŠ—è®­ç»ƒï¼ˆDATï¼‰, æ•°æ®åˆ†å¸ƒè¦†ç›–, æ‰©æ•£æ¨¡å‹LLM, è”åˆåˆ†å¸ƒå»ºæ¨¡, é«˜ä¼¼ç„¶é‡‡æ ·, åˆ†å¸ƒå†…æ”»å‡», æç¤ºæ”¹å†™æ”»å‡», è·¨è¯­è¨€æ”»å‡», è¿ç»­å¯¹æŠ—è®­ç»ƒ

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15238v2) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15238v2.pdf)

---

## [10. BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening](https://arxiv.org/abs/2602.15236v1)

**ä½œè€…**ï¼šAnjie Qiao, Zhen Wang, Yaliang Li ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šBindCLIP å°†CLIPå¼å¯¹æ¯”å­¦ä¹ ä¸å£è¢‹æ¡ä»¶æ‰©æ•£ç”Ÿæˆçš„å§¿æ€ç›‘ç£ç»Ÿä¸€è®­ç»ƒï¼Œå­¦ä¹ æ›´â€œäº¤äº’æ„ŸçŸ¥â€çš„å£è¢‹-é…ä½“åµŒå…¥ï¼Œä»è€Œæå‡è™šæ‹Ÿç­›é€‰æ’åºä¸OODæ³›åŒ–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰DrugCLIPç­‰æ£€ç´¢åµŒå…¥å¯èƒ½å¯¹ç»†ç²’åº¦ç»“åˆç›¸äº’ä½œç”¨ä¸æ•æ„Ÿï¼Œå¹¶å®¹æ˜“åˆ©ç”¨æ•°æ®ä¸­çš„â€œæ·å¾„ç›¸å…³æ€§â€ï¼Œå¯¼è‡´æ— æ³•æŒ‰çœŸå®ç»“åˆå…¼å®¹æ€§å¯é æ’åé…ä½“ã€‚ä¸ºæ­¤éœ€è¦å¼•å…¥èƒ½ç›´æ¥çº¦æŸç›¸äº’ä½œç”¨çš„å§¿æ€çº§ç›‘ç£ï¼Œå¹¶é™ä½å¯¹æ·å¾„çš„ä¾èµ–ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè”åˆè®­ç»ƒå£è¢‹/é…ä½“ç¼–ç å™¨ï¼šä¸€æ–¹é¢ç”¨CLIPå¼å¯¹æ¯”å­¦ä¹ å¯¹é½å£è¢‹-é…ä½“è¡¨å¾ï¼Œå¦ä¸€æ–¹é¢åŠ å…¥å£è¢‹æ¡ä»¶æ‰©æ•£ç›®æ ‡ç”Ÿæˆç»“åˆå§¿æ€ï¼Œä½¿å§¿æ€ç›‘ç£åå‘å¡‘é€ æ£€ç´¢åµŒå…¥ç©ºé—´ã€‚å¦é€šè¿‡hard-negativeå¢å¼ºä¸é…ä½“-é…ä½“é”šå®šæ­£åˆ™é˜²æ­¢è¡¨å¾å¡Œç¼©å¹¶ç¼“è§£æ·å¾„å­¦ä¹ ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ä¸¤é¡¹å…¬å¼€åŸºå‡†ä¸Šç›¸å¯¹å¼ºåŸºçº¿ç¨³å®šæå‡ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜çš„åˆ†å¸ƒå¤–è™šæ‹Ÿç­›é€‰ä¸FEP+é…ä½“ç±»ä¼¼ç‰©æ’åºä¸Šè·å¾—æ˜¾è‘—å¢ç›Šã€‚ç»“æœè¡¨æ˜å°†ç”Ÿæˆå¼å§¿æ€ç›‘ç£ä¸å¯¹æ¯”å­¦ä¹ ç»“åˆå¯æå‡äº¤äº’ç›¸å…³æ€§ä¸çœŸå®åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šè™šæ‹Ÿç­›é€‰, è›‹ç™½å£è¢‹-é…ä½“è”åˆè¡¨å¾, Diffusion, ç»“åˆæ„è±¡ç”Ÿæˆ, æ„è±¡çº§ç›‘ç£, ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜, é…ä½“-é…ä½“å¯¹é½æ­£åˆ™åŒ–, è¡¨ç¤ºåå¡Œé˜²æŠ¤, åˆ†å¸ƒå¤–æ³›åŒ–, FEP+åŸºå‡†è¯„æµ‹

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15236v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15236v1.pdf)

---

## [11. Automatically Finding Reward Model Biases](https://arxiv.org/abs/2602.15222v1)

**ä½œè€…**ï¼šAtticus Wang, IvÃ¡n Arcuschin, Arthur Conmy  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reward models are central to large language model (LLM) post-training. However, past work has shown that they can reward spurious or undesirable attributes such as length, format, hallucinations, and sycophancy. In this work, we introduce and study the research problem of automatically finding reward model biases in natural language. We offer a simple approach of using an LLM to iteratively propose and refine candidate biases. Our method can recover known biases and surface novel ones: for example, we found that Skywork-V2-8B, a leading open-weight reward model, often mistakenly favors responses with redundant spacing and responses with hallucinated content. In addition, we show evidence that evolutionary iteration outperforms flat best-of-N search, and we validate the recall of our pipeline using synthetically injected biases. We hope our work contributes to further research on improving RMs through automated interpretability methods.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§ç”¨LLMè‡ªåŠ¨å‘ç°å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åç½®çš„è¿­ä»£æœç´¢æ¡†æ¶ï¼Œèƒ½å¤ç°å·²çŸ¥åç½®å¹¶æŒ–æ˜å¦‚â€œåå¥½å†—ä½™ç©ºæ ¼/å¹»è§‰å†…å®¹â€ç­‰æ–°åç½®ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¥–åŠ±æ¨¡å‹åœ¨LLMåè®­ç»ƒä¸­è‡³å…³é‡è¦ï¼Œä½†å¸¸ä¼šå¥–åŠ±é•¿åº¦ã€æ ¼å¼ã€å¹»è§‰ã€è¿åˆç­‰â€œä¼ªç‰¹å¾â€ï¼Œéœ€è¦ä¸€ç§å¯æ‰©å±•ã€è‡ªåŠ¨åŒ–çš„æ–¹æ³•æ¥ç³»ç»Ÿå®šä½è¿™äº›åç½®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç”¨LLMç”Ÿæˆâ€œå€™é€‰åç½®æè¿°+è§¦å‘ç¤ºä¾‹â€ï¼Œå¹¶é€šè¿‡è¿›åŒ–å¼è¿­ä»£ï¼ˆåŸºäºRMåé¦ˆä¸æ–­æ”¹å†™/å˜å¼‚/ç­›é€‰ï¼‰æ¥å¼ºåŒ–èƒ½ç¨³å®šæé«˜RMè¯„åˆ†çš„åç½®ï¼›åŒæ—¶ç”¨åˆæˆæ³¨å…¥åç½®æ¥è¯„ä¼°ç®¡çº¿çš„å¬å›ç‡ï¼Œå¹¶å¯¹æ¯”å¹³é“ºçš„best-of-Næœç´¢ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•èƒ½å¤Ÿæ‰¾å›å·²çŸ¥RMåç½®å¹¶å‘ç°æ–°åç½®ï¼ˆå¦‚Skywork-V2-8Båå¥½å†—ä½™ç©ºæ ¼ä¸å¹»è§‰å†…å®¹ï¼‰ï¼Œä¸”è¿›åŒ–è¿­ä»£ä¼˜äºç®€å•best-of-Nï¼›åˆæˆåç½®å®éªŒè¡¨æ˜ç®¡çº¿å…·å¤‡è¾ƒå¥½çš„å¬å›ä¸æœ‰æ•ˆæ€§ï¼Œæœ‰åŠ©äºRMè‡ªåŠ¨åŒ–å¯è§£é‡Šæ€§ä¸æ”¹è¿›ã€‚

**å…³é”®è¯**ï¼šå¥–åŠ±æ¨¡å‹åå·®æ£€æµ‹, å¥–åŠ±æ¨¡å‹å¯è§£é‡Šæ€§, LLMå¼•å¯¼æœç´¢, è¿›åŒ–è¿­ä»£æœç´¢, åˆæˆåå·®æ³¨å…¥, é•¿åº¦åå¥½åå·®, æ ¼å¼åå¥½åå·®, å¹»è§‰åå¥½åå·®, å¥‰æ‰¿åå¥½åå·®, åè®­ç»ƒè¯„ä¼°

**è¯„åˆ†**ï¼š44

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15222v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15222v1.pdf)

---

## [12. MAVRL: Learning Reward Functions from Multiple Feedback Types with Amortized Variational Inference](https://arxiv.org/abs/2602.15206v1)

**ä½œè€…**ï¼šRaphaÃ«l Baur, Yannick Metz, Maria Gkoulta ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reward learning typically relies on a single feedback type or combines multiple feedback types using manually weighted loss terms. Currently, it remains unclear how to jointly learn reward functions from heterogeneous feedback types such as demonstrations, comparisons, ratings, and stops that provide qualitatively different signals. We address this challenge by formulating reward learning from multiple feedback types as Bayesian inference over a shared latent reward function, where each feedback type contributes information through an explicit likelihood. We introduce a scalable amortized variational inference approach that learns a shared reward encoder and feedback-specific likelihood decoders and is trained by optimizing a single evidence lower bound. Our approach avoids reducing feedback to a common intermediate representation and eliminates the need for manual loss balancing. Across discrete and continuous-control benchmarks, we show that jointly inferred reward posteriors outperform single-type baselines, exploit complementary information across feedback types, and yield policies that are more robust to environment perturbations. The inferred reward uncertainty further provides interpretable signals for analyzing model confidence and consistency across feedback types.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMAVRLå°†å¤šç§å¼‚è´¨äººç±»åé¦ˆç»Ÿä¸€ä¸ºå¯¹å…±äº«æ½œåœ¨å¥–åŠ±å‡½æ•°çš„è´å¶æ–¯æ¨æ–­ï¼Œå¹¶ç”¨æ‘Šé”€å˜åˆ†æ¨æ–­ç«¯åˆ°ç«¯è”åˆå­¦ä¹ ï¼Œä»è€Œå¾—åˆ°æ›´å‡†ç¡®ä¸”æ›´é²æ£’çš„å¥–åŠ±ä¸ç­–ç•¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¥–åŠ±å­¦ä¹ é€šå¸¸åªç”¨å•ä¸€åé¦ˆæˆ–é æ‰‹å·¥åŠ æƒèåˆå¤šç§åé¦ˆï¼Œä½†ä¸åŒåé¦ˆï¼ˆæ¼”ç¤ºã€åå¥½æ¯”è¾ƒã€æ‰“åˆ†ã€åœæ­¢ç­‰ï¼‰ä¿¡å·æ€§è´¨å·®å¼‚å¤§ï¼Œéš¾ä»¥ä¸€è‡´ä¸”å¯æ‰©å±•åœ°è”åˆå»ºæ¨¡ä¸è®­ç»ƒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæŠŠâ€œå¤šåé¦ˆå¥–åŠ±å­¦ä¹ â€è¡¨è¿°ä¸ºå¯¹åŒä¸€æ½œåœ¨å¥–åŠ±å‡½æ•°çš„è´å¶æ–¯åéªŒæ¨æ–­ï¼šæ¯ç§åé¦ˆç±»å‹é€šè¿‡å„è‡ªæ˜¾å¼ä¼¼ç„¶å‡½æ•°æä¾›çº¦æŸï¼›æå‡ºå¯æ‰©å±•çš„æ‘Šé”€å˜åˆ†æ¨æ–­æ¡†æ¶ï¼Œå­¦ä¹ å…±äº«çš„reward encoderä¸åé¦ˆç±»å‹ç‰¹å®šçš„likelihood decoderï¼Œå¹¶ç”¨å•ä¸€ELBOç›®æ ‡è”åˆä¼˜åŒ–ï¼Œé¿å…ä¸­é—´ç»Ÿä¸€è¡¨ç¤ºä¸æ‰‹åŠ¨losså¹³è¡¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ç¦»æ•£ä¸è¿ç»­æ§åˆ¶åŸºå‡†ä¸Šï¼Œè”åˆæ¨æ–­çš„å¥–åŠ±åéªŒä¼˜äºå•åé¦ˆåŸºçº¿ï¼Œèƒ½åˆ©ç”¨ä¸åŒåé¦ˆçš„äº’è¡¥ä¿¡æ¯å¹¶åœ¨ç¯å¢ƒæ‰°åŠ¨ä¸‹äº§ç”Ÿæ›´é²æ£’çš„ç­–ç•¥ï¼›åŒæ—¶åéªŒä¸ç¡®å®šæ€§å¯ä½œä¸ºå¯è§£é‡Šä¿¡å·ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç½®ä¿¡åº¦ä¸ä¸åŒåé¦ˆé—´ä¸€è‡´æ€§ã€‚

**å…³é”®è¯**ï¼šå¤šåé¦ˆå¥–åŠ±å­¦ä¹ , å¼‚æ„äººç±»åé¦ˆ, è´å¶æ–¯æ¨æ–­, æ½œåœ¨å¥–åŠ±å‡½æ•°, æ‘Šé”€å˜åˆ†æ¨æ–­, è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰, å¥–åŠ±ç¼–ç å™¨, åé¦ˆç‰¹å®šä¼¼ç„¶å»ºæ¨¡, å¥–åŠ±åéªŒåˆ†å¸ƒ, å¥–åŠ±ä¸ç¡®å®šæ€§ä¼°è®¡, é²æ£’å¼ºåŒ–å­¦ä¹ 

**è¯„åˆ†**ï¼š42

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15206v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15206v1.pdf)

---

## [13. COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression](https://arxiv.org/abs/2602.15200v1)

**ä½œè€…**ï¼šDenis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available $\href{https://github.com/mts-ai/COMPOT}{here}$.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCOMPOTæå‡ºä¸€ç§æ— éœ€å†è®­ç»ƒã€ä»…ç”¨å°‘é‡æ ¡å‡†æ•°æ®çš„Transformeråè®­ç»ƒå‹ç¼©æ¡†æ¶ï¼Œé€šè¿‡æ­£äº¤å­—å…¸çš„é—­å¼æ›´æ–°ä¸ä¸€æ¬¡æ€§ç¨€ç–ç¼–ç å®ç°æ›´ä¼˜çš„å‹ç¼©-ç²¾åº¦æƒè¡¡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿæˆªæ–­SVDè¦æ±‚æ‰€æœ‰æƒé‡å…±äº«å•ä¸€ä½ç§©å­ç©ºé—´ï¼Œå¯¼è‡´ä¸­ç­‰å‹ç¼©ç‡ä¸‹ç²¾åº¦æ˜æ˜¾ä¸‹é™ï¼›è€Œç¨€ç–å­—å…¸å­¦ä¹ è™½æ›´çµæ´»ï¼Œä½†é€šå¸¸éœ€è¦è¿­ä»£ä¼˜åŒ–å­—å…¸ä¸ç³»æ•°ï¼Œæˆæœ¬é«˜ä¸”å¤æ‚ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šCOMPOTç”¨æ ¡å‡†æ•°æ®ä¼°è®¡ç¨€ç–æƒé‡åˆ†è§£ï¼šé‡‡ç”¨æ­£äº¤å­—å…¸ä½¿å­—å…¸æ›´æ–°å¯ç”¨Procrustesé—­å¼è§£ï¼Œç³»æ•°å¯è§£æå•æ­¥ç¨€ç–ç¼–ç ï¼Œä»è€Œé¿å…è¿­ä»£ï¼›åŒæ—¶æå‡ºä¸€æ¬¡æ€§åŠ¨æ€åˆ†é…ç­–ç•¥ï¼Œåœ¨å…¨å±€å‹ç¼©é¢„ç®—ä¸‹æŒ‰å±‚æ•æ„Ÿåº¦è‡ªé€‚åº”åˆ†é…å‹ç¼©ç‡ï¼Œå¹¶ä¸PTQå…¼å®¹ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§æ¶æ„ä¸ä»»åŠ¡ä¸Šï¼ŒCOMPOTç›¸è¾ƒå¼ºä½ç§©/ç¨€ç–åŸºçº¿å–å¾—æ›´å¥½çš„è´¨é‡-å‹ç¼©æŠ˜ä¸­ï¼Œå¹¶èƒ½ä¸åè®­ç»ƒé‡åŒ–ç»“åˆå®ç°æ›´æç«¯å‹ç¼©è€Œä¿æŒè¾ƒé«˜ç²¾åº¦ã€‚

**å…³é”®è¯**ï¼šåè®­ç»ƒå‹ç¼©, ä½ç§©åˆ†è§£, ç¨€ç–å­—å…¸å­¦ä¹ , æ­£äº¤å­—å…¸, æ ¡å‡†æ•°æ®é›†, ä¸€é˜¶æ®µç¨€ç–ç¼–ç , å±‚çº§å‹ç¼©ç‡åŠ¨æ€åˆ†é…, åè®­ç»ƒé‡åŒ–

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15200v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15200v1.pdf)

---

## [14. Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge](https://arxiv.org/abs/2602.15184v1)

**ä½œè€…**ï¼šSiying Ma, Mehrdad M. Zadeh, Mauricio Soroco ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§å°†ç›®æ ‡PDEä¸å…¶æ›´åŸºæœ¬ç®€åŒ–å½¢å¼è”åˆè®­ç»ƒçš„å¤šç‰©ç†æ¡†æ¶ï¼Œä»è€Œè®©ç¥ç»ç®—å­æ›´çœæ•°æ®ä¸”å…·æ›´å¼ºOODæ³›åŒ–èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ç¥ç»ç®—å­å¤šä»…æ‹Ÿåˆç›®æ ‡PDEçš„æ¨¡æ‹Ÿæ•°æ®ï¼Œå¿½è§†æ”¯æ’‘è¿™äº›æ–¹ç¨‹çš„æ›´åŸºç¡€ç‰©ç†åŸåˆ™ï¼Œå¯¼è‡´æ•°æ®éœ€æ±‚é«˜ä¸”å¯¹å‚æ•°å˜åŒ–/ä»¿çœŸåˆ°çœŸå®è¿ç§»çš„æ³›åŒ–ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®¾è®¡å¤šç‰©ç†è”åˆè®­ç»ƒï¼šåŒæ—¶ç”¨åŸå§‹PDEä¸å…¶ç®€åŒ–â€œåŸºæœ¬å½¢å¼â€æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒæŠŠåŸºç¡€ç‰©ç†çŸ¥è¯†æ˜¾å¼æ³¨å…¥å­¦ä¹ è¿‡ç¨‹ï¼›æ–¹æ³•ä¸å…·ä½“ç¥ç»ç®—å­æ¶æ„æ— å…³ï¼Œå¯ç›´æ¥å¥—ç”¨åœ¨å¤šç§1D/2D/3D PDEä»»åŠ¡ä¸Šã€‚

**ä¸»è¦ç»“è®º**ï¼šè”åˆå­¦ä¹ åŸºç¡€ç‰©ç†ä¸ç›®æ ‡PDEå¯æ˜¾è‘—æå‡æ•°æ®æ•ˆç‡ã€é™ä½é¢„æµ‹nRMSEï¼Œå¹¶åœ¨ç‰©ç†å‚æ•°åˆ†å¸ƒåç§»ä¸synthetic-to-realç­‰OODåœºæ™¯ä¸‹è·å¾—æ›´ç¨³å¥çš„æ³›åŒ–è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šç¥ç»ç®—å­, ç§‘å­¦æœºå™¨å­¦ä¹ , åå¾®åˆ†æ–¹ç¨‹, ç‰©ç†å…ˆéªŒ, å¤šç‰©ç†è®­ç»ƒ, æ•°æ®æ•ˆç‡, åˆ†å¸ƒå¤–æ³›åŒ–, å‚æ•°æ¼‚ç§»é²æ£’æ€§, ä»¿çœŸåˆ°çœŸå®è¿ç§», å½’ä¸€åŒ–å‡æ–¹æ ¹è¯¯å·®ï¼ˆnRMSEï¼‰, æ¶æ„æ— å…³æ–¹æ³•

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15184v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15184v1.pdf)

---

## [15. Refine Now, Query Fast: A Decoupled Refinement Paradigm for Implicit Neural Fields](https://arxiv.org/abs/2602.15155v2)

**ä½œè€…**ï¼šTianyu Xiong, Skylar Wurster, Han-Wei Shen  
**åˆ†ç±»**ï¼šcs.LG, cs.CE, cs.CV, cs.GR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-16

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Implicit Neural Representations (INRs) have emerged as promising surrogates for large 3D scientific simulations due to their ability to continuously model spatial and conditional fields, yet they face a critical fidelity-speed dilemma: deep MLPs suffer from high inference cost, while efficient embedding-based models lack sufficient expressiveness. To resolve this, we propose the Decoupled Representation Refinement (DRR) architectural paradigm. DRR leverages a deep refiner network, alongside non-parametric transformations, in a one-time offline process to encode rich representations into a compact and efficient embedding structure. This approach decouples slow neural networks with high representational capacity from the fast inference path. We introduce DRR-Net, a simple network that validates this paradigm, and a novel data augmentation strategy, Variational Pairs (VP) for improving INRs under complex tasks like high-dimensional surrogate modeling. Experiments on several ensemble simulation datasets demonstrate that our approach achieves state-of-the-art fidelity, while being up to 27$\times$ faster at inference than high-fidelity baselines and remaining competitive with the fastest models. The DRR paradigm offers an effective strategy for building powerful and practical neural field surrogates and \rev{INRs in broader applications}, with a minimal compromise between speed and quality.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºDRRè§£è€¦å¼ç²¾ç‚¼èŒƒå¼ï¼šç”¨ä¸€æ¬¡ç¦»çº¿æ·±åº¦ç²¾ç‚¼æŠŠé«˜è¡¨è¾¾èƒ½åŠ›â€œå‹ç¼©â€è¿›è½»é‡åµŒå…¥ç»“æ„ï¼Œå®ç°éšå¼ç¥ç»åœºé«˜ä¿çœŸä¸”å¿«é€Ÿæ¨ç†ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šINRåœ¨3Dç§‘å­¦ä»¿çœŸæ›¿ä»£å»ºæ¨¡ä¸­é¢ä¸´â€œè´¨é‡-é€Ÿåº¦â€çŸ›ç›¾ï¼šæ·±MLPæ¨ç†æ…¢ä½†ç²¾åº¦é«˜ï¼ŒåµŒå…¥å¼é«˜æ•ˆæ¨¡å‹å¿«ä½†è¡¨è¾¾åŠ›ä¸è¶³ã€‚éœ€è¦ä¸€ç§èƒ½ä¿ç•™é«˜ä¿çœŸåŒæ—¶å°†æ¨ç†è·¯å¾„å˜å¿«çš„ç»“æ„æ€§æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šDRRåœ¨ç¦»çº¿é˜¶æ®µä½¿ç”¨æ·±refinerç½‘ç»œ+éå‚æ•°å˜æ¢å¯¹è¡¨ç¤ºè¿›è¡Œä¸€æ¬¡æ€§ç²¾ç‚¼ï¼Œå°†å¤æ‚ä¿¡æ¯ç¼–ç åˆ°ç´§å‡‘çš„embeddingç»“æ„ä¸­ï¼›åœ¨çº¿æ¨ç†åªèµ°è½»é‡å¿«é€Ÿè·¯å¾„ã€‚ä½œè€…å®ç°DRR-NetéªŒè¯è¯¥èŒƒå¼ï¼Œå¹¶æå‡ºVariational Pairsæ•°æ®å¢å¼ºä»¥æå‡é«˜ç»´ä»£ç†å»ºæ¨¡ç­‰å¤æ‚ä»»åŠ¡ä¸‹çš„INRæ•ˆæœã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šä¸ªé›†åˆä»¿çœŸæ•°æ®é›†ä¸Šï¼Œæ–¹æ³•è¾¾åˆ°SOTAä¿çœŸåº¦ï¼ŒåŒæ—¶ç›¸å¯¹é«˜ä¿çœŸåŸºçº¿æ¨ç†æœ€é«˜æé€Ÿ27Ã—ï¼Œä¸”é€Ÿåº¦ä¸Šä»ä¸æœ€å¿«æ¨¡å‹å…·ç«äº‰åŠ›ï¼Œè¯æ˜DRRèƒ½ä»¥å¾ˆå°è´¨é‡ä»£ä»·å…¼é¡¾å®ç”¨é€Ÿåº¦ä¸ç²¾åº¦ã€‚

**å…³é”®è¯**ï¼šéšå¼ç¥ç»è¡¨ç¤º, éšå¼ç¥ç»åœº, 3D ç§‘å­¦ä»¿çœŸä»£ç†å»ºæ¨¡, é«˜ç»´ä»£ç†å»ºæ¨¡, æ¨ç†åŠ é€Ÿ, è¡¨ç¤ºç²¾ç‚¼, è§£è€¦æ¶æ„, ç¦»çº¿é¢„è®¡ç®—, åµŒå…¥å¼è¡¨ç¤º, éå‚æ•°å˜æ¢, æ•°æ®å¢å¼º

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.15155v2) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.15155v2.pdf)

---

