# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-19

> å…± 30 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (8 ç¯‡)
- [cs.LG](#csLG) (14 ç¯‡)
- [cs.CL](#csCL) (5 ç¯‡)
- [cs.AI](#csAI) (3 ç¯‡)

---

## cs.AI

## [1. Towards a Science of AI Agent Reliability](https://arxiv.org/abs/2602.16666v1)

**ä½œè€…**ï¼šStephan Rabanser, Sayash Kapoor, Peter Kirgis ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CY, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ä»å®‰å…¨å·¥ç¨‹è§†è§’ç³»ç»ŸåŒ–å®šä¹‰å¹¶åº¦é‡â€œAIæ™ºèƒ½ä½“å¯é æ€§â€ï¼Œæå‡º12é¡¹æŒ‡æ ‡æ­ç¤ºå½“å‰é«˜èƒ½åŠ›æ¨¡å‹åœ¨å¯é æ€§ä¸Šä»å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è¯„ä¼°å¾€å¾€ç”¨å•ä¸€æˆåŠŸç‡æ¦‚æ‹¬æ™ºèƒ½ä½“è¡¨ç°ï¼Œä½†åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œæ™ºèƒ½ä½“ä»é¢‘ç¹å‡ºç°ä¸ç¨³å®šã€è„†å¼±å’Œä¸å¯é¢„æµ‹çš„å¤±è´¥ï¼ŒäºŸéœ€æ›´ç»†ç²’åº¦çš„å¯é æ€§è¯„ä»·ä½“ç³»ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ä»ä¸€è‡´æ€§ã€é²æ£’æ€§ã€å¯é¢„æµ‹æ€§å’Œå®‰å…¨æ€§å››ä¸ªç»´åº¦æ„å»º12ä¸ªå…·ä½“å¯é æ€§æŒ‡æ ‡ï¼Œå¹¶åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šç³»ç»Ÿè¯„æµ‹14ä¸ªæ™ºèƒ½ä½“æ¨¡å‹ï¼Œå½¢æˆå¤šç»´â€œæ€§èƒ½ç”»åƒâ€ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è¿‘å¹´çš„æ¨¡å‹èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œä½†åœ¨å¯é æ€§å„ç»´åº¦ä¸Šçš„æ”¹å–„æœ‰é™ä¸”ä¸å‡è¡¡ï¼›æ‰€æå‡ºæŒ‡æ ‡èƒ½æ­ç¤ºä¼ ç»Ÿå•ä¸€æˆåŠŸç‡æ— æ³•æš´éœ²çš„å¤±æ•ˆæ¨¡å¼ï¼Œä¸ºåˆ†æå’Œæ”¹è¿›æ™ºèƒ½ä½“åœ¨çœŸå®ç¯å¢ƒä¸­çš„è¡¨ç°ä¸å¤±æ•ˆæœºç†æä¾›äº†æ–°å·¥å…·ã€‚

**å…³é”®è¯**ï¼šAIä»£ç†, autonomous agents, å¤šæ™ºèƒ½ä½“è¯„ä¼°, agentå¯é æ€§åº¦é‡, é²æ£’æ€§æµ‹è¯•, å®‰å…¨å…³é”®åœºæ™¯, è¡Œä¸ºä¸€è‡´æ€§åˆ†æ, æ•…éšœæ¨¡å¼åˆ†æ, ä»»åŠ¡æ‰§è¡Œç¨³å®šæ€§, æ¨¡å‹æ€§èƒ½å‰–é¢

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16666v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16666v1.pdf)

---

## [2. Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653v1)

**ä½œè€…**ï¼šYangjie Xu, Lujun Li, Lama Sleem ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†åœ¨å·¥ä¸šåœºæ™¯ä¸­ï¼Œå°†â€œAgent Skillâ€èŒƒå¼åº”ç”¨äºå°å‚æ•°é‡è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„æ•ˆæœä¸å±€é™ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨å¯¹éšç§ä¸æˆæœ¬æ•æ„Ÿçš„å·¥ä¸šç¯å¢ƒä¸­ï¼Œé•¿æœŸä¾èµ–é—­æºå¤§æ¨¡å‹APIä¸å¯è¡Œï¼Œè€Œæœ¬åœ°å°æ¨¡å‹åˆåœ¨å¤æ‚è‡ªå®šä¹‰ä»»åŠ¡ä¸Šæ³›åŒ–ä¸è¶³ï¼Œå› æ­¤éœ€è¦éªŒè¯Agent Skillæ¡†æ¶æ˜¯å¦èƒ½æ˜¾è‘—æå‡SLMsçš„å®é™…å¯ç”¨æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæœ¬æ–‡é¦–å…ˆç»™å‡ºAgent Skillè¿‡ç¨‹çš„æ•°å­¦å½¢å¼åŒ–å®šä¹‰ï¼Œç„¶ååœ¨å¤šä¸ªç”¨ä¾‹ä¸Šï¼Œå¯¹ä¸åŒå‚æ•°è§„æ¨¡ï¼ˆä»tinyåˆ°çº¦80Bï¼‰çš„å¼€æº/ä¸“ç”¨å°æ¨¡å‹è¿›è¡Œç³»ç»Ÿå¯¹æ¯”å®éªŒï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå¼€æºä»»åŠ¡ä¸ä¸€ä¸ªçœŸå®ä¿é™©ç†èµ”æ•°æ®é›†ï¼Œä¸»è¦è€ƒå¯ŸæŠ€èƒ½é€‰æ‹©èƒ½åŠ›ã€ä»»åŠ¡å‡†ç¡®ç‡ä¸GPUæ•ˆç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼šæå°æ¨¡å‹åœ¨å¯é æŠ€èƒ½é€‰æ‹©ä¸Šè¡¨ç°è¾ƒå·®ï¼›ä¸­ç­‰è§„æ¨¡SLMsï¼ˆçº¦12Bâ€“30Bï¼‰é€šè¿‡Agent Skillå¯è·å¾—æ˜¾è‘—æ”¶ç›Šï¼›è€Œçº¦80Bå‚æ•°çš„ä»£ç ç‰¹åŒ–æ¨¡å‹åœ¨ä¿æŒæ›´é«˜GPUæ•ˆç‡çš„åŒæ—¶ï¼Œå…¶è¡¨ç°å·²å¯æ¯”è‚©é—­æºåŸºçº¿ï¼Œä¸ºåœ¨ä»¥SLMä¸ºä¸­å¿ƒçš„å·¥ä¸šéƒ¨ç½²ä¸­æœ‰æ•ˆä½¿ç”¨Agent Skillsæä¾›äº†å®è¯æŒ‡å¯¼ã€‚

**å…³é”®è¯**ï¼šå°è¯­è¨€æ¨¡å‹, å¤šæ™ºèƒ½ä½“agent, AgentSkillæ¡†æ¶, ä»£ç å¤§æ¨¡å‹, ä¸Šä¸‹æ–‡å·¥ç¨‹, å·¥ä¸šåœºæ™¯éƒ¨ç½², ä¿é™©ç†èµ”æ•°æ®é›†, å‚æ•°è§„æ¨¡è¯„ä¼°, ä»»åŠ¡æŠ€èƒ½é€‰æ‹©, GPUæ•ˆç‡ä¼˜åŒ–

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16653v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16653v1.pdf)

---

## [3. Creating a digital poet](https://arxiv.org/abs/2602.16578v1)

**ä½œè€…**ï¼šVered Tohar, Tsahi Hayat, Amir Leshem  
**åˆ†ç±»**ï¼šcs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡é€šè¿‡ä¸ºæœŸä¸ƒä¸ªæœˆçš„â€œè¯—æ­Œå·¥ä½œåŠå¼æç¤ºâ€å®è·µï¼Œå±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸é‡æ–°è®­ç»ƒçš„å‰æä¸‹ä¹Ÿèƒ½è¢«å¡‘é€ æˆé£æ ¼ç¨³å®šã€è´¨é‡æ¥è¿‘äººç±»çš„â€œæ•°å­—è¯—äººâ€ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä½œè€…æƒ³æ¢ç©¶ï¼šæœºå™¨æ˜¯å¦èƒ½å†™å‡ºè¢«ä¸¥è‚ƒè¯»è€…è®¤å¯çš„â€œå¥½è¯—â€ï¼Œä»¥åŠå¦‚æœå¯ä»¥ï¼Œè¿™å¯¹è‰ºæœ¯æœ¬è´¨ã€åˆ›é€ åŠ›å’Œä½œè€…èº«ä»½çš„ä¼ ç»Ÿè§‚å¿µæ„å‘³ç€ä»€ä¹ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨ä¸ƒä¸ªæœˆä¸­ï¼Œç ”ç©¶è€…ä»¥å·¥ä½œåŠå½¢å¼å¯¹å¤§æ¨¡å‹è¿›è¡Œå¤šè½®ä¸Šä¸‹æ–‡å†…ã€ä¸“å®¶çº§ç»†è‡´åé¦ˆå’Œè¿­ä»£æç¤ºï¼Œä¸åšå‚æ•°æ›´æ–°ï¼Œé€æ­¥å¡‘é€ å…¶è¯—æ­Œé£æ ¼ä¸ä½œå“æ•´ä½“æ€§ï¼Œå¹¶é€šè¿‡ç›²æµ‹è¯„ä¼°äººç±»ä¸AIè¯—ä½œçš„å¯åŒºåˆ†åº¦ã€‚

**ä¸»è¦ç»“è®º**ï¼šé€šè¿‡è¿­ä»£å¼å·¥ä½œåŠæç¤ºï¼Œæ¨¡å‹å½¢æˆäº†ç»Ÿä¸€é£æ ¼ã€è¿è´¯è¯—é›†å’Œâ€œç¬”å+ä½œè€…å½¢è±¡â€ï¼Œåœ¨äººæ–‡ä¸“ä¸šè¯»è€…çš„ç›²æµ‹ä¸­å…¶è¯—æ­Œä¸åå®¶è¯—æ­Œéš¾ä»¥åŒºåˆ†ï¼Œå¹¶æœ€ç»ˆç”±å•†ä¸šå‡ºç‰ˆç¤¾å‡ºç‰ˆè¯—é›†ï¼Œè¡¨æ˜é•¿å‘¨æœŸæç¤ºå¡‘å½¢å¯ä»¥æ”¯æŒé«˜æ°´å¹³åˆ›ä½œå¹¶é‡æ–°æ¿€æ´»å…³äºåˆ›ä½œä¸»ä½“ä¸ä½œè€…æƒåˆ©çš„è®¨è®ºã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, ç”Ÿæˆå¼, äººæœºåä½œåˆ›ä½œ, ä¸Šä¸‹æ–‡è¿­ä»£åé¦ˆ, è¯—æ­Œç”Ÿæˆ, é£æ ¼å¡‘é€ , ä½œè€…èº«ä»½æ„å»º, ç›²è¯„å®éªŒ, context

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16578v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16578v1.pdf)

---

## cs.CL

## [4. Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents](https://arxiv.org/abs/2602.16699v1)

**ä½œè€…**ï¼šWenxuan Ding, Nicholas Tomlin, Greg Durrett  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡º Calibrate-Then-Act (CTA) æ¡†æ¶ï¼Œè®©LLMæ˜¾å¼æƒè¡¡æ¢ç´¢æˆæœ¬ä¸ä¸ç¡®å®šæ€§ï¼Œä»è€Œåœ¨ä¿¡æ¯æ£€ç´¢ä¸ç¼–ç ç­‰äº¤äº’å¼ä»»åŠ¡ä¸­åšå‡ºæ›´ä¼˜æ¢ç´¢å†³ç­–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°å®ä¸­çš„LLMä»£ç†åœ¨æœç´¢ä¿¡æ¯ã€å†™ä»£ç ç­‰ä»»åŠ¡æ—¶ï¼Œéœ€è¦å¤šè½®ä¸ç¯å¢ƒäº¤äº’ï¼Œæ—¢è¦å‡å°‘è¯•æ¢æˆæœ¬åˆè¦é¿å…é”™è¯¯ï¼Œä½†ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹â€œä½•æ—¶ç»§ç»­æ¢ç´¢ã€ä½•æ—¶ä¸‹ç»“è®ºâ€çš„æ˜¾å¼æˆæœ¬-ä¸ç¡®å®šæ€§æƒè¡¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†ä¿¡æ¯æ£€ç´¢å’Œç®€åŒ–ç¼–ç ä»»åŠ¡å½¢å¼åŒ–ä¸ºå¸¦æ½œåœ¨çŠ¶æ€çš„ä¸ç¡®å®šåºè´¯å†³ç­–é—®é¢˜ï¼Œä¸ºLLMæä¾›å…³äºç¯å¢ƒå…ˆéªŒä¸æˆæœ¬ç»“æ„çš„é¢å¤–ä¸Šä¸‹æ–‡ï¼Œè®¾è®¡CTAæ¡†æ¶å¼•å¯¼æ¨¡å‹å…ˆæ ¡å‡†ï¼ˆä¼°è®¡ä¸ç¡®å®šæ€§ä¸æˆæœ¬æ”¶ç›Šï¼‰å†è¡ŒåŠ¨ï¼Œå¹¶åœ¨æœ‰/æ— å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸‹ä¸åŸºçº¿è¿›è¡Œå¯¹æ¯”ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼Œåœ¨ä¿¡æ¯æ£€ç´¢é—®ç­”å’Œç®€åŒ–ç¼–ç¨‹ä»»åŠ¡ä¸­ï¼Œæ˜¾å¼æ³¨å…¥æˆæœ¬æ”¶ç›Šä¸ä¸ç¡®å®šæ€§ç»“æ„çš„CTAä»£ç†èƒ½å­¦åˆ°æ›´ä¼˜æ¢ç´¢ç­–ç•¥ï¼Œåœ¨ç›¸ä¼¼æˆ–æ›´ä½äº¤äº’æˆæœ¬ä¸‹è·å¾—æ›´é«˜ä»»åŠ¡è¡¨ç°ï¼Œè¯¥ä¼˜åŠ¿åœ¨RLè®­ç»ƒåä¾ç„¶å­˜åœ¨ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, LLMä»£ç†, agent, äººæœºåä½œ, ä¿¡æ¯æ£€ç´¢, é¡ºåºå†³ç­–, æˆæœ¬æ•æ„Ÿæ¢ç´¢, ä¸ç¡®å®šæ€§å»ºæ¨¡, ä»£ç ç”Ÿæˆä¸æµ‹è¯•

**è¯„åˆ†**ï¼š48

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16699v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16699v1.pdf)

---

## [5. Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment](https://arxiv.org/abs/2602.16660v1)

**ä½œè€…**ï¼šYuyan Bu, Xiaohao Liu, ZhaoXing Ren ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºä¸€ç§å¯æ’æ‹”çš„å¤šè¯­ä¸€è‡´æ€§æŸå¤±MLCï¼Œåœ¨ä»…è¿›è¡Œä¸€æ¬¡å¯¹é½æ›´æ–°çš„å‰æä¸‹æ˜¾è‘—æå‡LLMçš„å¤šè¯­è¨€å®‰å…¨å¯¹é½èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šè¯­è¨€å®‰å…¨å¯¹é½å¾€å¾€ä¾èµ–å¤§é‡ç›®æ ‡è¯­è¨€æ ‡æ³¨æˆ–é€è¯­è¨€å¯¹é½ï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ï¼Œéœ€è¦ä¸€ç§åœ¨èµ„æºæœ‰é™æ¡ä»¶ä¸‹å³å¯æ³›åŒ–åˆ°å¤šè¯­è¨€çš„å®‰å…¨å¯¹é½æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨ç°æœ‰å•è¯­å¯¹é½æµç¨‹ä¸­åŠ å…¥å¤šè¯­ä¸€è‡´æ€§ï¼ˆMLCï¼‰æŸå¤±ï¼Œé€šè¿‡æå‡ä¸åŒè¯­è¨€è¡¨ç¤ºå‘é‡çš„å…±çº¿æ€§ï¼Œä½¿æ¨¡å‹åœ¨è¯­ä¹‰å±‚é¢è·¨è¯­è¨€ä¿æŒæ–¹å‘ä¸€è‡´ï¼Œä»è€Œåˆ©ç”¨å¤šè¯­è¨€æç¤ºå˜ä½“å®ç°ä¸€æ¬¡æ€§å¤šè¯­è¨€å¯¹é½ï¼Œæ— éœ€ä½èµ„æºè¯­è¨€çš„é¢å¤–å“åº”çº§ç›‘ç£ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜MLCåœ¨å¤šç§æ¨¡å‹æ¶æ„å’Œå¯¹é½èŒƒå¼ä¸‹éƒ½èƒ½æ˜¾è‘—æå‡å¤šè¯­è¨€å®‰å…¨æ€§ä¸”å¯¹é€šç”¨èƒ½åŠ›å½±å“æœ‰é™ï¼Œå¹¶åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­å±•ç°æ›´å¥½çš„è·¨è¯­ç§æ³›åŒ–ï¼Œä¸ºä½ç›‘ç£æ¡ä»¶ä¸‹çš„å¤šè¯­è¨€ä¸€è‡´æ€§å¯¹é½æä¾›äº†å®ç”¨æ–¹æ¡ˆã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, å®‰å…¨å¯¹é½, å¤šè¯­è¨€ä¸€è‡´æ€§, å¤šè¯­ç§è¡¨ç¤ºå­¦ä¹ , è¯­ä¹‰å‘é‡å¯¹é½, å¯¹é½æŸå¤±å‡½æ•°, è·¨è¯­è¨€æ³›åŒ–, ä½èµ„æºè¯­è¨€åœºæ™¯, å®‰å…¨è¯„ä¼°åŸºå‡†, ml

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16660v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16660v1.pdf)

---

## [6. Who can we trust? LLM-as-a-jury for Comparative Assessment](https://arxiv.org/abs/2602.16610v1)

**ä½œè€…**ï¼šMengjie Qian, Guangzhi Sun, Mark J. F. Gales ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºBT-sigmaæ¨¡å‹ï¼Œåœ¨â€œå¤šLLMè¯„å§”â€åœºæ™¯ä¸‹ä»æˆå¯¹æ¯”è¾ƒä¸­åŒæ—¶æ¨æ–­è¢«è¯„å¯¹è±¡æ’åå’Œå„è¯„å§”å¯ä¿¡åº¦ï¼Œä»è€Œæ›´å¯é åœ°ç”¨LLMåšè‡ªåŠ¨è¯„æµ‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ç”¨LLMåšNLGè‡ªåŠ¨è¯„ä¼°æ—¶é€šå¸¸ç®€å•å¹³å‡å¤šä¸ªLLMè¯„å§”æˆ–åªç”¨å•ä¸€è¯„å§”ï¼Œå¿½ç•¥äº†ä¸åŒLLMåœ¨ä»»åŠ¡ã€ç»´åº¦ä¸Šçš„å¯é æ€§å·®å¼‚åŠåˆ¤æ–­æ¦‚ç‡çš„ä¸ä¸€è‡´å’Œåå·®ï¼Œä¸”å¾€å¾€ç¼ºä¹äººå·¥æ ‡æ³¨æ¥æ ¡å‡†è¯„å§”ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨Bradley-Terryæˆå¯¹æ¯”è¾ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œå¼•å…¥å¯¹æ¯ä¸ªLLMè¯„å§”çš„åˆ¤åˆ«å‚æ•°ï¼ˆdiscriminatorï¼‰ï¼Œæ„æˆBT-sigmaï¼šé€šè¿‡ä»…ä¾èµ–LLMäº§ç”Ÿçš„æˆå¯¹æ¯”è¾ƒæ•°æ®ï¼Œè”åˆä¼°è®¡å€™é€‰æ–‡æœ¬çš„è´¨é‡å¾—åˆ†ä¸å„è¯„å§”çš„å¯é æ€§ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹å¯¹æ¯”è¾ƒç»“æœè¿›è¡Œèšåˆå’Œæ’åã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šä¸ªNLGè¯„æµ‹åŸºå‡†ä¸Šï¼ŒBT-sigmaç›¸æ¯”ç®€å•å¹³å‡ç­‰æ–¹æ³•æ˜¾è‘—æé«˜äº†æ’åæ•ˆæœï¼Œå…¶å­¦åˆ°çš„è¯„å§”åˆ¤åˆ«å‚æ•°ä¸LLMåˆ¤æ–­çš„å¾ªç¯ä¸€è‡´æ€§é«˜åº¦ç›¸å…³ï¼Œå¯è¢«è§†ä¸ºä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„â€œæ— ç›‘ç£æ ¡å‡†â€æœºåˆ¶ï¼Œç”¨äºæ›´å¥½åœ°èšåˆå¤šLLMè¯„å§”çš„æ„è§ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, LLMè¯„ä¼°, æ¯”è¾ƒè¯„ä¼°, Bradley-Terryæ¨¡å‹, åˆ¤å†³å¯é æ€§å»ºæ¨¡, æ— ç›‘ç£æ ¡å‡†, è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä»·, å¤šè£åˆ¤èšåˆ, æ¨¡å‹æ’åºæ¨æ–­

**è¯„åˆ†**ï¼š36

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16610v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16610v1.pdf)

---

## [7. ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models](https://arxiv.org/abs/2602.16609v1)

**ä½œè€…**ï¼šAntoine Chaffin, Luca Arnaboldi, AmÃ©lie Chatelain ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.IR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡è¡¨æ˜ï¼šé’ˆå¯¹å¤šå‘é‡æ£€ç´¢æ¨¡å‹ï¼ˆColBERTï¼‰è¿›è¡Œå¤§è§„æ¨¡ä¸“é—¨é¢„è®­ç»ƒï¼Œæ¯”åªåœ¨å¼ºå•å‘é‡æ¨¡å‹ä¸Šåšå°æ­¥è’¸é¦èƒ½å¾—åˆ°æ˜¾è‘—æ›´å¼ºçš„å¤šå‘é‡æ¨¡å‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šå‘é‡æ£€ç´¢æ¨¡å‹å¤§å¤šåªæ˜¯ä¾èµ–å·²é¢„è®­ç»ƒå¥½çš„å•å‘é‡æ¨¡å‹åšä¸€å°æ­¥çŸ¥è¯†è’¸é¦ï¼Œå°šä¸æ¸…æ¥šç›´æ¥å¯¹å¤šå‘é‡æ¶æ„åšå¤§è§„æ¨¡é¢„è®­ç»ƒæ˜¯å¦æ›´ä¼˜ï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆå¤ç”¨å·²æœ‰æ¨¡å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åœ¨å…¬å¼€æ•°æ®ä¸Šä»å¤´å¤§è§„æ¨¡é¢„è®­ç»ƒColBERTæ¶æ„å¾—åˆ°ColBERT-Zeroï¼Œå¹¶ç³»ç»Ÿå¯¹æ¯”ï¼šä»…å°æ­¥KDã€åŠ å…¥ç›‘ç£å¾®è°ƒ+KDã€ä»¥åŠåœ¨ä¸åŒé¢„è®­ç»ƒ/å¾®è°ƒé…ç½®ä¸‹çš„æ€§èƒ½å·®å¼‚ï¼ŒåŒæ—¶å‘å¸ƒå¤šä¸ªä¸­é—´checkpointç”¨äºå®éªŒã€‚

**ä¸»è¦ç»“è®º**ï¼šå…¨ç¨‹åŸºäºColBERTæ¶æ„è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒçš„ColBERT-Zeroåœ¨åŒç­‰è§„æ¨¡ä¸‹è¶…è¶ŠåŸºäºæ›´å¼ºé—­æºæ•°æ®çš„GTE-ModernColBERTåŠå…¶åº•åº§GTE-ModernBERTï¼Œè¯´æ˜å¤šå‘é‡æ¨¡å‹æœ¬èº«å€¼å¾—ç‹¬ç«‹é¢„è®­ç»ƒï¼›ä»…åšå°æ­¥KDä¸è¶³ï¼Œä½†è‹¥å…ˆåŠ ä¸€æ®µç›‘ç£è®­ç»ƒå†KDï¼Œå¯åœ¨è·³è¿‡æœ€æ˜‚è´µçš„æ— ç›‘ç£é˜¶æ®µçš„åŒæ—¶æ¥è¿‘å®Œå…¨é¢„è®­ç»ƒæ•ˆæœï¼Œä¸”å¾®è°ƒä¸é¢„è®­ç»ƒè®¾ç½®çš„ä¸€è‡´æ€§å¯¹è¿ç§»æ€§èƒ½è‡³å…³é‡è¦ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, å¤šå‘é‡æ£€ç´¢, ColBERT, çŸ¥è¯†è’¸é¦, è¯­ä¹‰æœç´¢, è¡¨ç¤ºå­¦ä¹ , å¯¹æ¯”å­¦ä¹ , é¢„è®­ç»ƒç­–ç•¥, rag

**è¯„åˆ†**ï¼š36

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16609v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16609v1.pdf)

---

## [8. CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes](https://arxiv.org/abs/2602.16607v1)

**ä½œè€…**ï¼šMiguel Marques, Ana LuÃ­sa Fernandes, Ana Filipa Pacheco ç­‰ 13 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªé¢å‘æ¬§æ´²è‘¡è¯­å¸‚æ”¿ä¼šè®®çºªè¦çš„ç»†ç²’åº¦ä¸»é¢˜æ‘˜è¦æ•°æ®é›† CitiLink-Summï¼Œå¹¶åœ¨å…¶ä¸Šå»ºç«‹è‡ªåŠ¨æ‘˜è¦åŸºçº¿ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¸‚æ”¿ä¼šè®®çºªè¦ç¯‡å¹…é•¿ä¸”è¯­è¨€å¤æ‚ï¼Œç°æœ‰å°¤å…¶æ˜¯ä½èµ„æºè¯­è¨€ï¼ˆå¦‚æ¬§æ´²è‘¡è¯­ï¼‰åœ¨è¯¥è¡Œæ”¿é¢†åŸŸç¼ºä¹é«˜è´¨é‡äººå·¥æ‘˜è¦æ•°æ®é›†ï¼Œå¯¼è‡´è‡ªåŠ¨æ‘˜è¦æ¨¡å‹éš¾ä»¥å¼€å‘ä¸è¯„æµ‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ä»å¸‚æ”¿ä¼šè®®çºªè¦ä¸­æŠ½å–è®¨è®ºè®®é¢˜ï¼Œæ•´ç†æˆ100ç¯‡æ–‡æ¡£ä¸2,322æ¡äººå·¥æ’°å†™çš„ä¸»é¢˜çº§æ‘˜è¦ï¼Œå¹¶ä½¿ç”¨BARTã€PRIMERAç­‰ç”Ÿæˆæ¨¡å‹åŠå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒä¸è¯„æµ‹ï¼Œé‡‡ç”¨ROUGEã€BLEUã€METEORã€BERTScoreç­‰æŒ‡æ ‡å»ºç«‹åŸºçº¿ã€‚

**ä¸»è¦ç»“è®º**ï¼šCitiLink-Summæˆä¸ºé¦–ä¸ªé¢å‘æ¬§æ´²è‘¡è¯­å¸‚æ”¿é¢†åŸŸçš„æ‘˜è¦åŸºå‡†èµ„æºï¼Œä¸ºå¤„ç†å¤æ‚è¡Œæ”¿æ–‡æœ¬çš„è‡ªåŠ¨æ‘˜è¦ç ”ç©¶æä¾›äº†æ•°æ®åŸºç¡€å’Œæ€§èƒ½å‚è€ƒï¼Œå¹¶æ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šä»æœ‰æ˜æ˜¾æ”¹è¿›ç©ºé—´ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, ç”Ÿæˆå¼æ¨¡å‹, è‡ªåŠ¨æ–‡æœ¬æ‘˜è¦, ç¥ç»ç½‘ç»œ, transformer, è¯­ä¹‰è¯„ä¼°, ä½èµ„æºè¯­è¨€å¤„ç†, æ”¿åºœä¼šè®®çºªè¦, è‘¡è„ç‰™è¯­NLP

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16607v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16607v1.pdf)

---

## cs.CV

## [9. TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos](https://arxiv.org/abs/2602.16711v1)

**ä½œè€…**ï¼šNamitha Padmanabhan, Matthew Gwilliam, Abhinav Shrivastava  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šTeCoNeRV æå‡ºä¸€ç§åˆ©ç”¨æ—¶é—´ä¸€è‡´æ€§çš„æ–°å‹å¯å‹ç¼©ç¥ç»è§†é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨æ˜¾è‘—é™ä½æ¯”ç‰¹ç‡å’Œå†…å­˜çš„åŒæ—¶æå‡é«˜åˆ†è¾¨ç‡è§†é¢‘å‹ç¼©è´¨é‡ä¸ç¼–ç é€Ÿåº¦ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŸºäºéšå¼ç¥ç»è¡¨ç¤ºçš„è§†é¢‘å‹ç¼©æ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªè§†é¢‘å•ç‹¬è¿‡æ‹Ÿåˆï¼Œéš¾ä»¥å…¼é¡¾é«˜åˆ†è¾¨ç‡ã€å‹ç¼©ç‡å’Œç¼–ç é€Ÿåº¦ï¼›ç°æœ‰è¶…ç½‘ç»œæ–¹æ³•è™½å¿«ä½†è´¨é‡ä½ã€ç æµå¤§ä¸”é«˜åˆ†è¾¨ç‡å†…å­˜å¼€é”€è¿‡é«˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šTeCoNeRV é€šè¿‡ä¸‰ç‚¹æ”¹è¿›ï¼šå°†è§†é¢‘åˆ’åˆ†ä¸ºç©ºé—´â€“æ—¶é—´ patch tubelets åˆ†åˆ«é¢„æµ‹æƒé‡ä»¥é™ä½é¢„è®­ç»ƒå†…å­˜ï¼›é‡‡ç”¨åªå­˜è¿ç»­ç‰‡æ®µæƒé‡å·®åˆ†çš„æ®‹å·®å­˜å‚¨ä»¥ç¼©å°ç æµï¼›å¼•å…¥æ—¶é—´ä¸€è‡´æ€§æ­£åˆ™ï¼Œä½¿æƒé‡éšæ—¶é—´å˜åŒ–ä¸è§†é¢‘å†…å®¹å˜åŒ–å¯¹é½ï¼Œä»è€Œæå‡å‹ç¼©æ•ˆç‡ä¸è¡¨ç¤ºç¨³å®šæ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ UVGã€HEVCã€MCL-JCV ç­‰æ•°æ®é›†ä¸Šï¼ŒTeCoNeRV åœ¨ 480p/720p/1080p ä¸Šé¦–æ¬¡å®ç°è¶…ç½‘ç»œæ–¹æ³•çš„ç³»ç»Ÿæ€§è¯„æµ‹ï¼Œç›¸æ¯”åŸºçº¿åœ¨ 480p å’Œ 720p ä¸Šåˆ†åˆ«æå‡çº¦ 2.47dB å’Œ 5.35dB PSNRï¼ŒåŒæ—¶æ¯”ç‰¹ç‡é™ä½çº¦ 36%ã€ç¼–ç é€Ÿåº¦æå‡ 1.5â€“3 å€ï¼Œå¹¶æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ã€‚

**å…³é”®è¯**ï¼šç¥ç»ç½‘ç»œ, éšå¼ç¥ç»è¡¨ç¤º, è§†é¢‘å‹ç¼©, è¶…ç½‘ç»œ, æƒé‡é¢„æµ‹, æ—¶åºä¸€è‡´æ€§æ­£åˆ™, æ®‹å·®ç¼–ç , é«˜åˆ†è¾¨ç‡è§†é¢‘, rag

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16711v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16711v1.pdf)

---

## [10. Are Object-Centric Representations Better At Compositional Generalization?](https://arxiv.org/abs/2602.16689v1)

**ä½œè€…**ï¼šFerdinand Kapl, Amir Mohammad Karimi Mamaghan, Maximilian Seitzer ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ç³»ç»Ÿæ¯”è¾ƒäº†å¸¦/ä¸å¸¦ç‰©ä½“å½’çº³åç½®çš„è§†è§‰ç¼–ç å™¨ï¼Œå‘ç°ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºåœ¨å—æ•°æ®é‡ã€æ•°æ®å¤šæ ·æ€§æˆ–ä¸‹æ¸¸ç®—åŠ›é™åˆ¶æ—¶èƒ½æ˜¾è‘—æå‡ç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç»„åˆæ³›åŒ–æ˜¯äººç±»è®¤çŸ¥æ ¸å¿ƒèƒ½åŠ›ï¼Œä½†å½“å‰è§†è§‰æ¨¡å‹åœ¨é‡åˆ°â€œç†Ÿæ‚‰å±æ€§çš„æ–°ç»„åˆâ€æ—¶è¡¨ç°ä¸ä½³ï¼›è™½ç„¶ç†è®ºä¸Šç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºè¢«è®¤ä¸ºæœ‰åŠ©äºç»„åˆæ³›åŒ–ï¼Œä½†åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸­çš„ç³»ç»Ÿæ€§å®è¯è¯æ®ä»ç„¶ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åŸºäºä¸‰ä¸ªäººå·¥å¯æ§è§†è§‰ä¸–ç•Œï¼ˆCLEVRTexã€Super-CLEVRã€MOVi-Cï¼‰æ„å»ºVQAåŸºå‡†ï¼Œæ§åˆ¶è®­ç»ƒæ•°æ®å¤šæ ·æ€§ã€æ ·æœ¬é‡ã€è¡¨ç¤ºç»´åº¦ã€ä¸‹æ¸¸æ¨¡å‹å®¹é‡å’Œç®—åŠ›ï¼Œå¹¶ä»¥DINOv2å’ŒSigLIP2ä¸ºåŸºåº•ï¼Œå¯¹æ¯”å…¶åŸå§‹è‡´å¯†è¡¨ç¤ºä¸åŠ å…¥ç‰©ä½“ä¸­å¿ƒåç½®çš„OCç‰ˆæœ¬åœ¨æœªè§å±æ€§ç»„åˆä¸Šçš„è¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼š(1) åœ¨æ›´å›°éš¾çš„ç»„åˆæ³›åŒ–è®¾ç½®ä¸­ï¼Œç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºæ˜æ˜¾ä¼˜äºè‡´å¯†è¡¨ç¤ºï¼›(2) è‡´å¯†è¡¨ç¤ºåªåœ¨è¾ƒç®€å•åœºæ™¯ä¸­å ä¼˜ä¸”å¾€å¾€éœ€è¦æ›´å¤šä¸‹æ¸¸ç®—åŠ›ï¼›(3) OCæ¨¡å‹åœ¨æ ·æœ¬æ•ˆç‡ä¸Šæ›´å¥½ï¼Œåœ¨æ•°æ®æˆ–å¤šæ ·æ€§ä¸è¶³æ—¶æ³›åŒ–æ›´å¼ºï¼Œè€Œè‡´å¯†æ¨¡å‹åªæœ‰åœ¨æ•°æ®å’Œå¤šæ ·æ€§è¶³å¤Ÿå¤§æ—¶æ‰èƒ½è¿½å¹³æˆ–è¶…è¶Šï¼›æ•´ä½“è€Œè¨€ï¼Œå½“æ•°æ®è§„æ¨¡ã€å¤šæ ·æ€§æˆ–ç®—åŠ›ä»»ä¸€å—é™æ—¶ï¼Œç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºæ›´èƒ½æ”¯æŒç»„åˆæ³›åŒ–ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, è§†è§‰é—®ç­”, ç»„åˆæ³›åŒ–, å¯¹è±¡ä¸­å¿ƒè¡¨ç¤º, è§†è§‰ç¼–ç å™¨, DINOv2, SigLIP2, æ ·æœ¬æ•ˆç‡, machine learning

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16689v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16689v1.pdf)

---

## [11. Learning Situated Awareness in the Real World](https://arxiv.org/abs/2602.16682v1)

**ä½œè€…**ï¼šChuhan Li, Ruilin Han, Joy Hsu ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºé’ˆå¯¹ç¬¬ä¸€äººç§°è§†è§’çš„â€œæƒ…å¢ƒæ„ŸçŸ¥â€è¯„ä¼°åŸºå‡†SAW-Benchï¼Œæ­ç¤ºç°æœ‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è§‚å¯Ÿè€…ä¸­å¿ƒç©ºé—´æ¨ç†ä¸Šä¸äººç±»å­˜åœ¨å·¨å¤§å·®è·ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ¨¡æ€è¯„æµ‹å¤šå…³æ³¨åœºæ™¯å†…ç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œè€Œå¾ˆå°‘è€ƒå¯Ÿç›¸æœº/è§‚å¯Ÿè€…è‡ªèº«çš„è§†è§’ã€å§¿æ€ä¸è¿åŠ¨ç›¸å…³çš„æ¨ç†èƒ½åŠ›ï¼Œå› æ­¤éš¾ä»¥å…¨é¢è¡¡é‡æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­çš„æƒ…å¢ƒæ„ŸçŸ¥ä¸è¡ŒåŠ¨ç›¸å…³ç†è§£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æ„å»ºSAW-BenchåŸºå‡†ï¼šä½¿ç”¨Ray-Ban Metaæ™ºèƒ½çœ¼é•œåœ¨å¤šç§å®¤å†…å¤–ç¯å¢ƒä¸­å½•åˆ¶786æ®µçœŸå®ç¬¬ä¸€äººç§°è§†é¢‘ï¼Œäººå·¥æ ‡æ³¨2071ä¸ªä¸è§‚å¯Ÿè€…ç›¸å…³çš„é—®ç­”æ ·æœ¬ï¼Œè¦†ç›–å…­ç±»æƒ…å¢ƒæ„ŸçŸ¥ä»»åŠ¡ï¼Œå¹¶ç³»ç»Ÿè¯„æµ‹åŒ…æ‹¬Gemini 3 Flashåœ¨å†…çš„å¤šç§å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹è¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒæ˜¾ç¤ºæœ€å¼ºæ¨¡å‹ä¸äººç±»ä¹‹é—´ä»æœ‰çº¦37.66%çš„æ€§èƒ½å·®è·ï¼Œæ¨¡å‹è™½èƒ½åˆ©ç”¨éƒ¨åˆ†å‡ ä½•çº¿ç´¢ï¼Œä½†éš¾ä»¥å½¢æˆå®Œæ•´çš„ç›¸æœºå‡ ä½•ä¸è¿è´¯ç©ºé—´è¡¨å¾ï¼Œå¯¼è‡´ç³»ç»Ÿæ€§ç©ºé—´æ¨ç†é”™è¯¯ï¼Œè¡¨æ˜éœ€è¦ä¸“é—¨é¢å‘è§‚å¯Ÿè€…ä¸­å¿ƒã€å…·èº«åŒ–æƒ…å¢ƒæ™ºèƒ½çš„æ–°æ–¹æ³•ä¸è®­ç»ƒèŒƒå¼ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹, æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, agent, è§‚å¯Ÿè€…è§†è§’ç†è§£, è‡ªæˆ‘å®šä½, ç©ºé—´æ¨ç†è¯„æµ‹, ç¬¬ä¸€äººç§°è§†é¢‘, æƒ…å¢ƒæ„ŸçŸ¥åŸºå‡†, äººæœºæ€§èƒ½å·®è·åˆ†æ

**è¯„åˆ†**ï¼š39

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16682v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16682v1.pdf)

---

## [12. VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection](https://arxiv.org/abs/2602.16681v1)

**ä½œè€…**ï¼šYingyuan Yang, Tian Lan, Yifei Gao ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šVETime æå‡ºä¸€ä¸ªç»Ÿä¸€æ—¶åºä¸è§†è§‰æ¨¡æ€çš„é›¶æ ·æœ¬æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œåœ¨ä¿æŒç²¾ç»†ç‚¹çº§å®šä½çš„åŒæ—¶è·å¾—å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰1Dæ—¶åºæ¨¡å‹ç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡ï¼Œè€Œ2Dè§†è§‰æ¨¡å‹åˆå› æ—¶é—´å¯¹é½ä¸è¶³å’Œä¿¡æ¯ç“¶é¢ˆéš¾ä»¥ç²¾ç»†å®šä½å¼‚å¸¸ï¼ŒäºŒè€…å­˜åœ¨æ ¹æœ¬æ€§æƒè¡¡ï¼Œé™åˆ¶äº†é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ•ˆæœã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šVETime é€šè¿‡å¯é€†å›¾åƒè½¬æ¢å’Œè¡¥ä¸çº§æ—¶é—´å¯¹é½å»ºç«‹å…±äº«çš„è§†è§‰-æ—¶é—´è½´ï¼Œå¹¶ç»“åˆå¼‚å¸¸çª—å£å¯¹æ¯”å­¦ä¹ ä¸ä»»åŠ¡è‡ªé€‚åº”å¤šæ¨¡æ€èåˆï¼Œä»¥åŠ¨æ€æ•´åˆæ—¶åºå’Œè§†è§‰ä¸¤ç§æ„ŸçŸ¥ä¼˜åŠ¿ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ VETime åœ¨é›¶æ ·æœ¬æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°æ›´é«˜çš„å¼‚å¸¸å®šä½ç²¾åº¦å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ï¼Œå…¶ä»£ç å·²å¼€æºä¾¿äºå¤ç°ä¸æ‰©å±•ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, å¤šæ¨¡æ€å¯¹é½, æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹, é›¶æ ·æœ¬æ£€æµ‹, å¯¹æ¯”å­¦ä¹ , è§†è§‰æ—¶é—´èåˆ, å¯é€†å›¾åƒè½¬æ¢, æ—¶é—´ç‰‡å¯¹é½, å¤šæ¨¡æ€èåˆ, context

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16681v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16681v1.pdf)

---

## [13. PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction](https://arxiv.org/abs/2602.16669v1)

**ä½œè€…**ï¼šBo Lang, Nirav Savaliya, Zhihao Zheng ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPredMapNet æå‡ºä¸€ä¸ªç»“åˆå†å²è·Ÿè¸ªä¸çŸ­æœŸé¢„æµ‹çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå®ç°åœ¨çº¿é«˜æ¸…çŸ¢é‡åœ°å›¾çš„æ—¶åºä¸€è‡´æ„å»ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŸºäºæŸ¥è¯¢çš„HDåœ°å›¾æ„å»ºæ–¹æ³•é‡‡ç”¨éšæœºæŸ¥è¯¢åˆå§‹åŒ–ä¸”ä»…åšéšå¼æ—¶é—´å»ºæ¨¡ï¼Œå¯¼è‡´å…¨å±€åœ°å›¾åœ¨æ—¶é—´ä¸Šä¸ç¨³å®šã€å…ƒç´ ä¸è¿ç»­ï¼Œéš¾ä»¥æ»¡è¶³è‡ªåŠ¨é©¾é©¶å¯¹å¯é æ—¶ç©ºä¸€è‡´åœ°å›¾çš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ–¹æ³•åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥æŸ¥è¯¢ç”Ÿæˆå™¨ï¼Œä»¥ç©ºé—´å¯¹é½çš„è¯­ä¹‰æ©ç åˆå§‹åŒ–æŸ¥è¯¢è·å–å…¨å±€åœºæ™¯è¯­ä¹‰ï¼›2ï¼‰æ„å»ºå†å²æ …æ ¼åŒ–åœ°å›¾è®°å¿†ï¼Œæ˜¾å¼å­˜å‚¨æ¯ä¸ªå®ä¾‹çš„ç²¾ç»†åœ°å›¾ï¼›3ï¼‰é€šè¿‡å†å²åœ°å›¾å¼•å¯¼æ¨¡å—å°†æ …æ ¼åŒ–ä¿¡æ¯èåˆè¿›è·Ÿè¸ªæŸ¥è¯¢ä»¥å¢å¼ºæ—¶é—´è¿ç»­æ€§ï¼›4ï¼‰åˆ©ç”¨çŸ­æœŸæœªæ¥å¼•å¯¼æ¨¡å—åŸºäºå†å²è½¨è¿¹é¢„æµ‹å®ä¾‹çš„çŸ­æœŸè¿åŠ¨ï¼Œå°†æœªæ¥ä½ç½®ä½œä¸ºå…ˆéªŒçº¦æŸé¿å…ä¸åˆç†é¢„æµ‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ nuScenes å’Œ Argoverse2 ä¸Šï¼ŒPredMapNet åœ¨ç²¾åº¦å’Œæ—¶åºä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰SOTAæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¥½æ¨ç†æ•ˆç‡ï¼ŒéªŒè¯äº†å¼•å…¥æ˜¾å¼å†å²è®°å¿†ä¸çŸ­æœŸé¢„æµ‹å¯¹åœ¨çº¿HDçŸ¢é‡åœ°å›¾æ„å»ºçš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, transformer, è¯­ä¹‰æ„ŸçŸ¥æŸ¥è¯¢, çŸ¢é‡åŒ–é«˜æ¸…åœ°å›¾, æ—¶åºä¸€è‡´æ€§å»ºå›¾, å†å²è½¨è¿¹è®°å¿†, çŸ­æœŸæœªæ¥é¢„æµ‹, è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£, ç«¯åˆ°ç«¯åœ¨çº¿å»ºå›¾

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16669v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16669v1.pdf)

---

## [14. Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge](https://arxiv.org/abs/2602.16664v1)

**ä½œè€…**ï¼šJiaming Liu, Felix Petersen, Yunhe Gao ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºè‡ªç›‘ç£è¯­ä¹‰æ¡¥æ¥(SSB)æ¡†æ¶ï¼Œå°†è‡ªç›‘ç£è§†è§‰ç¼–ç å™¨æä¾›çš„å‡ ä½•ä¸€è‡´è¯­ä¹‰ç©ºé—´ä¸æ‰©æ•£æ¡¥æ¨¡å‹ç»“åˆï¼Œå®ç°æ— éœ€æˆå¯¹æ•°æ®çš„é«˜ä¿çœŸè·¨åŸŸå›¾åƒç¿»è¯‘ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ— é…å¯¹å›¾åƒç¿»è¯‘æ–¹æ³•ä¸­ï¼Œå¯¹æŠ—å¼æ–¹æ³•ä¾èµ–ç›®æ ‡åŸŸåˆ¤åˆ«å™¨ï¼Œæ³›åŒ–æ€§å·®ï¼›æ‰©æ•£åæ¼”æ–¹æ³•åˆå› å™ªå£°æ½œç©ºé—´åæ¼”ä¸å‡†å¯¼è‡´ç»†èŠ‚å’Œç»“æ„å¤±çœŸï¼Œå°¤å…¶åœ¨åŒ»ç–—å›¾åƒç­‰é«˜è¦æ±‚åœºæ™¯è¡¨ç°ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåˆ©ç”¨è‡ªç›‘ç£è§†è§‰ç¼–ç å™¨å­¦ä¹ å¯¹å¤–è§‚å˜åŒ–ä¸æ•æ„Ÿä½†å‡ ä½•ç»“æ„ä¸€è‡´çš„å…±äº«è¯­ä¹‰æ½œç©ºé—´ï¼Œä»¥æ­¤ä½œä¸ºæ¡ä»¶è¾“å…¥æ‰©æ•£æ¡¥æ¨¡å‹ï¼Œåœ¨æ— è·¨åŸŸç›‘ç£ä¸‹å®ç°ç»“æ„ä¿çœŸçš„å›¾åƒåˆ°å›¾åƒè½¬æ¢ï¼Œå¹¶å¯æ‰©å±•åˆ°æ–‡æœ¬å¼•å¯¼ç¼–è¾‘ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§å›°éš¾åŒ»ç–—å›¾åƒåˆæˆä»»åŠ¡ä¸­ï¼ŒSSBåœ¨åŸŸå†…ä¸åŸŸå¤–æµ‹è¯•å‡ä¼˜äºå¼ºåŸºçº¿ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–æ€§ä¸ç»“æ„ä¸€è‡´æ€§ï¼ŒåŒæ—¶è¯æ˜è¯¥æ¡†æ¶å¯è‡ªç„¶æ‰©å±•åˆ°é«˜è´¨é‡çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç¼–è¾‘ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ , è¯­ä¹‰åµŒå…¥, æ— é…å¯¹å›¾åƒç¿»è¯‘, è·¨åŸŸå›¾åƒåˆæˆ, åŒ»å­¦å½±åƒåˆæˆ, æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘, diffusion

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16664v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16664v1.pdf)

---

## [15. Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face](https://arxiv.org/abs/2602.16569v1)

**ä½œè€…**ï¼šNicolÃ² Di Domenico, Annalisa Franco, Matteo Ferrara ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.CR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºåŸºäºArc2Faceçš„Arc2Morphæ–¹æ³•ï¼Œåœ¨ä¿æŒå¤šä¸»ä½“èº«ä»½ç‰¹å¾çš„åŒæ—¶ç”Ÿæˆé«˜æ”»å‡»åŠ›çš„é¢éƒ¨ä¼ªé€ å›¾åƒï¼Œå¯¹ç°æœ‰äººè„¸è¯†åˆ«ç³»ç»Ÿæ„æˆä¸¥é‡å¨èƒã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç”µå­èº«ä»½è¯å’ŒæŠ¤ç…§é‡‡é›†æµç¨‹ä¸­ç¼ºä¹å®æ—¶æ´»ä½“ç›‘ç£ï¼Œä½¿å¾—æ”»å‡»è€…å¯ä»¥ç”¨èåˆå¤šäººçš„äººè„¸å›¾åƒç»•è¿‡è¯†åˆ«ç³»ç»Ÿï¼Œå› æ­¤éœ€è¦æ›´é€¼çœŸä¸”èº«ä»½ä¿¡æ¯å¯æ§çš„å˜è„¸ç”Ÿæˆæ–¹æ³•æ¥è¯„ä¼°ä¸å¼ºåŒ–é˜²å¾¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåˆ©ç”¨Arc2Faceè¿™ä¸€èº«ä»½æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå°†å¤šä¸ªäººçš„ç´§å‡‘èº«ä»½è¡¨å¾åœ¨ç‰¹å¾ç©ºé—´ä¸­èåˆï¼Œå†ç”±æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸã€åŒæ—¶ä¿ç•™å¤šä¸»ä½“èº«ä»½ç‰¹å¾çš„ä¼ªé€ äººè„¸å›¾åƒï¼Œå¹¶åœ¨å¤šç§æ•°æ®é›†ä¸Šä¸å¤šç§ç°æœ‰å˜è„¸æŠ€æœ¯è¿›è¡Œå¯¹æ¯”ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒArc2Morphåœ¨ä¸¤ä¸ªå¤§è§„æ¨¡ä¿ç•™æ•°æ®é›†å’Œä¸¤ä¸ªæ–°æ„å»ºæ•°æ®é›†ä¸Šå–å¾—ä¸ä¼ ç»Ÿå…³é”®ç‚¹ï¼ˆlandmarkï¼‰æ–¹æ³•ç›¸å½“çš„é«˜â€œæ”»å‡»æ½œåŠ›â€ï¼ŒéªŒè¯äº†å…¶åœ¨ç”ŸæˆåŒæ—¶ä¿ç•™å’Œç®¡ç†å¤šé‡èº«ä»½ä¿¡æ¯çš„ä¼ªé€ äººè„¸æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸å¨èƒæ€§ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , äººè„¸è¯†åˆ«, ç”Ÿæˆå¼æ¨¡å‹, ç¥ç»ç½‘ç»œ, ç‰¹å¾åµŒå…¥, äººè„¸ä¼ªé€ æ£€æµ‹, èº«ä»½è¡¨å¾, äººè¯åˆä¸€éªŒè¯, ç”µå­è¯ä»¶å®‰å…¨, deep learning

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16569v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16569v1.pdf)

---

## [16. Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding](https://arxiv.org/abs/2602.16545v1)

**ä½œè€…**ï¼šKaiting Liu, Hazel Doughty  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºâ€œç±»åˆ«æ‹†åˆ†â€ä»»åŠ¡ï¼Œå¹¶é€šè¿‡é›¶æ ·æœ¬ç¼–è¾‘ç°æœ‰è§†é¢‘åˆ†ç±»å™¨ï¼Œåœ¨æ— éœ€é¢å¤–æ ‡æ³¨çš„æƒ…å†µä¸‹æŠŠç²—ç²’åº¦ç±»åˆ«ç»†åŒ–ä¸ºå­ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–ç±»åˆ«æ€§èƒ½ä¸é™ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è§†é¢‘è¯†åˆ«æ¨¡å‹ä¾èµ–å›ºå®šä¸”ç²—ç³™çš„æ ‡ç­¾ä½“ç³»ï¼Œå°†ä¸åŒç‰©ä½“ã€åŠ¨ä½œæ–¹å¼æˆ–ç»“æœæ··ä¸ºä¸€è°ˆï¼Œéš¾ä»¥é€‚åº”ä¸æ–­æ¼”åŒ–çš„ä»»åŠ¡éœ€æ±‚ï¼Œè€Œé‡æ–°æ ‡æ³¨ä¸è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æå‡ºç±»åˆ«æ‹†åˆ†ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è§†é¢‘åˆ†ç±»å™¨å†…éƒ¨æ½œåœ¨çš„ç»„åˆç»“æ„è¿›è¡Œé›¶æ ·æœ¬æ¨¡å‹ç¼–è¾‘ï¼Œå°†ä¸€ä¸ªç²—ç±»åˆ«æ‹†åˆ†ä¸ºå¤šä¸ªç»†ç²’åº¦å­ç±»åˆ«ï¼›åŒæ—¶è®¾è®¡å°‘æ ·æœ¬å¾®è°ƒæ–¹æ¡ˆï¼Œä»¥é›¶æ ·æœ¬ç¼–è¾‘ç»“æœä½œä¸ºåˆå§‹åŒ–è¿›ä¸€æ­¥æå‡ç»†ç²’åº¦è¯†åˆ«æ•ˆæœã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ–°æ„å»ºçš„è§†é¢‘ç±»åˆ«æ‹†åˆ†åŸºå‡†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒè§†è§‰-è¯­è¨€åŸºçº¿æ˜¾è‘—æå‡æ–°æ‹†åˆ†å­ç±»çš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œä¸”åŸºæœ¬ä¸æŸå¤±å¯¹åŸæœ‰å…¶ä»–ç±»åˆ«çš„æ€§èƒ½ï¼Œå°‘æ ·æœ¬å¾®è°ƒåœ¨é›¶æ ·æœ¬åˆå§‹åŒ–åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¢å¼ºäº†æ•ˆæœã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , è§†é¢‘åˆ†ç±», é›¶æ ·æœ¬å­¦ä¹ , ç±»åˆ«ç»†ç²’åº¦åˆ’åˆ†, æ¨¡å‹ç¼–è¾‘, å°‘æ ·æœ¬å¾®è°ƒ, è§†è§‰è¯­è¨€æ¨¡å‹, åˆ†ç±»å™¨å¯å¡‘æ€§, rag

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16545v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16545v1.pdf)

---

## cs.LG

## [17. Knowledge-Embedded Latent Projection for Robust Representation Learning](https://arxiv.org/abs/2602.16709v1)

**ä½œè€…**ï¼šWeijing Tang, Ming Yuan, Zongqi Xia ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, math.ST, stat.ME  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºä¸€ç§å°†å¤–éƒ¨è¯­ä¹‰çŸ¥è¯†åµŒå…¥æ½œåœ¨ç©ºé—´çš„æŠ•å½±æ¨¡å‹ï¼Œåœ¨æ ·æœ¬æ•°è¿œå°äºç‰¹å¾æ•°çš„å¤±è¡¡åœºæ™¯ä¸‹å®ç°æ›´ç¨³å¥çš„è¡¨ç¤ºå­¦ä¹ ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨ç”µå­å¥åº·è®°å½•ç­‰åœºæ™¯ä¸­ï¼Œæ‚£è€…æ•°é‡æœ‰é™è€Œç‰¹å¾ï¼ˆåŒ»ç–—ç¼–ç ï¼‰æå¤šï¼Œä¼ ç»Ÿæ½œåœ¨ç©ºé—´æ¨¡å‹åœ¨è¿™ç§é«˜åº¦å¤±è¡¡è®¾å®šä¸‹ä¼°è®¡ä¸ç¨³å®šï¼Œè€Œåˆå­˜åœ¨å¤§é‡å¯åˆ©ç”¨çš„ä¸´åºŠæ¦‚å¿µé¢„è®­ç»ƒè¯­ä¹‰åµŒå…¥å°šæœªè¢«æœ‰æ•ˆæ•´åˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†åˆ—åµŒå…¥å»ºæ¨¡ä¸ºè¯­ä¹‰åµŒå…¥åœ¨å†ç”Ÿæ ¸Hilbertç©ºé—´ä¸­çš„å¹³æ»‘å‡½æ•°ï¼Œé€šè¿‡æ ¸PCAæ„é€ ç”±è¯­ä¹‰æŒ‡å¯¼çš„ä½ç»´å­ç©ºé—´ï¼Œå†ç»“åˆæŠ•å½±æ¢¯åº¦ä¸‹é™è¿›è¡Œä¸¤æ­¥ä¼°è®¡ï¼Œå¹¶ç»™å‡ºç»Ÿè®¡è¯¯å·®ä¸æ ¸æŠ•å½±è¿‘ä¼¼è¯¯å·®ä¹‹é—´æƒè¡¡çš„è¯¯å·®ç•Œä»¥åŠéå‡¸ä¼˜åŒ–çš„å±€éƒ¨æ”¶æ•›ä¿è¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šä»¿çœŸå’ŒçœŸå®EHRæ•°æ®å®éªŒè¡¨æ˜ï¼Œè¯¥çŸ¥è¯†åµŒå…¥çš„æ½œåœ¨æŠ•å½±æ–¹æ³•åœ¨å°æ ·æœ¬ã€å¤§ç‰¹å¾çš„å¤±è¡¡åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æå‡è¡¨ç¤ºå­¦ä¹ çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œç†è®ºåˆ†æåˆ™å±•ç¤ºäº†å…¶åœ¨è¯¯å·®æ§åˆ¶å’Œæ”¶æ•›æ€§è´¨ä¸Šçš„å¯é æ€§ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , è¡¨ç¤ºå­¦ä¹ , è¯­ä¹‰åµŒå…¥, æ ¸æ–¹æ³•, kPCA, æŠ•å½±æ¢¯åº¦ä¸‹é™, ç”µå­å¥åº·è®°å½•, EHRè¡¨ç¤ºå­¦ä¹ , é²æ£’è¡¨å¾, é«˜ç»´æ•°æ®å»ºæ¨¡, embedding

**è¯„åˆ†**ï¼š28

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16709v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16709v1.pdf)

---

## [18. Protecting the Undeleted in Machine Unlearning](https://arxiv.org/abs/2602.16697v1)

**ä½œè€…**ï¼šAloni Cohen, Refael Kohen, Kobbi Nissim ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.DS  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Machine unlearning aims to remove specific data points from a trained model, often striving to emulate "perfect retraining", i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $Ï‰(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æŒ‡å‡ºç°æœ‰ä»¥â€œå®Œç¾é‡è®­ç»ƒâ€ä¸ºç›®æ ‡çš„æœºå™¨åé—å¿˜å®šä¹‰ä¼šæ³„éœ²æœªè¢«åˆ é™¤æ ·æœ¬éšç§ï¼Œå¹¶æå‡ºä¸€ç§ä¸“é—¨ä¿æŠ¤æœªåˆ æ•°æ®çš„æ–°å®‰å…¨å®šä¹‰ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æœºå™¨åé—å¿˜å·¥ä½œå¤šä»¥æ¨¡æ‹Ÿâ€œä»æœªè§è¿‡è¢«åˆ æ ·æœ¬â€çš„å®Œç¾é‡è®­ç»ƒä¸ºç›®æ ‡ï¼Œå´å‡ ä¹æœªè€ƒè™‘æ­¤è¿‡ç¨‹å¯¹å‰©ä½™æœªåˆ æ ·æœ¬éšç§é€ æˆçš„æ½œåœ¨æ³„éœ²é£é™©ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æ„é€ äº†ä¸€ç±»é‡æ„æ”»å‡»ï¼šå¯¹æ»¡è¶³å®Œç¾é‡è®­ç»ƒçš„æœºåˆ¶ï¼Œåªéœ€æ§åˆ¶Ï‰(1)ä¸ªæ ·æœ¬å¹¶å¤šæ¬¡å‘èµ·åˆ é™¤è¯·æ±‚ï¼Œå°±èƒ½å‡ ä¹é‡å»ºæ•´ä¸ªè®­ç»ƒæ•°æ®ï¼›éšåç³»ç»Ÿæ€§åˆ†ææ—¢æœ‰åé—å¿˜å®‰å…¨å®šä¹‰çš„ä¸è¶³ï¼Œå¹¶åœ¨å½¢å¼åŒ–æ¨¡å‹ä¸‹ç»™å‡ºæ–°å®šä¹‰ï¼Œè¯æ˜å…¶æ—¢èƒ½æŠµæŠ—æ­¤ç±»æ”»å‡»åˆæ”¯æŒå…¬å‘Šæ¿ã€æ±‚å’Œä¸ç»Ÿè®¡å­¦ä¹ ç­‰åŠŸèƒ½ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®Œç¾é‡è®­ç»ƒä½œä¸ºæœºå™¨åé—å¿˜çš„å®‰å…¨ç›®æ ‡åœ¨éšç§ä¸Šå¹¶ä¸å®‰å…¨ï¼Œç°æœ‰å®šä¹‰è¦ä¹ˆæ˜“å—é‡æ„æ”»å‡»è¦ä¹ˆè¿‡äºä¸¥æ ¼éš¾ä»¥å®ç”¨ï¼›æ–°æå‡ºçš„â€œä¿æŠ¤æœªåˆ æ•°æ®â€çš„å®‰å…¨å®šä¹‰åœ¨å¯è¡Œæ€§ä¸éšç§é—´æä¾›äº†æ›´åˆç†æŠ˜ä¸­ï¼Œå¯å®‰å…¨æ”¯æŒè‹¥å¹²å…³é”®æ•°æ®åˆ†æä¸å­¦ä¹ ä»»åŠ¡ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , æœºå™¨åå­¦ä¹ , æ¨¡å‹åˆ é™¤, æ•°æ®éšç§ä¿æŠ¤, é‡æ„æ”»å‡», å®‰å…¨å®šä¹‰, ç»Ÿè®¡å­¦ä¹ , éšç§é£é™©åˆ†æ, agent

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16697v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16697v1.pdf)

---

## [19. Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition](https://arxiv.org/abs/2602.16684v1)

**ä½œè€…**ï¼šBo Pan, Peter Zhiping Zhang, Hao-Wei Pang ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºåŸºäºå¤§è§„æ¨¡åŒ¹é…åˆ†å­å¯¹å˜æ¢ï¼ˆMMPTï¼‰çš„æ£€ç´¢å¢å¼ºåŸºç¡€æ¨¡å‹ï¼Œç”¨å¯æ§ä¸æ£€ç´¢å¼•å¯¼çš„æ–¹å¼è‡ªåŠ¨ç”Ÿæˆç¬¦åˆè¯ç‰©åŒ–å­¦ç›´è§‰çš„åˆ†å­ç±»ä¼¼ç‰©ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åˆ†å­ç”Ÿæˆæ–¹æ³•è¦ä¹ˆåœ¨æ•´åˆ†å­å±‚é¢ç¼ºä¹ç²¾ç»†â€œå±€éƒ¨æ”¹é€ â€æ§åˆ¶ï¼Œè¦ä¹ˆä»…åœ¨å°æ•°æ®å’Œå—é™åœºæ™¯å­¦ä¹ æœ‰é™çš„MMPå¼ç¼–è¾‘ï¼Œéš¾ä»¥å¤ç°è¯ç‰©åŒ–å­¦å®¶åœ¨é¡¹ç›®ä¸­çš„å®é™…ä¿®é¥°ç­–ç•¥ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†ç±»ä¼¼ç‰©è®¾è®¡å½¢å¼åŒ–ä¸ºâ€œå˜é‡åˆ°å˜é‡â€çš„MMPTç”Ÿæˆä»»åŠ¡ï¼Œåœ¨å¤§è§„æ¨¡MMPä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¼•å…¥å¯æŒ‡å®šè½¬åŒ–æ¨¡å¼çš„æç¤ºæœºåˆ¶ä¸åŸºäºå¤–éƒ¨å‚è€ƒç±»ä¼¼ç‰©çš„æ£€ç´¢å¢å¼ºæ¡†æ¶MMPT-RAGï¼Œä»¥é¡¹ç›®ç‰¹å¼‚æ€§çš„ç»“æ„ä¸ºæ¡ä»¶å¼•å¯¼ç”Ÿæˆã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨é€šç”¨åŒ–å­¦è¯­æ–™å’Œä¸“åˆ©æ•°æ®ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·æ€§ã€æ–°é¢–æ€§å’Œå¯æ§æ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”èƒ½åœ¨çœŸå®è¯ç‰©å‘ç°åœºæ™¯ä¸­è¾ƒå¥½å¤ç°åˆç†çš„ç±»ä¼¼ç‰©ç»“æ„ï¼Œä½“ç°å‡ºå°†è¯ç‰©åŒ–å­¦ç›´è§‰è§„æ¨¡åŒ–å»ºæ¨¡çš„æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, ç”Ÿæˆå¼æ¨¡å‹, æ£€ç´¢å¢å¼ºç”ŸæˆRAG, åˆ†å­ç”Ÿæˆ, è¯ç‰©å‘ç°, åŒ¹é…åˆ†å­å¯¹, åŸºç¡€æ¨¡å‹, æ¡ä»¶ç”Ÿæˆ, åŒ–å­¦ç©ºé—´æ¢ç´¢

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16684v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16684v1.pdf)

---

## [20. Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment](https://arxiv.org/abs/2602.16643v1)

**ä½œè€…**ï¼šShuta Kikuchi, Shu Tanaka  
**åˆ†ç±»**ï¼šcs.LG, cond-mat.stat-mech  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡åŸºäºå› å­åˆ†è§£æœºä¸é€€ç«å¼äºŒæ¬¡ä¼˜åŒ–ï¼ˆFMQAï¼‰æ„å»ºRNAåæŠ˜å æ±‚è§£æ¡†æ¶ï¼Œå¹¶ç³»ç»Ÿæ¯”è¾ƒä¸åŒäºŒè¿›åˆ¶ç¼–ç ä¸ç¢±åŸºèµ‹å€¼ç­–ç•¥å¯¹ä¼˜åŒ–æ•ˆæœçš„å½±å“ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰RNAåæŠ˜å æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡åºåˆ—è¯„ä¼°ï¼Œæˆæœ¬é«˜ä¸”å¯¹ç¦»æ•£ä¼˜åŒ–ä¸ç¼–ç æ–¹å¼çš„ç³»ç»Ÿåˆ†æä¸è¶³ï¼Œå°¤å…¶æ˜¯ä¸åŒäºŒè¿›åˆ¶ç¼–ç ä¸ç¢±åŸºâ†’æ•´æ•°æ˜ å°„å¦‚ä½•å½±å“æœç´¢æ™¯è§‚ä¸è§£è´¨é‡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†RNAåºåˆ—è®¾è®¡å»ºæ¨¡ä¸ºFMQAçš„ç¦»æ•£é»‘ç›’ä¼˜åŒ–é—®é¢˜ï¼Œç©·ä¸¾å››ç§äºŒè¿›åˆ¶ç¼–ç ï¼ˆä¸€ä½ä¸€å…ƒã€åŸŸå£ã€æ™®é€šäºŒè¿›åˆ¶ã€unaryï¼‰ä¸24ç§ç¢±åŸºåˆ°0â€“3æ•´æ•°çš„èµ‹å€¼ç»„åˆï¼Œåœ¨å›ºå®šç»“æ„ç›®æ ‡ä¸‹æ¯”è¾ƒä¼˜åŒ–å¾—åˆ°çš„åºåˆ—ï¼ˆä»¥normalized ensemble defectè¯„ä»·ï¼‰ã€‚

**ä¸»è¦ç»“è®º**ï¼šä¸€ä½ä¸€å…ƒç¼–ç å’ŒåŸŸå£ç¼–ç æ˜¾è‘—ä¼˜äºæ™®é€šäºŒè¿›åˆ¶ä¸unaryç¼–ç ï¼›åœ¨åŸŸå£ç¼–ç ä¸­ï¼Œè¢«èµ‹å€¼ä¸ºè¾¹ç•Œæ•´æ•°0å’Œ3çš„ç¢±åŸºå‡ºç°é¢‘ç‡æ›´é«˜ï¼Œå°†Gã€Cæ˜ å°„åˆ°0å’Œ3å¯å¢å¼ºèŒåŒºGCå¯Œé›†ã€æå‡ç»“æ„çƒ­ç¨³å®šæ€§ï¼Œä¼˜äºä¸€ä½ä¸€å…ƒç¼–ç ä¸‹çš„ç»“æœã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , é»‘ç›’ä¼˜åŒ–, å› å­åˆ†è§£æœº, ç¦»æ•£ä¼˜åŒ–, åºåˆ—è®¾è®¡, RNAåæŠ˜å , äºŒè¿›åˆ¶ç¼–ç , machine learning

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16643v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16643v1.pdf)

---

## [21. Optimizer choice matters for the emergence of Neural Collapse](https://arxiv.org/abs/2602.16642v1)

**ä½œè€…**ï¼šJim Zhao, Tin Sum Cheng, Wojciech Masarczyk ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡è¡¨æ˜â€œNeural Collapseâ€å¹¶éä¸ä¼˜åŒ–å™¨æ— å…³ï¼Œè€Œæ˜¯å¼ºçƒˆä¾èµ–äºä¼˜åŒ–å™¨å½¢å¼ï¼Œå°¤å…¶æ˜¯æƒé‡è¡°å‡æ˜¯è€¦åˆè¿˜æ˜¯è§£è€¦ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¯¹Neural Collapseçš„ç†è®ºå¤šæ•°å¿½ç•¥ä¼˜åŒ–å™¨å·®å¼‚ï¼Œé»˜è®¤å…¶ä¸ºæ™®é€‚ç°è±¡ï¼Œä½†åœ¨å®è·µä¸­ä¸åŒä¼˜åŒ–å™¨ï¼ˆå¦‚SGDä¸AdamWï¼‰è¡¨ç°å‡ºä¸åŒçš„å‡ ä½•ç»“æ„ï¼ŒäºŸéœ€ç»Ÿä¸€çš„ç†è®ºè§£é‡Šã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æå‡ºä¸€ä¸ªæ–°çš„è¯Šæ–­æŒ‡æ ‡NC0ï¼Œç”¨å…¶åˆ»ç”»Neural Collapseå‡ºç°çš„å¿…è¦æ¡ä»¶ï¼Œå¹¶åœ¨ç†è®ºä¸Šå¯¹æ¯”åˆ†æSGDã€å¸¦è€¦åˆæƒé‡è¡°å‡çš„SignGDï¼ˆç±»Adamï¼‰å’Œå¸¦è§£è€¦æƒé‡è¡°å‡çš„SignGDï¼ˆç±»AdamWï¼‰çš„NC0åŠ¨æ€è¡Œä¸ºï¼Œå¹¶è¾…ä»¥æ¶µç›–å¤šæ•°æ®é›†/ç»“æ„/ä¼˜åŒ–å™¨/è¶…å‚çš„3900æ¬¡å¤§è§„æ¨¡å®éªŒéªŒè¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»“æœè¯æ˜å¸¦è§£è€¦æƒé‡è¡°å‡çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼ˆå¦‚AdamWï¼‰åœ¨ç†è®ºä¸Šæ— æ³•è‡ªç„¶äº§ç”ŸNeural Collapseï¼Œè€ŒSGDåŠå¸¦åŠ¨é‡çš„å˜ä½“ä¸ä»…æ›´æ˜“äº§ç”ŸNeural Collapseï¼ŒåŠ¨é‡è¿˜ä¼šåŠ é€Ÿå…¶å½¢æˆï¼›æƒé‡è¡°å‡çš„è€¦åˆæ–¹å¼æ˜¯ä¼˜åŒ–å™¨éšå¼åç½®å’ŒNCæ¶Œç°çš„å…³é”®å› ç´ ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, ä¼˜åŒ–å™¨é€‰æ‹©, æƒé‡è¡°å‡è€¦åˆ, è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•, åŠ¨é‡åŠ é€Ÿè®­ç»ƒ, è¡¨ç¤ºå­¦ä¹ å‡ ä½•ç»“æ„, è®­ç»ƒéšå¼åç½®, neural network

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16642v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16642v1.pdf)

---

## [22. Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes](https://arxiv.org/abs/2602.16629v1)

**ä½œè€…**ï¼šEthan Blaser, Jiuqi Wang, Shangtong Zhang  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡è¯æ˜äº†å¹³å‡å›æŠ¥å¼ºåŒ–å­¦ä¹ ä¸­å·®åˆ†TDç®—æ³•åœ¨æ›´æ¥è¿‘å®é™…ä½¿ç”¨çš„å­¦ä¹ ç‡è®¾å®šä¸‹å‡ ä¹å¿…ç„¶æ”¶æ•›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä»¥å¹³å‡å›æŠ¥ä¸ºç›®æ ‡çš„RLä¸­ï¼Œå·®åˆ†TDæ˜¯é«˜æ•ˆçš„åœ¨çº¿æ–¹æ³•ï¼Œä½†ç°æœ‰æ”¶æ•›æ€§åˆ†æä¾èµ–ä¸çŠ¶æ€è®¿é—®æ¬¡æ•°ç»‘å®šçš„â€œæœ¬åœ°æ—¶é’Ÿâ€å­¦ä¹ ç‡ï¼Œæ—¢ä¸å®ç”¨ä¹Ÿéš¾ä»¥æ¨å¹¿åˆ°éè¡¨æ ¼è®¾ç½®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åœ¨ç†è®ºä¸Šåˆ†æä»»æ„næ­¥å·®åˆ†TDï¼Œé¦–å…ˆå¯¹åœ¨ç­–ç•¥æƒ…å½¢ä¸‹ä½¿ç”¨å…¨å±€é€’å‡å­¦ä¹ ç‡ï¼ˆæ— æœ¬åœ°æ—¶é’Ÿï¼‰å»ºç«‹å‡ ä¹å¿…ç„¶æ”¶æ•›æ€§è¯æ˜ï¼Œéšåæå‡ºä¸‰ç±»è¶³å¤Ÿæ¡ä»¶ï¼Œä½¿ç¦»ç­–ç•¥næ­¥å·®åˆ†TDåœ¨åŒæ ·çš„æ— æœ¬åœ°æ—¶é’Ÿè®¾å®šä¸‹ä¹Ÿå¯è¯æ˜æ”¶æ•›ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ–‡ç« è¡¨æ˜åœ¨æ ‡å‡†çš„é€’å‡å…¨å±€å­¦ä¹ ç‡ä¸‹ï¼Œnæ­¥åœ¨ç­–ç•¥å·®åˆ†TDå¿…ç„¶æ”¶æ•›ï¼Œä¸”åœ¨æ»¡è¶³ç‰¹å®šæ¡ä»¶æ—¶ç¦»ç­–ç•¥æƒ…å½¢ä¹Ÿèƒ½æ”¶æ•›ï¼Œä»è€Œæ˜¾è‘—å¼ºåŒ–äº†å·®åˆ†TDçš„ç†è®ºåŸºç¡€ï¼Œå¹¶ä½¿æ”¶æ•›åˆ†ææ›´è´´è¿‘å®é™…å¼ºåŒ–å­¦ä¹ å®ç°ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , å¹³å‡å›æŠ¥, æ—¶åºå·®åˆ†å­¦ä¹ , ä»·å€¼å‡½æ•°ä¼°è®¡, åœ¨çº¿å­¦ä¹ , æ”¶æ•›æ€§åˆ†æ, é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹, næ­¥TDç®—æ³•, agent

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16629v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16629v1.pdf)

---

## [23. Sequential Membership Inference Attacks](https://arxiv.org/abs/2602.16596v1)

**ä½œè€…**ï¼šThomas Michel, Debabrota Basu, Emilie Kaufmann  
**åˆ†ç±»**ï¼šcs.LG, cs.CR, math.ST, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè¿™ç¯‡è®ºæ–‡æå‡ºå¹¶åˆ†æäº†ä¸€ç§åˆ©ç”¨æ¨¡å‹å¤šæ¬¡æ›´æ–°åºåˆ—çš„æœ€ä¼˜é¡ºåºæˆå‘˜æ¨æ–­æ”»å‡» SeMI*ï¼Œåœ¨ç†è®ºå’Œå®éªŒä¸Šéƒ½æ¯”ä»…é’ˆå¯¹æœ€ç»ˆæ¨¡å‹çš„ä¼ ç»Ÿæ”»å‡»æ›´å¼ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æˆå‘˜æ¨æ–­ç ”ç©¶å¤šé’ˆå¯¹é™æ€æ¨¡å‹ä¸”ä¾èµ–æ— é™æ ·æœ¬ï¼Œç°å®ä¸­æ¨¡å‹ä¼šå¤šè½®æ›´æ–°ï¼Œä¸”åªçœ‹æœ€ç»ˆæ¨¡å‹æ—¶éšç§ä¿¡å·ä¼šéšè®­ç»ƒæ•°æ®å¢å¤šè€Œè¢«â€œç¨€é‡Šâ€ï¼Œéœ€è¦ç³»ç»Ÿç ”ç©¶å¦‚ä½•åˆ©ç”¨æ•´ä¸ªæ›´æ–°åºåˆ—è¿›è¡Œæ›´æœ‰åŠ›çš„éšç§æ”»å‡»ä¸å®¡è®¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å½¢å¼åŒ–â€œç›®æ ‡æ ·æœ¬åœ¨æŸä¸€è½®è¢«æ’å…¥â€çš„é¡ºåºæˆå‘˜æ¨æ–­é—®é¢˜ï¼Œåœ¨ç»éªŒå‡å€¼è®¡ç®—åœºæ™¯ä¸‹æ¨å¯¼å‡ºåˆ©ç”¨æ¨¡å‹æ›´æ–°åºåˆ—çš„æœ€ä¼˜æ”»å‡»ç­–ç•¥ SeMI* çš„æ£€éªŒç»Ÿè®¡é‡ä¸ç»Ÿè®¡åŠŸæ•ˆï¼ŒåŒ…æ‹¬æœ‰é™æ ·æœ¬ã€å¸¦/ä¸å¸¦å·®åˆ†éšç§å™ªå£°çš„æƒ…å†µï¼Œå¹¶æ„é€ å®é™…å¯å®ç°çš„å˜ä½“ç”¨äºå®éªŒã€‚

**ä¸»è¦ç»“è®º**ï¼šç†è®ºä¸Šï¼ŒSeMI* èƒ½åœ¨æœ‰é™æ ·æœ¬ä¸‹è¾¾åˆ°æœ€ä¼˜åŠŸæ•ˆå¹¶æ”¶æ•›åˆ°æ—¢æœ‰çš„æ¸è¿‘ç»“æœï¼ŒåŒæ—¶åˆ©ç”¨æ›´æ–°åºåˆ—å¯é¿å…ä¼ ç»Ÿâ€œåªçœ‹æœ€ç»ˆæ¨¡å‹â€æ—¶ä¿¡å·éšæ•°æ®é‡å¢å¤§è€Œæ¶ˆå¤±ï¼›å®è·µä¸Šï¼Œåœ¨å¤šç§æ•°æ®åˆ†å¸ƒå’Œä½¿ç”¨ DP-SGD è®­ç»ƒæˆ–å¾®è°ƒçš„æ¨¡å‹ä¸Šï¼ŒSeMI* çš„å®ç°ç‰ˆæœ¬ç»™å‡ºçš„éšç§å®¡è®¡æ˜¾è‘—æ¯”ç°æœ‰åŸºçº¿æ›´ç´§ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, æ¨¡å‹æ›´æ–°åºåˆ—, æˆå‘˜æ¨æ–­æ”»å‡», å·®åˆ†éšç§è®­ç»ƒ, DP-SGDéšç§å®¡è®¡, ç›®æ ‡æ’å…¥æ£€æµ‹, æœ€ä¼˜æ”»å‡»ç­–ç•¥, agent

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16596v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16596v1.pdf)

---

## [24. AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS](https://arxiv.org/abs/2602.16579v1)

**ä½œè€…**ï¼šMaria Luisa Taccari, Kenza Tazi, OisÃ­n M. Morrison ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, physics.app-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šAIFL æ˜¯ä¸€ä¸ªåŸºäºç¡®å®šæ€§ LSTM çš„å…¨çƒæ—¥å°ºåº¦å¾„æµé¢„æŠ¥æ¨¡å‹ï¼Œé€šè¿‡åœ¨ ERA5-Land é¢„è®­ç»ƒå¹¶åœ¨ IFS é¢„æŠ¥ä¸Šå¾®è°ƒï¼Œæœ‰æ•ˆæå‡ä»å†åˆ†æåˆ°ä¸šåŠ¡é¢„æŠ¥åœºæ™¯çš„é¢„æµ‹æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ•°æ®é©±åŠ¨å¾„æµæ¨¡å‹åœ¨ä»å†åˆ†ææ•°æ®è½¬åˆ°ä¸šåŠ¡æ•°å€¼é¢„æŠ¥äº§å“æ—¶å­˜åœ¨æ˜æ˜¾æ€§èƒ½è¡°å‡ï¼Œå½±å“å…¨çƒæ´ªæ°´é¢„è­¦å’Œæ°´èµ„æºç®¡ç†çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨ CARAVAN æ•°æ®é›†ä¸­é€‰å– 18,588 ä¸ªæµåŸŸï¼Œå…ˆç”¨ 40 å¹´ ERA5-Land å†åˆ†ææ•°æ®å¯¹ LSTM è¿›è¡Œé¢„è®­ç»ƒä»¥å­¦ä¹ ç¨³å¥æ°´æ–‡è¿‡ç¨‹ï¼Œå†ç”¨ 2016â€“2019 å¹´ IFS æ§åˆ¶é¢„æŠ¥è¿›è¡Œå¾®è°ƒï¼Œä»è€Œé€‚é…ä¸šåŠ¡æ¨¡å¼çš„è¯¯å·®ç»“æ„å’Œåå·®ï¼Œå¹¶åœ¨ 2021â€“2024 å¹´ç‹¬ç«‹æ—¶æ®µä¸Šè¯„ä¼°æ€§èƒ½ã€‚

**ä¸»è¦ç»“è®º**ï¼šAIFL åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šå–å¾—ä¸­ä½ KGE' ä¸º 0.66ã€NSE ä¸º 0.53 çš„é«˜é¢„æŠ¥æŠ€å·§ï¼Œæ•´ä½“æ•ˆæœä¸å½“å‰æœ€å…ˆè¿›å…¨çƒç³»ç»Ÿç›¸å½“ï¼Œä¸”åœ¨æç«¯äº‹ä»¶è¯†åˆ«ä¸Šè¡¨ç°å¯é ï¼Œä¸ºå…¨çƒæ°´æ–‡ç•Œæä¾›äº†é€æ˜ã€å¯å¤ç°çš„åŸºçº¿æ–¹æ¡ˆã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, LSTM, æ—¶é—´åºåˆ—é¢„æµ‹, æ°´æ–‡é¢„æŠ¥, æ´ªæ°´é¢„è­¦, å…¨çƒæµé‡é¢„æµ‹, å†åˆ†ææ•°æ®é¢„è®­ç»ƒ, é¢†åŸŸè‡ªé€‚åº”, artificial intelligence

**è¯„åˆ†**ï¼š26

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16579v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16579v1.pdf)

---

## [25. MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models](https://arxiv.org/abs/2602.16573v1)

**ä½œè€…**ï¼šAntonios Tziorvas, George S. Theodoropoulos, Yannis Theodoridis  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Urban demand forecasting plays a critical role in optimizing routing, dispatching, and congestion management within Intelligent Transportation Systems. By leveraging data fusion and analytics techniques, traffic demand forecasting serves as a key intermediate measure for identifying emerging spatial and temporal demand patterns. In this paper, we tackle this challenge by proposing two gradient boosting model variations, one for classiffication and one for regression, both capable of generating demand forecasts at various temporal horizons, from 5 minutes up to one hour. Our overall approach effectively integrates temporal and contextual features, enabling accurate predictions that are essential for improving the efficiency of shared (micro-) mobility services. To evaluate its effectiveness, we utilize open shared mobility data derived from e-scooter and e-bike networks in five metropolitan areas. These real-world datasets allow us to compare our approach with state-of-the-art methods as well as a Generative AI-based model, demonstrating its effectiveness in capturing the complexities of modern urban mobility. Ultimately, our methodology offers novel insights on urban micro-mobility management, helping to tackle the challenges arising from rapid urbanization and thus, contributing to more sustainable, efficient, and livable cities.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºä¸¤ç§æ¢¯åº¦æå‡æ¨¡å‹ï¼ˆåˆ†ç±»ä¸å›å½’ï¼‰ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆé¢„æµ‹å…±äº«å¾®å‡ºè¡Œåœ¨5åˆ†é’Ÿåˆ°1å°æ—¶å¤šæ—¶é—´å°ºåº¦ä¸Šçš„éœ€æ±‚ï¼Œå¹¶åœ¨å¤šåŸå¸‚çœŸå®æ•°æ®ä¸Šä¼˜äºå¤šç§å¯¹æ¯”æ–¹æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€åŸå¸‚å…±äº«ç”µåŠ¨è½¦ã€å…±äº«å•è½¦ç­‰å¾®å‡ºè¡Œè¿…é€Ÿå‘å±•ï¼Œç²¾å‡†é¢„æµ‹ä¸åŒæ—¶é—´ä¸åŒºåŸŸçš„å‡ºè¡Œéœ€æ±‚å¯¹äºè°ƒåº¦è½¦è¾†ã€ç¼“è§£æ‹¥å µå’Œæå‡ç³»ç»Ÿæ•ˆç‡è‡³å…³é‡è¦ï¼Œå°¤å…¶éœ€è¦æ—¢å‡†ç¡®åˆèƒ½åœ¨è¾¹ç¼˜ä¾§éƒ¨ç½²çš„è½»é‡çº§æ¨¡å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…è®¾è®¡äº†åä¸º MoDE-Boost çš„ä¸¤ç§æ¢¯åº¦æå‡å˜ä½“ï¼ˆä¸€ä¸ªç”¨äºåˆ†ç±»ï¼Œä¸€ä¸ªç”¨äºå›å½’ï¼‰ï¼Œèåˆå†å²æ—¶åºä¿¡æ¯å’Œå¤šç§ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆå¦‚æ—¶é—´ã€ç©ºé—´ä¸ç¯å¢ƒå› ç´ ï¼‰ï¼Œåœ¨5åˆ†é’Ÿè‡³1å°æ—¶å¤šæ—¶é—´ç²’åº¦ä¸Šè¿›è¡Œéœ€æ±‚é¢„æµ‹ï¼Œå¹¶åˆ©ç”¨äº”ä¸ªå¤§åŸå¸‚çš„å…±äº«ç”µåŠ¨æ»‘æ¿è½¦/ç”µåŠ¨è‡ªè¡Œè½¦å…¬å¼€æ•°æ®ï¼Œä¸ç°æœ‰æ–¹æ³•å’Œä¸€ä¸ªç”Ÿæˆå¼AIæ¨¡å‹è¿›è¡Œç³»ç»Ÿå¯¹æ¯”ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒMoDE-Boost èƒ½æ›´å¥½æ•æ‰åŸå¸‚å¾®å‡ºè¡Œéœ€æ±‚çš„æ—¶ç©ºå¤æ‚æ€§ï¼Œåœ¨å¤šåŸå¸‚æ•°æ®ä¸Šå–å¾—ä¼˜äºç°æœ‰æ–¹æ³•å’Œç”Ÿæˆå¼AIåŸºçº¿çš„é¢„æµ‹è¡¨ç°ï¼Œé€‚åˆéƒ¨ç½²åœ¨è¾¹ç¼˜ç¯å¢ƒä¸­ï¼Œä¸ºå…±äº«å¾®å‡ºè¡Œè¿ç»´ä¸åŸå¸‚äº¤é€šç®¡ç†æä¾›æ›´é«˜æ•ˆå’Œå¯æŒç»­çš„å†³ç­–æ”¯æŒã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç”Ÿæˆå¼æ¨¡å‹, æ¢¯åº¦æå‡æ¨¡å‹, å…±äº«å‡ºè¡Œéœ€æ±‚é¢„æµ‹, æ—¶ç©ºç‰¹å¾èåˆ, è¾¹ç¼˜è®¡ç®—éƒ¨ç½², æ™ºèƒ½äº¤é€šç³»ç»Ÿ, å¾®å‡ºè¡Œè°ƒåº¦, generative

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16573v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16573v1.pdf)

---

## [26. Steering diffusion models with quadratic rewards: a fine-grained analysis](https://arxiv.org/abs/2602.16570v1)

**ä½œè€…**ï¼šAnkur Moitra, Andrej Risteski, Dhruv Rohatgi  
**åˆ†ç±»**ï¼šcs.LG, cs.DS  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.   In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\star}(x) \propto p(x) \exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\top A x + b^\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ä»ç†è®ºä¸Šåˆ†æäº†åœ¨æ‰©æ•£æ¨¡å‹ä¸Šæ–½åŠ äºŒæ¬¡å¥–åŠ±ï¼ˆreward tiltï¼‰è¿›è¡Œé‡‡æ ·çš„å¯è®¡ç®—æ€§è¾¹ç•Œï¼Œå¹¶ç»™å‡ºå¯¹ç‰¹å®šå½¢å¼å¥–åŠ±çš„é«˜æ•ˆç®—æ³•ä¸ä¸å¯èƒ½ç»“æœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¨ç†æ—¶ç®—æ³•å¤šæ˜¯å¯å‘å¼æ–¹æ³•ï¼Œç¼ºä¹å…³äºâ€œåœ¨ç»™å®šå¥–åŠ±å‡½æ•°ä¸‹èƒ½å¦é«˜æ•ˆé‡é‡‡æ ·åˆ†å¸ƒâ€çš„ç³»ç»Ÿç†è§£ï¼Œå°¤å…¶æ˜¯å½“å¥–åŠ±æ˜¯å¸¸ç”¨çš„äºŒæ¬¡å½¢å¼æ—¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå½¢å¼åŒ–é—®é¢˜ä¸ºä» p*(x) âˆ p(x)exp(r(x)) é‡‡æ ·ï¼›å…ˆè¯æ˜çº¿æ€§å¥–åŠ± tilt æ€»æ˜¯å¯é«˜æ•ˆé‡‡æ ·ï¼Œç„¶åå¼•å…¥ Hubbard-Stratonovich å˜æ¢ï¼Œå°†ä½ç§©æ­£å®šäºŒæ¬¡å¥–åŠ±åˆ†è§£ä¸ºè‹¥å¹²çº¿æ€§ tilt çš„ç»„åˆï¼Œä»è€Œæ„é€ é«˜æ•ˆé‡‡æ ·ç®—æ³•ï¼Œå¹¶å¯¹è´Ÿå®šäºŒæ¬¡ tilt è¿›è¡Œå¤æ‚åº¦ä¸‹ç•Œåˆ†æã€‚

**ä¸»è¦ç»“è®º**ï¼š1ï¼‰çº¿æ€§å¥–åŠ± tilt åœ¨æ‰©æ•£æ¨¡å‹ä¸‹å§‹ç»ˆå¯é«˜æ•ˆé‡‡æ ·ï¼›2ï¼‰å¯¹ç§©ä¸º O(1) çš„æ­£å®šä½ç§©äºŒæ¬¡å¥–åŠ±ï¼Œå¯ç”¨ Hubbard-Stratonovich å˜æ¢å¾—åˆ°é«˜æ•ˆé‡‡æ ·ç®—æ³•ï¼›3ï¼‰å¯¹äºè´Ÿå®šäºŒæ¬¡ tiltï¼Œå³ä¾¿åªæ˜¯ç§© 1 ä¸”ç³»æ•°æŒ‡æ•°çº§ï¼Œä¹Ÿå¯ä»¥è¯æ˜é‡‡æ ·é—®é¢˜åœ¨è®¡ç®—ä¸Šæ˜¯ä¸å¯è¡Œï¼Œä»è€Œåˆ»ç”»äº†æ‰©æ•£æ¨¡å‹åœ¨äºŒæ¬¡å¥–åŠ± steering ä¸‹çš„å¯è¡Œä¸ä¸å¯è¡Œè¾¹ç•Œã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, diffusion, ç”Ÿæˆå¼, å¥–åŠ±æ¨¡å‹, æ¨ç†æ—¶ç®—æ³•, é‡‡æ ·ç®—æ³•, ä½ç§©çŸ©é˜µ, æ­£å®šäºŒæ¬¡å‹, å›¾åƒç”Ÿæˆ

**è¯„åˆ†**ï¼š32

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16570v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16570v1.pdf)

---

## [27. Illustration of Barren Plateaus in Quantum Computing](https://arxiv.org/abs/2602.16558v1)

**ä½œè€…**ï¼šGerhard Stenzel, Tobias Rohe, Michael KÃ¶lle ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, quant-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Variational Quantum Circuits (VQCs) have emerged as a promising paradigm for quantum machine learning in the NISQ era. While parameter sharing in VQCs can reduce the parameter space dimensionality and potentially mitigate the barren plateau phenomenon, it introduces a complex trade-off that has been largely overlooked. This paper investigates how parameter sharing, despite creating better global optima with fewer parameters, fundamentally alters the optimization landscape through deceptive gradients -- regions where gradient information exists but systematically misleads optimizers away from global optima. Through systematic experimental analysis, we demonstrate that increasing degrees of parameter sharing generate more complex solution landscapes with heightened gradient magnitudes and measurably higher deceptiveness ratios. Our findings reveal that traditional gradient-based optimizers (Adam, SGD) show progressively degraded convergence as parameter sharing increases, with performance heavily dependent on hyperparameter selection. We introduce a novel gradient deceptiveness detection algorithm and a quantitative framework for measuring optimization difficulty in quantum circuits, establishing that while parameter sharing can improve circuit expressivity by orders of magnitude, this comes at the cost of significantly increased landscape deceptiveness. These insights provide important considerations for quantum circuit design in practical applications, highlighting the fundamental mismatch between classical optimization strategies and quantum parameter landscapes shaped by parameter sharing.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ç³»ç»Ÿç ”ç©¶äº†å˜åˆ†é‡å­ç”µè·¯ä¸­å‚æ•°å…±äº«å¯¹ä¼˜åŒ–æ™¯è§‚çš„å½±å“ï¼Œå‘ç°å…¶è™½èƒ½æå‡è¡¨è¾¾èƒ½åŠ›ã€ç¼“è§£å¹³å¦é«˜åŸï¼Œä½†ä¼šæ˜¾è‘—å¢åŠ â€œæ¬ºéª—æ€§æ¢¯åº¦â€å¯¼è‡´ä¼˜åŒ–å›°éš¾ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè™½ç„¶å‚æ•°å…±äº«è¢«è®¤ä¸ºå¯å‡å°‘å‚æ•°ç»´åº¦å¹¶ç¼“è§£ barren plateau é—®é¢˜ï¼Œä½†å…¶å¦‚ä½•æ”¹å˜é‡å­ç”µè·¯çš„æŸå¤±ä¸æ¢¯åº¦æ™¯è§‚ã€ä»¥åŠå¯¹ç»å…¸ä¼˜åŒ–å™¨çš„çœŸå®å½±å“ä»ç¼ºä¹ç³»ç»Ÿç†è§£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æ„é€ ä¸åŒç¨‹åº¦å‚æ•°å…±äº«çš„å˜åˆ†é‡å­ç”µè·¯ï¼Œé€šè¿‡å¤§è§„æ¨¡æ•°å€¼å®éªŒåˆ†ææ¢¯åº¦åˆ†å¸ƒã€è§£ç©ºé—´ç»“æ„ä¸æ”¶æ•›è¡Œä¸ºï¼Œå¹¶æå‡ºæ¢¯åº¦æ¬ºéª—æ€§æ£€æµ‹ç®—æ³•å’Œé‡åŒ–ä¼˜åŒ–éš¾åº¦çš„æŒ‡æ ‡ä½“ç³»ã€‚

**ä¸»è¦ç»“è®º**ï¼šéšç€å‚æ•°å…±äº«ç¨‹åº¦æé«˜ï¼Œç”µè·¯è§£ç©ºé—´æ›´å¤æ‚ã€æ¢¯åº¦å¹…å€¼å¢å¤§ä½†æ¬ºéª—æ€§æ¯”ä¾‹ä¸Šå‡ï¼Œä¼ ç»Ÿæ¢¯åº¦ä¼˜åŒ–å™¨ï¼ˆAdamã€SGDï¼‰æ”¶æ•›æ€§èƒ½æ˜¾è‘—æ¶åŒ–å¹¶é«˜åº¦ä¾èµ–è¶…å‚æ•°ï¼›å‚æ•°å…±äº«åœ¨æå‡ç”µè·¯è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶å¼•å…¥ä¸¥é‡çš„æ™¯è§‚æ¬ºéª—æ€§ï¼Œå‡¸æ˜¾ç»å…¸ä¼˜åŒ–ç­–ç•¥ä¸æ­¤ç±»é‡å­å‚æ•°æ™¯è§‚ä¹‹é—´çš„æ ¹æœ¬é”™é…ï¼Œéœ€è¦åœ¨ç”µè·¯è®¾è®¡ä¸ä¼˜åŒ–æ–¹æ³•ä¸Šç»Ÿç­¹æƒè¡¡ã€‚

**å…³é”®è¯**ï¼šé‡å­æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, variational quantum circuits, å‚æ•°å…±äº«ä¼˜åŒ–, æ¢¯åº¦æ¬ºéª—æ£€æµ‹, ä¼˜åŒ–æ™¯è§‚å¤æ‚åº¦, é‡å­ç”µè·¯è®¾è®¡, æ¢¯åº¦ä¸‹é™æ”¶æ•›æ€§, machine learning

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16558v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16558v1.pdf)

---

## [28. RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion](https://arxiv.org/abs/2602.16548v1)

**ä½œè€…**ï¼šTianmeng Hu, Yongzheng Cui, Biao Luo ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šRIDER æå‡ºä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ çš„3D RNAåå‘è®¾è®¡æ¡†æ¶ï¼Œç›´æ¥ä¼˜åŒ–ç”Ÿæˆåºåˆ—çš„ä¸‰ç»´ç»“æ„ç›¸ä¼¼åº¦è€Œéä»…ä»…è¿½æ±‚åŸç”Ÿåºåˆ—è¿˜åŸç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰RNAåå‘è®¾è®¡æ–¹æ³•å¤šä»¥â€œåŸç”Ÿåºåˆ—æ¢å¤ç‡â€ä½œä¸ºä¼˜åŒ–å’Œè¯„ä¼°ç›®æ ‡ï¼Œä½†åŒä¸€3Dç»“æ„å¯å¯¹åº”å¤šç§ä¸åŒåºåˆ—ï¼Œé«˜æ¢å¤ç‡å¹¶ä¸ç­‰ä»·äºæ­£ç¡®æŠ˜å ï¼Œå› æ­¤éœ€è¦ç›´æ¥é’ˆå¯¹ä¸‰ç»´ç»“æ„ä¿çœŸåº¦è¿›è¡Œä¼˜åŒ–ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…é¦–å…ˆæ„å»ºå¹¶é¢„è®­ç»ƒä¸€ä¸ªä»¥ç›®æ ‡3Dç»“æ„ä¸ºæ¡ä»¶çš„GNNæ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºç”ŸæˆRNAåºåˆ—ï¼›éšåå¼•å…¥æ”¹è¿›çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œä»¥å››ä¸ªåŸºäº3Dè‡ªæ´½æ€§çš„ä»»åŠ¡å¥–åŠ±å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œä»è€Œä½¿ç”Ÿæˆåºåˆ—åœ¨ä¸‰ç»´ç»“æ„ä¸Šæ›´åŠ æ¥è¿‘ç›®æ ‡æ„å‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒRIDERåœ¨åŸç”Ÿåºåˆ—æ¢å¤ç‡ä¸Šç›¸æ¯”ç°æœ‰æ–¹æ³•æå‡çº¦9%ï¼Œå¹¶åœ¨å¤šç§ä¸‰ç»´ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡ä¸Šå®ç°è¶…è¿‡100%çš„æå‡ï¼ŒåŒæ—¶èƒ½å¤Ÿäº§ç”Ÿä¸åŸç”Ÿåºåˆ—æ˜æ˜¾ä¸åŒä½†èƒ½æŠ˜å åˆ°ç›¸ä¼¼3Dç»“æ„çš„RNAè®¾è®¡ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , æ‰©æ•£æ¨¡å‹, ç”Ÿæˆå¼æ¨¡å‹, å¼ºåŒ–å­¦ä¹ , å¥–åŠ±æ¨¡å‹, é€†å‘åˆ†å­è®¾è®¡, RNAä¸‰ç»´ç»“æ„è®¾è®¡, GNNå»ºæ¨¡, deep learning

**è¯„åˆ†**ï¼š41

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16548v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16548v1.pdf)

---

## [29. Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning](https://arxiv.org/abs/2602.16543v1)

**ä½œè€…**ï¼šJialiang Fan, Shixiong Jiang, Mengyu Liu ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§é»‘ç›’å¯¹æŠ—æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡é€†çº¦æŸå¼ºåŒ–å­¦ä¹ æš´éœ²å®‰å…¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨ç°å®å¯¹æŠ—æ‰°åŠ¨ä¸‹çš„è„†å¼±æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾ç¯å¢ƒè‰¯æ€§ï¼Œä¸”å¤šæ•°å¯¹æŠ—æ”»å‡»éœ€è¦è®¿é—®ç­–ç•¥æ¢¯åº¦ï¼Œåœ¨çœŸå®åœºæ™¯ä¸­ä¸ç°å®ï¼Œå› æ­¤éœ€è¦åœ¨æœ‰é™è®¿é—®ã€æœªçŸ¥çº¦æŸçš„æ¡ä»¶ä¸‹ç³»ç»Ÿæ€§è¯„ä¼°å¹¶æ”»å‡»Safe RLç­–ç•¥ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåˆ©ç”¨ä¸“å®¶ç¤ºä¾‹å’Œä¸ç¯å¢ƒçš„é»‘ç›’äº¤äº’ï¼Œé¦–å…ˆé€šè¿‡é€†çº¦æŸå¼ºåŒ–å­¦ä¹ å­¦ä¹ ç¯å¢ƒçš„çº¦æŸæ¨¡å‹å’Œä»£ç†ç­–ç•¥ï¼Œå†åœ¨è¯¥ä»£ç†ä¸Šè¿›è¡Œæ¢¯åº¦é©±åŠ¨çš„å¯¹æŠ—æ‰°åŠ¨ä¼˜åŒ–ï¼Œå¹¶ç»™å‡ºå¯è¡Œæ€§ä¸æ‰°åŠ¨å¤§å°çš„ç†è®ºç•Œã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§å®‰å…¨å¼ºåŒ–å­¦ä¹ åŸºå‡†ä¸Šï¼Œè¯¥é»‘ç›’æ”»å‡»åœ¨ä»…æœ‰æœ‰é™ç‰¹æƒä¿¡æ¯çš„æƒ…å†µä¸‹ä»èƒ½æœ‰æ•ˆç ´åå®‰å…¨ç­–ç•¥çš„çº¦æŸæ»¡è¶³æ€§ï¼Œæ­ç¤ºå½“å‰Safe RLæ–¹æ³•åœ¨å¯¹æŠ—ç¯å¢ƒä¸‹å­˜åœ¨æ˜¾è‘—å®‰å…¨éšæ‚£ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , å®‰å…¨å¼ºåŒ–å­¦ä¹ , æ·±åº¦å­¦ä¹ , reward model, é€†å‘çº¦æŸå¼ºåŒ–å­¦ä¹ , å¯¹æŠ—æ”»å‡», é»‘ç›’ç¯å¢ƒäº¤äº’, ç­–ç•¥é²æ£’æ€§åˆ†æ, å®‰å…¨çº¦æŸå»ºæ¨¡, ä¸“å®¶ç¤ºèŒƒå­¦ä¹ 

**è¯„åˆ†**ï¼š20

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16543v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16543v1.pdf)

---

## [30. Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing](https://arxiv.org/abs/2602.16531v1)

**ä½œè€…**ï¼šDaniel Boharon, Yehuda Dar  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-18

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.   We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.   Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç ”ç©¶çº¿æ€§å›å½’åœºæ™¯ä¸‹å¦‚ä½•åˆ©ç”¨å¤šä¸ªï¼ˆå¯èƒ½è¿‡å‚æ•°åŒ–çš„ï¼‰é¢„è®­ç»ƒæœ€å°äºŒä¹˜æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œå¹¶é€šè¿‡å»åç­–ç•¥ä»æ›´å¤šé¢„è®­ç»ƒæ¨¡å‹ä¸­è·ç›Šã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨è¿‡å‚æ•°åŒ–çº¿æ€§æ¨¡å‹å¹¿æ³›å­˜åœ¨çš„èƒŒæ™¯ä¸‹ï¼Œç°æœ‰è¿ç§»å­¦ä¹ é€šå¸¸åªåˆ©ç”¨å°‘é‡æˆ–å•ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå°šä¸æ¸…æ¥šåœ¨å­˜åœ¨è¿‡å‚æ•°åŒ–åå·®æ—¶ï¼Œå¦‚ä½•ç³»ç»Ÿåœ°åˆ©ç”¨â€œæ›´å¤šâ€é¢„è®­ç»ƒæ¨¡å‹æ¥æé«˜ç›®æ ‡ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†ç›®æ ‡ä»»åŠ¡å½¢å¼åŒ–ä¸ºï¼šåœ¨ç›®æ ‡æ•°æ®ä¸Šæœ€å°åŒ–å¹³æ–¹è¯¯å·®ï¼ŒåŒæ—¶åŠ ä¸Šä¸å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹å‚æ•°è·ç¦»çš„æƒ©ç½šé¡¹ï¼›åœ¨æ­¤æ¡†æ¶ä¸‹æ¨å¯¼ç›®æ ‡æ¨¡å‹çš„æµ‹è¯•è¯¯å·®è§£æå¼ï¼Œå¹¶æå‡ºé€šè¿‡ä¸€ä¸ªç®€å•çš„ä¹˜æ€§ä¿®æ­£å› å­å¯¹é¢„è®­ç»ƒæƒé‡è¿›è¡Œå»åï¼Œä»¥ç¼“è§£è¿‡å‚æ•°åŒ–å¸¦æ¥çš„å­ç©ºé—´é™åˆ¶ã€‚

**ä¸»è¦ç»“è®º**ï¼šç†è®ºä¸å®éªŒè¡¨æ˜ï¼šå½“é¢„è®­ç»ƒæ¨¡å‹æ˜¯è¿‡å‚æ•°åŒ–æ—¶ï¼Œåªæœ‰åœ¨æ•°é‡è¶³å¤Ÿå¤šæ—¶å¤šæ¨¡å‹è¿ç§»æ‰æ˜æ˜¾å—ç›Šï¼Œä½†å…¶æ•ˆæœä¼šå—åˆ°æœ€å°â„“2èŒƒæ•°è§£æ‰€å¸¦æ¥çš„è¿‡å‚æ•°åŒ–åå·®é™åˆ¶ï¼›é€šè¿‡æå‡ºçš„ä¹˜æ€§å»åä¿®æ­£ï¼Œå¯ä»¥æ˜¾è‘—å‡å¼±è¿™ä¸€åå·®ï¼Œä½¿æ›´å¤šé¢„è®­ç»ƒæ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸­è¢«æ›´æœ‰æ•ˆåœ°æ•´åˆï¼Œä»è€Œæå‡è¿ç§»å­¦ä¹ æ€§èƒ½ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , è¿ç§»å­¦ä¹ , çº¿æ€§å›å½’, é¢„è®­ç»ƒæ¨¡å‹, è¿‡å‚æ•°åŒ–å»å, æ³›åŒ–è¯¯å·®åˆ†æ, æ­£åˆ™åŒ–ä¼˜åŒ–, é«˜ç»´ç»Ÿè®¡ç†è®º, rag

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.16531v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.16531v1.pdf)

---

