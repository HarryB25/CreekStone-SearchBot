# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-19

> å…± 15 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.AI](#csAI) (1 ç¯‡)
- [cs.LG](#csLG) (10 ç¯‡)
- [cs.CL](#csCL) (1 ç¯‡)
- [cs.CV](#csCV) (3 ç¯‡)

---

## cs.AI

## [1. El Agente GrÃ¡fico: Structured Execution Graphs for Scientific Agents](https://arxiv.org/abs/2602.17902v1)

**ä½œè€…**ï¼šJiaru Bai, Abdulrahman Aldossary, Thomas Swanick ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.MA, cs.SE, physics.chem-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente GrÃ¡fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºâ€œEl Agente GrÃ¡ficoâ€å•æ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨ç±»å‹å®‰å…¨çš„æ‰§è¡Œå›¾ä¸çŸ¥è¯†å›¾è°±æŒä¹…åŒ–æ¥æ›¿ä»£åŸºäºéç»“æ„åŒ–æ–‡æœ¬çš„ç§‘å­¦æ™ºèƒ½ä½“ç¼–æ’ï¼Œå®ç°æ›´å¯é å¯å®¡è®¡çš„ç§‘å­¦å·¥ä½œæµè‡ªåŠ¨åŒ–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰LLMç§‘å­¦æ™ºèƒ½ä½“å¸¸ç”¨éç»“æ„åŒ–æ–‡æœ¬ç®¡ç†ä¸Šä¸‹æ–‡ä¸å·¥å…·åè°ƒï¼Œä¿¡æ¯å†—ä½™ä¸”éš¾ä»¥è¿½è¸ªå†³ç­–æ¥æºï¼Œå¯¼è‡´æ‰§è¡Œè„†å¼±ã€éš¾ä»¥å®¡è®¡ä¸å¤ç°ã€‚ä½œè€…å¸Œæœ›ç”¨ç»“æ„åŒ–ä¸ç±»å‹å®‰å…¨æœºåˆ¶æå‡ä¸€è‡´æ€§ã€å¯è¿½æº¯æ€§ä¸å·¥å…·ç¼–æ’çš„é²æ£’æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†LLMå†³ç­–åµŒå…¥ç±»å‹å®‰å…¨çš„æ‰§è¡Œç¯å¢ƒï¼šä»¥ç»“æ„åŒ–ç§‘å­¦æ¦‚å¿µæŠ½è±¡+å¯¹è±¡-å›¾æ˜ å°„å™¨å°†è®¡ç®—çŠ¶æ€è¡¨ç¤ºä¸ºå¸¦ç±»å‹çš„Pythonå¯¹è±¡ï¼Œå¹¶å¯æŒä¹…åŒ–åˆ°å¤–éƒ¨çŸ¥è¯†å›¾è°±ï¼›ä¸Šä¸‹æ–‡é€šè¿‡ç¬¦å·åŒ–çš„typedæ ‡è¯†ç¬¦ç®¡ç†è€ŒéåŸå§‹æ–‡æœ¬ï¼Œä»è€Œæ”¯æŒå¹¶è¡Œä¸å¤šæ­¥å·¥å…·è°ƒç”¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨é‡å­åŒ–å­¦ä»»åŠ¡åŸºå‡†ä¸Šï¼Œé…åˆå¯é æ‰§è¡Œå¼•æ“çš„å•æ™ºèƒ½ä½“å³å¯ç¨³å®šå®Œæˆå¤æ‚å¤šæ­¥ä¸å¹¶è¡Œè®¡ç®—ï¼Œæ•ˆæœå¯æ›¿ä»£å…ˆå‰å¤šæ™ºèƒ½ä½“æ–¹æ¡ˆï¼›åœ¨æ„è±¡é›†ç”Ÿæˆä¸MOFè®¾è®¡ä¸­ï¼ŒçŸ¥è¯†å›¾è°±åŒæ—¶ä½œä¸ºè®°å¿†ä¸æ¨ç†åº•åº§ï¼Œè¯æ˜ç±»å‹å®‰å…¨ä¸æŠ½è±¡å¯ä¸ºå¯æ‰©å±•çš„ç§‘å­¦æ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æä¾›æ›´ç¨³å¥åŸºç¡€ã€‚

**å…³é”®è¯**ï¼šç§‘å­¦å·¥ä½œæµè‡ªåŠ¨åŒ–, ç§‘å­¦æ™ºèƒ½ä½“, ç»“æ„åŒ–æ‰§è¡Œå›¾, ç±»å‹å®‰å…¨æ‰§è¡Œç¯å¢ƒ, çŸ¥è¯†å›¾è°±è®°å¿†, ç¬¦å·åŒ–ä¸Šä¸‹æ–‡ç®¡ç†, å·¥å…·ç¼–æ’, è‡ªåŠ¨åŒ–åŸºå‡†è¯„æµ‹, é‡å­åŒ–å­¦ä»»åŠ¡

**è¯„åˆ†**ï¼š54

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17902v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17902v1.pdf)

---

## cs.CL

## [2. Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881v1)

**ä½œè€…**ï¼šJoschka Braun  
**åˆ†ç±»**ï¼šcs.CL, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šç ”ç©¶è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹ä¸­çš„å¼•å¯¼å‘é‡åœ¨ä¸åŒç›®æ ‡è¡Œä¸ºä¸Šçš„å¯é æ€§å·®å¼‚ä¸è®­ç»ƒæ•°æ®çš„ç›¸ä¼¼æ€§å’Œåˆ†ç¦»ç¨‹åº¦æœ‰å…³ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ¢è®¨å¼•å¯¼å‘é‡åœ¨æ§åˆ¶è¯­è¨€æ¨¡å‹è¡Œä¸ºæ—¶çš„å¯é æ€§é—®é¢˜ï¼Œæ—¨åœ¨ç†è§£å…¶å˜å¼‚æ€§åŠå½±å“å› ç´ ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡åˆ†æè®­ç»ƒæ¿€æ´»å·®å¼‚çš„ä½™å¼¦ç›¸ä¼¼æ€§å’Œè¡Œä¸ºæ•°æ®é›†ä¸­æ­£è´Ÿæ¿€æ´»çš„åˆ†ç¦»ç¨‹åº¦ï¼Œè¯„ä¼°å¼•å¯¼å‘é‡çš„å¯é æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šå¼•å¯¼å‘é‡åœ¨ç›®æ ‡è¡Œä¸ºè¡¨ç¤ºæ— æ³•æœ‰æ•ˆçº¿æ€§è¿‘ä¼¼æ—¶è¡¨ç°ä¸å¯é ï¼Œæç¤ºéœ€è¦å¼€å‘æ›´ç¨³å¥çš„å¼•å¯¼æ–¹æ³•ä»¥å¤„ç†éçº¿æ€§è¡Œä¸ºè¡¨ç¤ºã€‚

**å…³é”®è¯**ï¼šLLM è¡Œä¸ºæ§åˆ¶, æ¿€æ´»åç½®æ³¨å…¥, æ¨ç†æ—¶å¹²é¢„, å¯æ§æ€§å¯é æ€§, è¡Œä¸ºè¡¨ç¤ºçº¿æ€§è¿‘ä¼¼, ä½™å¼¦ç›¸ä¼¼åº¦é¢„æµ‹, æ¿€æ´»å·®åˆ†ä¸€è‡´æ€§, æŠ•å½±å¯åˆ†æ€§, æç¤ºå˜ä½“é²æ£’æ€§, éçº¿æ€§æ½œåœ¨è¡¨ç¤º

**è¯„åˆ†**ï¼š30

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17881v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17881v1.pdf)

---

## cs.CV

## [3. Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871v1)

**ä½œè€…**ï¼šDhruba Ghosh, Yuhui Zhang, Ludwig Schmidt  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.LG, cs.MM  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ç³»ç»Ÿè¯„æµ‹å¤šç§è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦åˆ†ç±»ä¸Šçš„èƒ½åŠ›ï¼Œå‘ç°æå‡è§†è§‰ç¼–ç å™¨ä¸é¢„è®­ç»ƒç­–ç•¥ï¼ˆå°¤å…¶è§£å†»è¯­è¨€æ¨¡å‹æƒé‡ï¼‰å¯¹ç»†ç²’åº¦çŸ¥è¯†æœ€å…³é”®ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå°½ç®¡VLMåœ¨VQAã€æ–‡æ¡£ç†è§£ç­‰åŸºå‡†ä¸Šè¡¨ç°æ˜¾è‘—æå‡ï¼Œä½†åœ¨ä¼ ç»Ÿç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸Šè½åï¼Œè¯´æ˜å…¶â€œç»†ç²’åº¦è§†è§‰çŸ¥è¯†â€ä¸å…¶ä»–è§†è§‰èƒ½åŠ›å­˜åœ¨è„±èŠ‚ï¼Œéœ€è¦æ‰¾å‡ºåŸå› ä¸æ”¹è¿›è·¯å¾„ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å¯¹å¤§é‡è¿‘æœŸVLMåœ¨ç»†ç²’åº¦åˆ†ç±»åŸºå‡†ä¸Šè¿›è¡Œå¯¹æ¯”è¯„æµ‹ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒåˆ†åˆ«æ›¿æ¢/å¢å¼ºLLMã€è§†è§‰ç¼–ç å™¨ä¸é¢„è®­ç»ƒè®¾ç½®ï¼ˆå«æ˜¯å¦åœ¨é¢„è®­ç»ƒé˜¶æ®µè§£å†»è¯­è¨€æ¨¡å‹æƒé‡ï¼‰æ¥å®šä½å½±å“å› ç´ ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ›´å¼ºçš„LLMä¼šè¾ƒä¸ºå‡è¡¡åœ°æå‡å„ç±»åŸºå‡†ï¼Œè€Œæ›´å¼ºçš„è§†è§‰ç¼–ç å™¨ä¼šå¯¹ç»†ç²’åº¦åˆ†ç±»å¸¦æ¥æ›´æ˜¾è‘—çš„å¢ç›Šï¼›æ­¤å¤–é¢„è®­ç»ƒé˜¶æ®µå¯¹ç»†ç²’åº¦æ€§èƒ½è‡³å…³é‡è¦ï¼Œå°¤å…¶å½“é¢„è®­ç»ƒæ—¶è§£å†»è¯­è¨€æ¨¡å‹æƒé‡æ—¶ç»†ç²’åº¦è¡¨ç°æå‡æ›´æ˜æ˜¾ã€‚

**å…³é”®è¯**ï¼šè§†è§‰è¯­è¨€æ¨¡å‹, ç»†ç²’åº¦å›¾åƒåˆ†ç±», ç»†ç²’åº¦è§†è§‰çŸ¥è¯†, è§†è§‰é—®ç­”è¯„æµ‹, ç»†ç²’åº¦åˆ†ç±»åŸºå‡†, æ¶ˆèå®éªŒ, è§†è§‰ç¼–ç å™¨, LLM, è·¨æ¨¡æ€å¯¹é½æ¶æ„, é¢„è®­ç»ƒç­–ç•¥

**è¯„åˆ†**ï¼š22

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17871v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17871v1.pdf)

---

## [4. Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869v1)

**ä½œè€…**ï¼šYuxiao Chen, Jue Wang, Zhikang Zhang ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯æ¶æ„ï¼Œé€šè¿‡ä¿¡æ¯å¯†åº¦è‡ªé€‚åº”è§†é¢‘é‡‡æ ·å™¨å’ŒåŸºäºè‡ªç¼–ç å™¨çš„æ—¶ç©ºè§†é¢‘å‹ç¼©å™¨ï¼Œå®ç°äº†é«˜æ•ˆçš„é•¿æ ¼å¼è§†é¢‘ç†è§£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€è§†é¢‘æ¶æ„çš„è¿›æ­¥å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œåˆ†æé•¿æ—¶é—´è§†é¢‘æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œè§†é¢‘åºåˆ—çš„å†—ä½™ç‰¹æ€§ç»™ç°æœ‰æ¨¡å‹å¸¦æ¥äº†å†…å­˜å’Œä¿¡æ¯æå–çš„æŒ‘æˆ˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§é›†æˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”è§†é¢‘é‡‡æ ·å™¨å’Œæ—¶ç©ºè§†é¢‘å‹ç¼©å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé‡è¦ä¿¡æ¯çš„åŒæ—¶é«˜æ•ˆå‹ç¼©è§†é¢‘æ•°æ®ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ¡†æ¶åœ¨é•¿æ ¼å¼è§†é¢‘ç†è§£ä»»åŠ¡å’Œæ ‡å‡†è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤æ‚è§†é¢‘åºåˆ—ä¸­çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šç´§å‡‘è§†é¢‘è¡¨å¾, è‡ªé€‚åº”è§†é¢‘é‡‡æ ·, ä¿¡æ¯å¯†åº¦å»ºæ¨¡, å¸§é€‰æ‹©, æ—¶ç©ºè§†é¢‘å‹ç¼©, è§†é¢‘è‡ªç¼–ç å™¨, å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰, è§†é¢‘å†—ä½™æ¶ˆé™¤, å†…å­˜å—é™æ¨ç†, é«˜å‹ç¼©ç‡è¡¨å¾

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17869v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17869v1.pdf)

---

## [5. On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854v1)

**ä½œè€…**ï¼šDomonkos Varga  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and SzirÃ¡nyi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡åˆ†æäº†åŸºäºæ·±åº¦å­¦ä¹ çš„æ‰‹åŠ¿è¯†åˆ«è¯„ä¼°åè®®ï¼Œæ­ç¤ºæ•°æ®æ³„æ¼é—®é¢˜å½±å“äº†å‡†ç¡®æ€§è¯„ä¼°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç ”ç©¶æ—¨åœ¨æ­ç¤ºç°æœ‰æ‰‹åŠ¿è¯†åˆ«è¯„ä¼°æ–¹æ³•çš„ç¼ºé™·ï¼Œä»¥æé«˜å…¶åœ¨æ— äººæœºæ•‘æ´æ“ä½œä¸­çš„å¯é æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡åˆ†ææ··æ·†çŸ©é˜µã€å­¦ä¹ æ›²çº¿åŠæ•°æ®é›†æ„å»ºï¼ŒæŒ‡å‡ºç°æœ‰è¯„ä¼°æ–¹æ³•æœªèƒ½æœ‰æ•ˆæµ‹é‡å¯¹æœªè§ä¸ªä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚

**ä¸»è¦ç»“è®º**ï¼šå¼ºè°ƒåœ¨è§†è§‰æ‰‹åŠ¿è¯†åˆ«ç ”ç©¶ä¸­é‡‡ç”¨ç‹¬ç«‹äºä¸»ä½“çš„æ•°æ®åˆ’åˆ†çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿å¯¹æ–°ç”¨æˆ·çš„æ‰‹åŠ¿è¯†åˆ«å‡†ç¡®æ€§ã€‚

**å…³é”®è¯**ï¼šæ‰‹åŠ¿è¯†åˆ«, æ— äººæœºæ•‘æ´, æ— äººæœº-äººäº¤äº’, è¯„æµ‹åè®®, å—è¯•è€…ç‹¬ç«‹åˆ’åˆ†, è·¨ä¸»ä½“æ³›åŒ–, æ•°æ®æ³„æ¼, å¸§çº§éšæœºåˆ’åˆ†, è®­ç»ƒ-æµ‹è¯•åˆ’åˆ†, æ··æ·†çŸ©é˜µåˆ†æ, å­¦ä¹ æ›²çº¿åˆ†æ

**è¯„åˆ†**ï¼š15

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17854v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17854v1.pdf)

---

## cs.LG

## [6. Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898v1)

**ä½œè€…**ï¼šJingquan Yan, Yuwei Miao, Peiran Yu ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ä»ç†è®ºä¸Šè§£é‡Šäº†æ³¨æ„åŠ›å›å½’è®­ç»ƒä¸­â€œPCCï¼ˆçš®å°”é€Šç›¸å…³ï¼‰æ—©æ—©åœæ»è€ŒMSEç»§ç»­ä¸‹é™â€çš„åŸå› ï¼Œå¹¶æå‡ºECAæœºåˆ¶çªç ´è¯¥ç›¸å…³æ€§å¹³å°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ³¨æ„åŠ›å›å½’å¸¸è”åˆä¼˜åŒ–MSEä¸PCCï¼Œä½†è®­ç»ƒä¸­PCCå¾ˆå¿«è¿›å…¥å¹³å°æœŸã€éš¾ä»¥ç»§ç»­æå‡ï¼Œç°è±¡æ™®éå´ç¼ºä¹ä¸¥è°¨è§£é‡Šä¸æœ‰æ•ˆæ”¹è¿›æ‰‹æ®µã€‚ä½œè€…å¸Œæœ›å˜æ¸…å…¶ä¼˜åŒ–åŠ¨åŠ›å­¦ä¸æ¨¡å‹å®¹é‡ä¸Šçš„æ ¹æœ¬é™åˆ¶ï¼Œå¹¶ç»™å‡ºå¯çªç ´é™åˆ¶çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç†è®ºä¸Šåˆ†æè”åˆæŸå¤±ä¸‹çš„æ¢¯åº¦å†²çªï¼šMSEä¸‹é™ä¼šæŠ‘åˆ¶PCCçš„æ¢¯åº¦æ›´æ–°ï¼Œä¸”åœ¨è¾“å…¥åŒè´¨ã€softmaxæ³¨æ„åŠ›è¶‹äºå¹³å‡/å¡Œç¼©æ—¶é—®é¢˜æ›´ä¸¥é‡ï¼›è¿›ä¸€æ­¥è¯æ˜ä»»ä½•â€œå‡¸èšåˆå™¨â€ï¼ˆå«softmaxæ³¨æ„åŠ›ï¼‰çš„PCCæå‡å—è¾“å…¥å‡¸åŒ…ä¸¥æ ¼çº¦æŸã€‚åŸºäºä¸Šè¿°ç»“è®ºæå‡ºExtrapolative Correlation Attentionï¼ˆECAï¼‰ï¼Œé€šè¿‡å¯â€œå¤–æ¨â€è¶…å‡ºå‡¸åŒ…çš„æœºåˆ¶ä¸æ”¹è¿›çš„ç›¸å…³æ€§ä¼˜åŒ–è®¾è®¡æ¥æå‡PCCä¸”ä¸ç‰ºç‰²MSEã€‚

**ä¸»è¦ç»“è®º**ï¼šPCCå¹³å°æœŸæ¥è‡ªä¸¤ç±»æ ¹å› ï¼šä¼˜åŒ–å±‚é¢çš„MSEâ†”PCCæ¢¯åº¦å†²çªä¸softmaxåœ¨åŒè´¨æ•°æ®ä¸‹çš„æ”¾å¤§æ•ˆåº”ï¼Œä»¥åŠå®¹é‡å±‚é¢çš„å‡¸èšåˆå™¨å‡¸åŒ…ä¸Šç•Œå¯¼è‡´çš„PCCæå‡æé™ï¼›æ•°æ®åŒè´¨æ€§ä¼šåŒæ—¶åŠ å‰§ä¸¤è€…ã€‚å®éªŒè¡¨æ˜ECAèƒ½ç¨³å®šæ‰“ç ´PCCå¹³å°ï¼Œåœ¨å¤šç§åŸºå‡†ï¼ˆå«é«˜åº¦åŒè´¨åœºæ™¯ï¼‰æ˜¾è‘—æå‡ç›¸å…³æ€§å¹¶ä¿æŒMSEæ€§èƒ½ã€‚

**å…³é”®è¯**ï¼šæ³¨æ„åŠ›å›å½’, MSE-PCC è”åˆæŸå¤±, ç›¸å…³ç³»æ•°å¹³å°æœŸ, ç›¸å…³ç³»æ•°æ¢¯åº¦æŠ‘åˆ¶, ä¼˜åŒ–åŠ¨åŠ›å­¦, æ¨¡å‹å®¹é‡ä¸Šé™, å‡¸èšåˆå™¨, å‡¸åŒ…çº¦æŸ, æ•°æ®åŒè´¨æ€§, å¤–æ¨ç›¸å…³æ³¨æ„åŠ›ï¼ˆECAï¼‰, å‡¸åŒ…å¤–æ¨

**è¯„åˆ†**ï¼š28

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17898v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17898v1.pdf)

---

## [7. COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models](https://arxiv.org/abs/2602.17893v1)

**ä½œè€…**ï¼šJiajun Shen, Yufei Jin, Yi He ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCOMBAæ˜¯ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡å›¾å­¦ä¹ çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œé€šè¿‡å›¾ä¸Šä¸‹æ–‡é—¨æ§å’Œè·¨æ‰¹æ¬¡èšåˆæ¥æå‡æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨å¤„ç†å¤§å‹å›¾æ•°æ®æ—¶ï¼Œä¼ ç»Ÿçš„çŠ¶æ€ç©ºé—´æ¨¡å‹é¢ä¸´è½¬æ¢å›¾ä¸ºåºåˆ—çš„é«˜æˆæœ¬é—®é¢˜ï¼Œå› æ­¤éœ€è¦ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å­¦ä¹ å›¾ç»“æ„æ•°æ®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šCOMBAé€šè¿‡å¼•å…¥å›¾ä¸Šä¸‹æ–‡é—¨æ§å’Œè·¨æ‰¹æ¬¡èšåˆçš„æŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ èŠ‚ç‚¹çš„é‚»å±…ä¿¡æ¯ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¿¡æ¯çš„è·¨æ‰¹æ¬¡èšåˆã€‚

**ä¸»è¦ç»“è®º**ï¼šç ”ç©¶è¡¨æ˜ï¼Œè·¨æ‰¹æ¬¡èšåˆèƒ½å¤Ÿæ˜¾è‘—é™ä½è¯¯å·®ï¼Œå¹¶åœ¨æ ‡å‡†ç½‘ç»œä¸Šå®ç°äº†ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚

**å…³é”®è¯**ï¼šå¤§è§„æ¨¡å›¾å­¦ä¹ , çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰, å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰, é•¿ç¨‹ä¾èµ–å»ºæ¨¡, å›¾ä¸Šä¸‹æ–‡é—¨æ§, è·¨æ‰¹æ¬¡èšåˆ, é‚»å±…èšåˆæ§åˆ¶, å¤šè·³é‚»åŸŸå»ºæ¨¡, èŠ‚ç‚¹æ‰¹é‡‡æ ·, å¯æ‰©å±•è®­ç»ƒ, è¯¯å·®ç•Œç†è®ºåˆ†æ, åŸºå‡†å›¾æ•°æ®é›†è¯„æµ‹

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17893v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17893v1.pdf)

---

## [8. Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888v1)

**ä½œè€…**ï¼šSayeed Shafayet Chowdhury, Karen D'Souza, V. Siva Kakumani ç­‰ 12 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç”¨æœ¯å‰ä¸´åºŠæ•°æ®è®­ç»ƒç›‘ç£å¼æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé¢„æµ‹æ…¢æ€§é¼»çª¦ç‚ï¼ˆCRSï¼‰æ‚£è€…æ˜¯å¦èƒ½ä»æ‰‹æœ¯ä¸­è·ç›Šï¼Œå¹¶åœ¨å‡†ç¡®ç‡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡ä¸“å®¶ä¸´åºŠåˆ¤æ–­ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šCRSæ‰‹æœ¯å†³ç­–éœ€è¦åœ¨å·²çŸ¥æ‰‹æœ¯é£é™©ä¸ä¸ªä½“åŒ–ç–—æ•ˆä¸ç¡®å®šæ€§é—´æƒè¡¡ï¼Œè€Œé’ˆå¯¹å‰ç»æ€§ã€æ ‡å‡†åŒ–ä¸´åºŠè¯•éªŒæ•°æ®çš„æœºå™¨å­¦ä¹ é¢„åé¢„æµ‹ç ”ç©¶ä»ç›¸å¯¹ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåŸºäºå‰ç»æ€§è§‚å¯Ÿæ€§å¹²é¢„è¯•éªŒé˜Ÿåˆ—ï¼ˆå‡æ¥å—æ‰‹æœ¯ï¼‰ï¼Œä»¥SNOT-22ä½œä¸ºä¸»è¦ç»“å±€æ ‡ç­¾ï¼Œä»…ä½¿ç”¨æœ¯å‰ç‰¹å¾è®­ç»ƒå¤šç§ç›‘ç£å­¦ä¹ ç®—æ³•åŠé›†æˆæ¨¡å‹ï¼Œè¯„ä¼°å…¶å¯¹â€œæ‰‹æœ¯è·ç›Š/å€™é€‰æ€§â€çš„åˆ†ç±»é¢„æµ‹ï¼Œå¹¶ä¸ä¸“å®¶åœ¨ç‹¬ç«‹ç•™å‡ºæ ·æœ¬ä¸Šçš„è¡¨ç°å¯¹æ¯”ã€‚

**ä¸»è¦ç»“è®º**ï¼šæœ€ä½³æ¨¡å‹åœ¨æ•´ä½“ä¸Šçº¦è¾¾85%åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸”åœ¨30ä¾‹æ··åˆéš¾åº¦ç•™å‡ºé›†ä¸Šè¾¾80%ï¼Œä¼˜äºä¸“å®¶å¹³å‡75.6%ï¼Œæ˜¾ç¤ºå…¶å¯æä¾›å¯è§£é‡Šçš„å€™é€‰æ€§é¢„æµ‹å¹¶è¾…åŠ©ä¸ªä½“åŒ–æ‰‹æœ¯å†³ç­–ã€‚

**å…³é”®è¯**ï¼šæ…¢æ€§é¼»çª¦ç‚, æ‰‹æœ¯ç»“å±€é¢„æµ‹, æ‰‹æœ¯è·ç›Šé¢„æµ‹, æœ¯å‰ä¸´åºŠæ•°æ®, æ‚£è€…æŠ¥å‘Šç»“å±€, ç›‘ç£å­¦ä¹ , é›†æˆå­¦ä¹ , å¯è§£é‡Šæ€§é¢„æµ‹æ¨¡å‹, è§‚å¯Ÿæ€§å¹²é¢„è¯•éªŒæ•°æ®, æ‰‹æœ¯é€‚åº”è¯åˆ¤åˆ«

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17888v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17888v1.pdf)

---

## [9. The Geometry of Multi-Task Grokking: Transverse Instability, Superposition, and Weight Decay Phase Structure](https://arxiv.org/abs/2602.18523v1)

**ä½œè€…**ï¼šYongzhong Xu  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Grokking -- the abrupt transition from memorization to generalization long after near-zero training loss -- has been studied mainly in single-task settings. We extend geometric analysis to multi-task modular arithmetic, training shared-trunk Transformers on dual-task (mod-add + mod-mul) and tri-task (mod-add + mod-mul + mod-sq) objectives across a systematic weight decay sweep. Five consistent phenomena emerge. (1) Staggered grokking order: multiplication generalizes first, followed by squaring, then addition, with consistent delays across seeds. (2) Universal integrability: optimization trajectories remain confined to an empirically invariant low-dimensional execution manifold; commutator defects orthogonal to this manifold reliably precede generalization. (3) Weight decay phase structure: grokking timescale, curvature depth, reconstruction threshold, and defect lead covary systematically with weight decay, revealing distinct dynamical regimes and a sharp no-decay failure mode. (4) Holographic incompressibility: final solutions occupy only 4--8 principal trajectory directions yet are distributed across full-rank weights and destroyed by minimal perturbations; SVD truncation, magnitude pruning, and uniform scaling all fail to preserve performance. (5) Transverse fragility and redundancy: removing less than 10% of orthogonal gradient components eliminates grokking, yet dual-task models exhibit partial recovery under extreme deletion, suggesting redundant center manifolds enabled by overparameterization. Together, these results support a dynamical picture in which multi-task grokking constructs a compact superposition subspace in parameter space, with weight decay acting as compression pressure and excess parameters supplying geometric redundancy in optimization pathways.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šåœ¨å¤šä»»åŠ¡æ¨¡å—ç®—æœ¯ä¸Šï¼Œä½œè€…ç”¨å‡ ä½•è§†è§’æ­ç¤ºå¤šä»»åŠ¡grokkingå­˜åœ¨ç¨³å®šçš„ä½ç»´æ‰§è¡Œæµå½¢ä¸å¯é¢„æµ‹çš„â€œæ¨ªå‘ç¼ºé™·â€å‰å…†ï¼Œå¹¶å‘ˆç°éšweight decayå˜åŒ–çš„æ¸…æ™°ç›¸ç»“æ„ä¸è„†å¼±/å†—ä½™å¹¶å­˜çš„ä¼˜åŒ–åŠ¨åŠ›å­¦ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä»¥å¾€grokkingç ”ç©¶å¤šèšç„¦å•ä»»åŠ¡ï¼Œå°šä¸æ¸…æ¥šå¤šä»»åŠ¡å…±äº«å‚æ•°ä¸‹çš„æ³›åŒ–è·ƒè¿é¡ºåºã€å‡ ä½•æœºåˆ¶ä»¥åŠæ­£åˆ™åŒ–ï¼ˆweight decayï¼‰å¦‚ä½•å¡‘é€ å…¶åŠ¨åŠ›å­¦ä¸å¯è¾¾æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®­ç»ƒå…±äº«trunkçš„TransformeråŒæ—¶å­¦ä¹ mod-add/mod-mulï¼ˆåŠåŠ ä¸Šmod-sqï¼‰çš„å¤šä»»åŠ¡ç›®æ ‡ï¼Œç³»ç»Ÿæ‰«weight decayï¼›ç”¨è½¨è¿¹å‡ ä½•åˆ†æï¼ˆä½ç»´æµå½¢/ä¸»æ–¹å‘ã€æ›²ç‡ä¸â€œå¯¹æ˜“å­ç¼ºé™·â€ç­‰æ¨ªå‘é‡ï¼‰ã€ä»¥åŠåˆ å‡æ­£äº¤æ¢¯åº¦åˆ†é‡ä¸SVDæˆªæ–­/å‰ªæ/ç¼©æ”¾ç­‰æ‰°åŠ¨å®éªŒè¯„ä¼°ç¨³å®šæ€§ä¸å¯å‹ç¼©æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šè§‚å¯Ÿåˆ°ç¨³å®šçš„åˆ†é˜¶æ®µgrokkingé¡ºåºï¼ˆä¹˜æ³•â†’å¹³æ–¹â†’åŠ æ³•ï¼‰ä¸â€œæ™®é€‚å¯ç§¯â€ä½ç»´æ‰§è¡Œæµå½¢ï¼Œä¸”æ­£äº¤äºæµå½¢çš„ç¼ºé™·ä¿¡å·å¯é¢†å…ˆé¢„æµ‹æ³›åŒ–ï¼›weight decayå¼•å…¥ä¸åŒåŠ¨åŠ›å­¦ç›¸å¹¶åœ¨æ— è¡°å‡æ—¶å‡ºç°å¤±è´¥æ¨¡å¼ï¼›æœ€ç»ˆè§£åœ¨å°‘æ•°è½¨è¿¹ä¸»æ–¹å‘ä¸Šâ€œä¸å¯å‹ç¼©â€ä¸”å¯¹å¾®å°æ‰°åŠ¨æè„†å¼±ï¼ŒåŒæ—¶è¿‡å‚æ•°åŒ–å¸¦æ¥ä¸€å®šå†—ä½™ä¸­å¿ƒæµå½¢ä½¿æç«¯åˆ é™¤ä¸‹ä»å¯èƒ½éƒ¨åˆ†æ¢å¤ã€‚

**å…³é”®è¯**ï¼šæ¨¡å—åŒ–ç®—æœ¯ä»»åŠ¡, æƒé‡è¡°å‡, ç›¸ç»“æ„, ä¼˜åŒ–è½¨è¿¹å‡ ä½•, ä½ç»´æ‰§è¡Œæµå½¢, å‚æ•°ç©ºé—´å åŠ å­ç©ºé—´, ä¸­å¿ƒæµå½¢å†—ä½™, æ¨ªå‘ä¸ç¨³å®šæ€§, ä¸å¯å‹ç¼©æ€§

**è¯„åˆ†**ï¼š19

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.18523v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.18523v1.pdf)

---

## [10. ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867v1)

**ä½œè€…**ï¼šJoÃ£o N. Cardoso, Arlindo L. Oliveira, Bruno Martins  
**åˆ†ç±»**ï¼šcs.LG, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šADAPT é€šè¿‡â€œæŸæœç´¢åˆå§‹åŒ– + è‡ªé€‚åº”æ¢¯åº¦å¼•å¯¼å˜å¼‚â€çš„æ··åˆæç¤ºä¼˜åŒ–ï¼Œä½¿åœ¨LLMæ¿€æ´»ç©ºé—´ä¸­å¯¹ç‰¹å¾æ–¹å‘çš„å¯è§£é‡Šè¾“å…¥æœç´¢æ›´ç¨³å®šã€æ›´æœ‰æ•ˆã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¸ºç†è§£LLMæ¿€æ´»ç©ºé—´ä¸­å­¦ä¹ åˆ°çš„æ–¹å‘ï¼ˆå¦‚SAEéšå˜é‡ï¼‰ç¼–ç äº†ä»€ä¹ˆï¼Œéœ€è¦æ‰¾åˆ°èƒ½å¼ºæ¿€æ´»è¯¥æ–¹å‘çš„æ–‡æœ¬è¾“å…¥ï¼›ä½†æ–‡æœ¬ç¦»æ•£æ€§ä¸æç¤ºä¼˜åŒ–æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œä½¿ç°æœ‰ç‰¹å¾å¯è§†åŒ–æ–¹æ³•æ•ˆæœä¸ä½³ä¸”å¯¹æ¯”ä¸å¤Ÿä¸¥è°¨ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºADAPTï¼šå…ˆç”¨beam searchç”Ÿæˆé«˜æ¿€æ´»çš„åˆå§‹promptä»¥ç¼“è§£å±€éƒ¨æå°å€¼é—®é¢˜ï¼Œå†ç”¨è‡ªé€‚åº”çš„ã€ç”±æ¢¯åº¦ä¿¡å·å¼•å¯¼çš„ç¦»æ•£â€œå˜å¼‚/ç¼–è¾‘â€è¿­ä»£ä¼˜åŒ–æ–‡æœ¬ï¼›å¹¶åœ¨Gemma 2 2Bçš„ç¨€ç–è‡ªç¼–ç å™¨(latents)ä¸Šè¯„ä¼°ï¼Œä½¿ç”¨åŸºäºæ•°æ®é›†æ¿€æ´»ç»Ÿè®¡çš„æŒ‡æ ‡è¿›è¡Œå¯æ¯”æ€§è¡¡é‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šADAPTåœ¨ä¸åŒå±‚ä¸ä¸åŒlatentç±»å‹ä¸Šç¨³å®šä¼˜äºæ—¢æœ‰æç¤ºä¼˜åŒ–/ç‰¹å¾å¯è§†åŒ–æ–¹æ³•ï¼Œè¡¨æ˜LLMç‰¹å¾å¯è§†åŒ–æ˜¯å¯è¡Œçš„ï¼Œä½†éœ€è¦é’ˆå¯¹ç¦»æ•£æ–‡æœ¬ä¸å±€éƒ¨æœ€ä¼˜ç­‰é¢†åŸŸå¤±æ•ˆæ¨¡å¼åšä¸“é—¨è®¾è®¡ã€‚

**å…³é”®è¯**ï¼šç‰¹å¾å¯è§†åŒ–, LLM, æç¤ºä¼˜åŒ–, æ··åˆæ–¹æ³•, è‡ªé€‚åº”æ¢¯åº¦, å±€éƒ¨æœ€ä¼˜, ç¨€ç–è‡ªç¼–ç å™¨, æ¿€æ´»ç»Ÿè®¡, æ€§èƒ½è¯„ä¼°

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17867v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17867v1.pdf)

---

## [11. Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865v1)

**ä½œè€…**ï¼šAndrzej PodobiÅ„ski, JarosÅ‚aw A. Chudziak  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šN/A

**ç ”ç©¶åŠ¨æœº**ï¼šN/A

**æ ¸å¿ƒæ–¹æ³•**ï¼šN/A

**ä¸»è¦ç»“è®º**ï¼šN/A

**å…³é”®è¯**ï¼šé‡‘èæ—¶é—´åºåˆ—, æ—¶é—´åºåˆ—é¢„æµ‹, æ•°æ®å¢å¼º, ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰, åˆæˆæ—¶é—´åºåˆ—æ•°æ®, æ•°æ®ç¨€ç¼º, ç”Ÿæˆæ•°æ®è´¨é‡è¯„ä¼°, åŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰, æ•°æ®é›†å·®å¼‚åº¦é‡ï¼ˆDeD-iMsï¼‰, å¤šé¢„æµ‹æ­¥é•¿

**è¯„åˆ†**ï¼š21

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17865v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17865v1.pdf)

---

## [12. JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861v1)

**ä½œè€…**ï¼šRyan McKenna, Galen Andrew, Borja Balle ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šJAX-Privacy æ˜¯ä¸€ä¸ªé¢å‘å·®åˆ†éšç§æœºå™¨å­¦ä¹ çš„ JAX åº“ï¼Œæä¾›å¯éªŒè¯ä¸”æ¨¡å—åŒ–çš„éšç§æœºåˆ¶ç»„ä»¶ï¼Œå…¼é¡¾æ˜“ç”¨æ€§ã€çµæ´»æ€§ä¸æ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå·®åˆ†éšç§è®­ç»ƒæ¶‰åŠæ‰¹æ¬¡é‡‡æ ·ã€æ¢¯åº¦è£å‰ªã€åŠ å™ªã€éšç§ä¼šè®¡ä¸å®¡è®¡ç­‰å¤æ‚ç¯èŠ‚ï¼Œå®è·µä¸­å®¹æ˜“å‡ºé”™ä¸”éš¾ä»¥å…¼é¡¾æ€§èƒ½ä¸å¯å¤ç°ã€‚ä½œè€…å¸Œæœ›ç”¨ç»Ÿä¸€çš„åº“é™ä½éƒ¨ç½²é—¨æ§›ï¼ŒåŒæ—¶æ”¯æŒç ”ç©¶è€…æ·±åº¦å®šåˆ¶ä¸å·¥ç¨‹è½åœ°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåº“ä»¥â€œå¯ç”¨ã€çµæ´»ã€é«˜æ•ˆâ€ä¸ºè®¾è®¡åŸåˆ™ï¼Œæä¾›ç»è¿‡éªŒè¯çš„æ¨¡å—åŒ–åŸè¯­è¦†ç›– DP æœºåˆ¶è®¾è®¡å…¨é“¾è·¯ï¼šbatch selectionã€gradient clippingã€noise additionã€accountingã€auditingï¼Œå¹¶æ•´åˆè¿‘æœŸ DP-ML ç ”ç©¶æˆæœä»¥ä¾¿ç»„åˆä¸æ‰©å±•ã€‚

**ä¸»è¦ç»“è®º**ï¼šJAX-Privacy é€šè¿‡æ ‡å‡†åŒ–ä¸”å¯ç»„åˆçš„ DP ç»„ä»¶ï¼Œä½¿å·®åˆ†éšç§æœºå™¨å­¦ä¹ æ›´å®¹æ˜“æ­£ç¡®å®ç°å¹¶ä¿æŒè¾ƒé«˜æ€§èƒ½ï¼›åŒæ—¶æ—¢èƒ½å¼€ç®±å³ç”¨ï¼Œä¹Ÿèƒ½æ”¯æŒç ”ç©¶çº§åˆ«çš„æœºåˆ¶æ¢ç´¢ä¸è‡ªå®šä¹‰ã€‚

**å…³é”®è¯**ï¼šå·®åˆ†éšç§, éšç§ä¿æŠ¤æœºå™¨å­¦ä¹ , æ¢¯åº¦è£å‰ª, å™ªå£°æ³¨å…¥, éšç§å®¡è®¡, æ‰¹é‡é‡‡æ ·, æ¨¡å—åŒ–éšç§æœºåˆ¶, JAX-Privacy

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17861v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17861v1.pdf)

---

## [13. Neural Prior Estimation: Learning Class Priors from Latent Representations](https://arxiv.org/abs/2602.17853v1)

**ä½œè€…**ï¼šMasoud Yavari, Payman Moallem  
**åˆ†ç±»**ï¼šcs.LG, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºNPEä»ç½‘ç»œæ½œè¡¨ç¤ºä¸­å­¦ä¹ ç±»åˆ«å…ˆéªŒå¹¶ç”¨äºlogitè°ƒæ•´ï¼Œåœ¨ä¸ä¾èµ–æ˜¾å¼ç±»é¢‘ç»Ÿè®¡çš„æƒ…å†µä¸‹æå‡é•¿å°¾ä¸ä¸å‡è¡¡ä»»åŠ¡è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç±»åˆ«ä¸å‡è¡¡ä¼šåœ¨è®­ç»ƒä¸­å¼•å…¥åæ–œçš„æœ‰æ•ˆç±»å…ˆéªŒï¼Œå¯¼è‡´æ¨¡å‹å¯¹å°‘æ•°ç±»ç³»ç»Ÿæ€§åç½®ï¼›ç°æœ‰å…ˆéªŒä¿®æ­£å¸¸ä¾èµ–ç±»è®¡æ•°æˆ–æ‰‹å·¥è¶…å‚ï¼Œé€‚åº”æ€§ä¸å¯ç”¨æ€§å—é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®¾è®¡Neural Prior Estimatorï¼ˆNPEï¼‰ï¼Œé€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ªPrior Estimation Moduleä»æ½œç‰¹å¾ä¸­é¢„æµ‹ç‰¹å¾æ¡ä»¶çš„log-priorï¼Œå¹¶ä¸ä¸»å¹²ç½‘ç»œç”¨å•å‘logisticæŸå¤±è”åˆè®­ç»ƒï¼›å°†å­¦åˆ°çš„å…ˆéªŒä¼°è®¡æ³¨å…¥logit adjustmentå½¢æˆNPE-LAè¿›è¡Œåç½®æ„ŸçŸ¥é¢„æµ‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šç†è®ºä¸Šåœ¨Neural Collapseæ¡ä»¶ä¸‹ï¼ŒNPEå¯æ¢å¤ç±»åˆ«log-priorï¼ˆå·®ä¸€ä¸ªåŠ æ€§å¸¸æ•°ï¼‰ï¼Œæä¾›æœ‰ä¾æ®çš„è‡ªé€‚åº”å…ˆéªŒä¿¡å·ï¼›åœ¨é•¿å°¾CIFARä¸ä¸å‡è¡¡è¯­ä¹‰åˆ†å‰²ï¼ˆSTAREã€ADE20Kï¼‰ä¸Šç¨³å®šæå‡ï¼Œå°¤å…¶æ”¹å–„å°‘æ•°ç±»æ€§èƒ½ï¼Œä¸”æ–¹æ³•è½»é‡æ— éœ€æ˜¾å¼ç±»é¢‘æˆ–åˆ†å¸ƒç‰¹å®šè¶…å‚ã€‚

**å…³é”®è¯**ï¼šç±»åˆ«ä¸å¹³è¡¡, ç±»åˆ«å…ˆéªŒä¼°è®¡, ç‰¹å¾æ¡ä»¶å…ˆéªŒ, æ½œåœ¨è¡¨å¾, é•¿å°¾è¯†åˆ«, åç½®æ ¡æ­£é¢„æµ‹, è¯­ä¹‰åˆ†å‰²ä¸å¹³è¡¡, è”åˆè®­ç»ƒæ¨¡å—

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17853v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17853v1.pdf)

---

## [14. Quad Length Codes for Lossless Compression of e4m3](https://arxiv.org/abs/2602.17849v2)

**ä½œè€…**ï¼šAditya Agrawal, Albert Magyar, Hiteshwar Eswaraiah ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.IT  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Quad Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. The coding scheme uses 3 prefix bits to divide the 256 symbols into 8 areas. Each area has a different code length and encodes a different number of symbols. The scheme uses a Look Up Table with 256 entries, significantly simplifying the hardware implementation compared to Huffman trees. The coding scheme can be adapted for different distributions. For the e4m3 data type, the scheme achieves a compressibility of 13.9% in comparison to 15.9% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºQuad Length Codesï¼Œç”¨å›ºå®šå‰ç¼€åˆ†åŒº+æŸ¥è¡¨çš„æ–¹å¼å¯¹e4m3è¿›è¡Œæ— æŸå‹ç¼©ï¼Œåœ¨æ¥è¿‘Huffmanå‹ç¼©ç‡çš„åŒæ—¶æ˜¾è‘—æå‡è§£ç é€Ÿåº¦å¹¶é™ä½ç¡¬ä»¶å¤æ‚åº¦ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šLLMè®­ç»ƒ/æ¨ç†ä¸­çš„é›†ä½“é€šä¿¡å—ç½‘ç»œå¸¦å®½é™åˆ¶ï¼Œéœ€è¦æ— æŸå‹ç¼©ç¼“è§£ï¼›ä½†Huffmanè§£ç éœ€æ·±æ ‘éå†ã€ä½ä¸²è¡Œä¸”ç¡¬ä»¶å¤æ‚ï¼Œé€šç”¨ç è™½å¿«å´éš¾ä»¥åˆ©ç”¨çœŸå®ç¬¦å·åˆ†å¸ƒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç”¨3ä¸ªå‰ç¼€ä½å°†256ä¸ªç¬¦å·åˆ’åˆ†ä¸º8ä¸ªåŒºåŸŸï¼Œæ¯åŒºé‡‡ç”¨ä¸åŒç é•¿å¹¶è¦†ç›–ä¸åŒæ•°é‡ç¬¦å·ï¼Œä»¥é€‚é…åˆ†å¸ƒï¼›è§£ç é€šè¿‡256é¡¹LUTç›´æ¥æ˜ å°„ï¼Œé¿å…Huffmanæ ‘éå†å¹¶ä¾¿äºç¡¬ä»¶å®ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨e4m3æ•°æ®ä¸Šï¼Œè¯¥æ–¹æ³•å‹ç¼©ç‡çº¦13.9%ï¼ˆHuffmançº¦15.9%ï¼‰ï¼Œè™½ç•¥é€Šä½†æ˜¾è‘—åŠ é€Ÿè§£ç å¹¶ç®€åŒ–ç¡¬ä»¶å®ç°ï¼Œä¸”å¯é’ˆå¯¹ä¸åŒåˆ†å¸ƒè¿›è¡Œè°ƒæ•´ã€‚

**å…³é”®è¯**ï¼šæ— æŸå‹ç¼©, å››é•¿åº¦ç¼–ç , è§£ç é€Ÿåº¦, ç¡¬ä»¶å¤æ‚æ€§, ç¬¦å·é¢‘ç‡åˆ†å¸ƒ, æŸ¥æ‰¾è¡¨, e4m3æ•°æ®ç±»å‹, Quad

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17849v2) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17849v2.pdf)

---

## [15. Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846v1)

**ä½œè€…**ï¼šNick Dodson, Xinyu Gao, Qingsong Wang ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-19

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ä¸ªå‡ ä½•æ¡†æ¶æŠŠæ‰©æ•£æ¨¡å‹çš„å™ªå£°æ—¥ç¨‹åˆ†ä¸ºâ€œå°/ä¸­/å¤§å™ªå£°â€ä¸‰æ®µï¼ŒæŒ‡å‡ºè®°å¿†åŒ–é£é™©ä¸»è¦é›†ä¸­åœ¨ä¸­ç­‰å™ªå£°æ®µï¼Œå¹¶ç»™å‡ºé’ˆå¯¹æ€§çš„å‡ ä½•å¹²é¢„æ¥ç¼“è§£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ‰©æ•£æ¨¡å‹å¯èƒ½è®°å¿†è®­ç»ƒæ ·æœ¬å¸¦æ¥éšç§é£é™©ï¼Œä½†ç°æœ‰ç†è§£ä¸æ¸…æ¥šâ€œè®°å¿†åŒ–åœ¨å™ªå£°æ—¥ç¨‹çš„å“ªä¸ªé˜¶æ®µå‘ç”Ÿâ€ã€æ•°æ®å‡ ä½•å¦‚ä½•å½±å“ä»¥åŠä¸åŒå™ªå£°å°ºåº¦ç°è±¡å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåŸºäºä¸¤ç±»å…³é”®å‡ ä½•å¯¹è±¡â€”â€”è®­ç»ƒæ•°æ®è¢«é«˜æ–¯å£³è¦†ç›–çš„æ€§è´¨ä¸åéªŒåˆ†å¸ƒçš„é›†ä¸­è¡Œä¸ºâ€”â€”å°†å™ªå£°æ—¥ç¨‹åˆ’åˆ†ä¸ºä¸‰ä¸ªæœºåˆ¶ä¸åŒçš„åŒºé—´ï¼Œå¹¶æ®æ­¤åˆ†æå„åŒºé—´çš„è®°å¿†åŒ–å€¾å‘ï¼›åœ¨ä¸­å™ªå£°åŒºæå‡ºæ»¡è¶³ç‰¹å®šå‡ ä½•æ¡ä»¶çš„â€œå®šå‘å¹²é¢„â€ä»¥æŠ‘åˆ¶è®°å¿†åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šè®°å¿†åŒ–é£é™©åœ¨å™ªå£°æ°´å¹³ä¸Šé«˜åº¦ä¸å‡åŒ€ï¼šä¸­ç­‰å™ªå£°å­˜åœ¨æœ€å±é™©çš„â€œè®°å¿†åŒ–åŒºâ€ï¼›å°å™ªå£°å› è®­ç»ƒè¦†ç›–ä¸è¶³è€Œä¸æ˜“è®°å¿†ï¼Œå¤§å™ªå£°å› åéªŒä¸é›†ä¸­ä¸”å»å™ªè¿‘ä¼¼çº¿æ€§é«˜æ–¯è€Œå¤©ç„¶æŠ—è®°å¿†ï¼Œå¹¶å¯é€šè¿‡å‡ ä½•å¼•å¯¼çš„å¹²é¢„é™ä½ä¸­å™ªå£°æ®µçš„è®°å¿†é£é™©ã€‚

**å…³é”®è¯**ï¼šDiffusion, è®­ç»ƒæ•°æ®è®°å¿†åŒ–, éšç§æ³„éœ²é£é™©, å™ªå£°è°ƒåº¦, ä¸­ç­‰å™ªå£°å±é™©åŒº, æ•°æ®å‡ ä½•, é«˜æ–¯å£³è¦†ç›–, åéªŒåˆ†å¸ƒé›†ä¸­, çº¿æ€§é«˜æ–¯å»å™ª, å®šå‘å¹²é¢„ç¼“è§£

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.17846v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.17846v1.pdf)

---

