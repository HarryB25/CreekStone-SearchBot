# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-12

> å…± 30 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (8 ç¯‡)
- [cs.LG](#csLG) (14 ç¯‡)
- [cs.AI](#csAI) (1 ç¯‡)
- [cs.CL](#csCL) (7 ç¯‡)

---

## cs.AI

## [1. FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136v1)

**ä½œè€…**ï¼šJiayi Zhou, Yang Sheng, Hantao Lou ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFormalJudge æå‡ºä¸€ç§ç»“åˆå¤§æ¨¡å‹ä¸å½¢å¼éªŒè¯çš„ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œç”¨å½¢å¼åŒ–çº¦æŸå’Œå®šç†è¯æ˜æ›¿ä»£ä¼ ç»Ÿâ€œLLMå½“è£åˆ¤â€çš„æ¦‚ç‡å¼ç›‘ç£ï¼Œä»¥æ›´å¯é åœ°è¯„ä¼°ä¸ä¿éšœæ™ºèƒ½ä½“è¡Œä¸ºå®‰å…¨ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ LLM-as-a-Judge ç›‘ç£èŒƒå¼æœ¬èº«æ˜¯æ¦‚ç‡ç³»ç»Ÿï¼Œå®¹æ˜“ç»§æ‰¿è¢«ç›‘ç£æ™ºèƒ½ä½“çš„é”™è¯¯å’Œåå·®ï¼Œéš¾ä»¥åœ¨é«˜é£é™©åœºæ™¯ä¸­æä¾›å¯éªŒè¯çš„å®‰å…¨ä¿è¯ï¼Œå› æ­¤éœ€è¦å°†è‡ªç„¶è¯­è¨€å®‰å…¨éœ€æ±‚è½¬åŒ–ä¸ºå¯å½¢å¼éªŒè¯çš„è§„æ ¼å¹¶è¿›è¡Œä¸¥æ ¼è¯æ˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º Formal-of-Thought åŒå‘æ¶æ„ï¼šå…ˆç”± LLM è‡ªç„¶è¯­è¨€â†’å½¢å¼è§„æ ¼ï¼Œå°†é«˜å±‚äººç±»æ„å›¾è‡ªé¡¶å‘ä¸‹åˆ†è§£ä¸ºå¯éªŒè¯çš„åŸå­çº¦æŸï¼Œå†å€ŸåŠ© Dafny è§„æ ¼ä¸ Z3 SMT æ±‚è§£å™¨è‡ªåº•å‘ä¸Šè¯æ˜æ™ºèƒ½ä½“è¾“å‡ºæ˜¯å¦æ»¡è¶³è¿™äº›çº¦æŸï¼Œä»è€Œè¾“å‡ºæ•°å­¦çº§åˆ«çš„åˆè§„ç»“è®ºè€Œéæ¦‚ç‡æ‰“åˆ†ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨è¡Œä¸ºå®‰å…¨ã€å¤šé¢†åŸŸçº¦æŸéµå®ˆå’Œæ™ºèƒ½ä½“æ¬ºéª—æ£€æµ‹ä¸‰ç±»åŸºå‡†ä¸Šï¼ŒFormalJudge ç›¸æ¯” LLM-as-a-Judge å¹³å‡æå‡çº¦ 16.6%ï¼›7B è§„æ¨¡è£åˆ¤æ¨¡å‹å³å¯é«˜å‡†ç¡®ç‡è¯†åˆ« 72B æ¨¡å‹çš„æ¬ºéª—è¡Œä¸ºï¼Œå¹¶ä¸”é€šè¿‡è¿­ä»£çº¦æŸä¸éªŒè¯æµç¨‹å¯è¿‘çº¿æ€§æå‡æ•´ä½“å®‰å…¨æ€§ï¼Œå±•ç¤ºäº†ç¥ç»ç¬¦å·ç›‘ç£åœ¨æ™ºèƒ½ä½“ç›‘ç®¡ä¸­çš„ä¼˜åŠ¿ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, ç¥ç»ç¬¦å·æ¨ç†, agenticç›‘ç£, è¡Œä¸ºå®‰å…¨çº¦æŸ, å½¢å¼åŒ–éªŒè¯, Dafnyè§„èŒƒ, Z3å®šç†è¯æ˜, æ¬ºéª—æ£€æµ‹, çº¦æŸéµå¾ª, å¼±å¼ºæ³›åŒ–

**è¯„åˆ†**ï¼š69

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11136v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11136v1.pdf)

---

## cs.CL

## [2. TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection](https://arxiv.org/abs/2602.11106v1)

**ä½œè€…**ï¼šGÃ©raud Faye, Wassila Ouerdane, Guillaume Gadek ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured information in the form of a graph and encoding both the text and the graph for classification purposes. Through extensive experiments, we demonstrate that this hybrid representation enhances misinformation detection performance compared to using language models alone. Furthermore, we introduce TEGRA, an extension of our framework that integrates domain-specific knowledge, further enhancing classification accuracy in most cases.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šTEGRAé€šè¿‡å°†æ–‡æœ¬è½¬ä¸ºå›¾ç»“æ„å¹¶ç»“åˆå¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢ï¼Œå®ç°æ¯”å•çº¯è¯­è¨€æ¨¡å‹æ›´å¥½çš„è™šå‡ä¿¡æ¯æ£€æµ‹æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è™šå‡ä¿¡æ¯æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬è¡¨é¢ç‰¹å¾æˆ–è¯­è¨€æ¨¡å‹ï¼Œç¼ºä¹åƒäººå·¥äº‹å®æ ¸æŸ¥é‚£æ ·åˆ©ç”¨ç»“æ„åŒ–ä¸å¤–éƒ¨çŸ¥è¯†çš„èƒ½åŠ›ï¼Œéš¾ä»¥åœ¨å¤æ‚åœºæ™¯ä¸­ä¿æŒé«˜å‡†ç¡®ç‡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºTEGæ¡†æ¶ï¼Œå°†æ–‡æ¡£æŠ½å–ä¸ºç»“æ„åŒ–å›¾ï¼Œå¹¶è”åˆç¼–ç æ–‡æœ¬ä¸å›¾è¿›è¡Œåˆ†ç±»ï¼›åœ¨æ­¤åŸºç¡€ä¸Šæ‰©å±•ä¸ºTEGRAï¼Œé€šè¿‡æ£€ç´¢å¹¶æ³¨å…¥é¢†åŸŸçŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œå›¾ç»“æ„å¢å¼ºçš„æ–‡æœ¬è¡¨ç¤ºåœ¨è™šå‡ä¿¡æ¯æ£€æµ‹ä¸Šä¼˜äºå•ä¸€è¯­è¨€æ¨¡å‹ï¼Œè€ŒåŠ å…¥é¢†åŸŸçŸ¥è¯†æ£€ç´¢çš„TEGRAåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šèƒ½è¿›ä¸€æ­¥æå‡åˆ†ç±»å‡†ç¡®ç‡ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, æ–‡æœ¬ç¼–ç , å›¾ç¥ç»ç½‘ç»œ, çŸ¥è¯†æ£€ç´¢, æ£€ç´¢å¢å¼º, è™šå‡ä¿¡æ¯æ£€æµ‹, çŸ¥è¯†å›¾è°±, è¡¨ç¤ºå­¦ä¹ , retrieval

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11106v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11106v1.pdf)

---

## [3. Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096v1)

**ä½œè€…**ï¼šSoumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix ("Wait, think safely") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºSafeThinkï¼Œä¸€ç§åœ¨æ¨ç†æ—©æœŸæ³¨å…¥çŸ­â€œå®‰å…¨çº åå‰ç¼€â€çš„è½»é‡é˜²å¾¡æ–¹æ³•ï¼Œå¯å¤§å¹…é™ä½å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„è¶Šç‹±æˆåŠŸç‡ä¸”å‡ ä¹ä¸æŸå¤±æ¨ç†èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰é€šè¿‡RLå¼ºåŒ–é“¾å¼æ€ç»´çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹è™½ç„¶æ¨ç†æ›´å¼ºï¼Œä½†å®‰å…¨å¯¹é½è¢«å‰Šå¼±ã€è¶Šç‹±ç‡å‡é«˜ï¼Œç¼ºä¹ä¸€ç§æ—¢èƒ½æ¢å¤å®‰å…¨åˆä¸ç‰ºç‰²æ¨ç†æ€§èƒ½çš„ä½æˆæœ¬æ¨ç†æ—¶é˜²å¾¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šSafeThinkåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”¨å®‰å…¨å¥–åŠ±æ¨¡å‹å®æ—¶ç›‘æ§ä¸­é—´æ¨ç†è½¨è¿¹ï¼Œä¸€æ—¦å®‰å…¨å¾—åˆ†ä½äºé˜ˆå€¼ï¼Œå°±åœ¨å‰1â€“3æ­¥æ³¨å…¥ç»è¿‡ä¼˜åŒ–çš„çŸ­çº åå‰ç¼€ï¼ˆå¦‚â€œç­‰ç­‰ï¼Œå®‰å…¨åœ°æ€è€ƒâ€ï¼‰æ¥é‡å®šå‘åç»­ç”Ÿæˆï¼Œè€Œä¸æ˜¯å…¨ç¨‹å¼ºè¡Œæœ€å¤§åŒ–å®‰å…¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨6ä¸ªå¼€æºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ä¸4ä¸ªè¶Šç‹±åŸºå‡†ä¸Šï¼ŒSafeThinkå°†æ”»å‡»æˆåŠŸç‡é™ä½30â€“60%ç”šè‡³ä»çº¦60%é™è‡³çº¦5%ï¼ŒåŒæ—¶å‡ ä¹ä¿æŒåŸæœ‰æ¨ç†ç²¾åº¦ï¼Œè¡¨æ˜â€œå®‰å…¨æ¢å¤â€é€šå¸¸åªéœ€åœ¨æœ€åˆå°‘æ•°æ¨ç†æ­¥ä¸­è¿›è¡Œè½»å¾®å¹²é¢„å³å¯ã€‚

**å…³é”®è¯**ï¼šå¤§æ¨¡å‹æ¨ç†, å¤šæ¨¡æ€å¤§æ¨¡å‹, å®‰å…¨å¯¹é½, å¼ºåŒ–å­¦ä¹ åè®­ç»ƒ, å®‰å…¨å¥–åŠ±æ¨¡å‹, æ€ç»´é“¾å¹²é¢„, å¯¹æŠ—è¶Šç‹±é˜²å¾¡, æ¨ç†å®‰å…¨æ§åˆ¶, å‰ç¼€å¼•å¯¼ç­–ç•¥, ml

**è¯„åˆ†**ï¼š38

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11096v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11096v1.pdf)

---

## [4. Can Large Language Models Make Everyone Happy?](https://arxiv.org/abs/2602.11091v1)

**ä½œè€…**ï¼šUsman Naseem, Gautam Siddharth Kashyap, Ebad Shabbir ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate these dimensions in isolation and therefore provide limited insight into their interactions and trade-offs. More recent efforts, including MIB and INTERPRETABILITY BENCHMARK-based on mechanistic interpretability, offer valuable perspectives on model failures; however, they remain insufficient for systematically characterizing cross-dimensional trade-offs. To address these gaps, we introduce MisAlign-Profile, a unified benchmark for measuring misalignment trade-offs inspired by mechanistic profiling. First, we construct MISALIGNTRADE, an English misaligned-aligned dataset across 112 normative domains taxonomies, including 14 safety, 56 value, and 42 cultural domains. In addition to domain labels, each prompt is classified with one of three orthogonal semantic types-object, attribute, or relations misalignment-using Gemma-2-9B-it and expanded via Qwen3-30B-A3B-Instruct-2507 with SimHash-based fingerprinting to avoid deduplication. Each prompt is paired with misaligned and aligned responses through two-stage rejection sampling to ensure quality. Second, we benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE-revealing 12%-34% misalignment trade-offs across dimensions.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºç»Ÿä¸€åŸºå‡† MisAlign-Profile å’Œæ•°æ®é›† MISALIGNTRADEï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¤§æ¨¡å‹åœ¨å®‰å…¨ã€ä»·å€¼è§‚ä¸æ–‡åŒ–ç»´åº¦ä¹‹é—´çš„é”™é…åŠæƒè¡¡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å®‰å…¨ã€ä»·å€¼ã€æ–‡åŒ–ç±»åŸºå‡†å¤§å¤šåªå•ç‹¬è¯„ä¼°æŸä¸€ç»´åº¦ï¼Œæ— æ³•åˆ»ç”»çœŸå®åº”ç”¨ä¸­å¤šç»´è§„èŒƒåŒæ—¶å­˜åœ¨æ—¶çš„å†²çªä¸æƒè¡¡ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªèƒ½ç»Ÿä¸€åº¦é‡è·¨ç»´åº¦é”™é…çš„ç³»ç»ŸåŒ–æ¡†æ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æ„å»º MISALIGNTRADE æ•°æ®é›†ï¼Œè¦†ç›–112ä¸ªè§„èŒƒé¢†åŸŸï¼ˆ14å®‰å…¨ã€56ä»·å€¼ã€42æ–‡åŒ–ï¼‰ï¼Œå¹¶æŒ‰è¯­ä¹‰ç±»å‹ï¼ˆå®¢ä½“/å±æ€§/å…³ç³»é”™é…ï¼‰æ ‡æ³¨ï¼›åˆ©ç”¨å¤§å‹æ¨¡å‹ç”Ÿæˆå¹¶é€šè¿‡ä¸¤é˜¶æ®µæ‹’ç»é‡‡æ ·è·å¾—æˆå¯¹â€œé”™é…/å¯¹é½â€å›ç­”ï¼Œç„¶ååœ¨æ­¤åŸºå‡†ä¸Šå¯¹é€šç”¨ã€å¾®è°ƒåŠå¼€æºLLMè¿›è¡Œç³»ç»Ÿè¯„æµ‹å½¢æˆ MisAlign-Profileã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜å½“å‰å„ç±»LLMåœ¨å®‰å…¨ã€ä»·å€¼ä¸æ–‡åŒ–ç»´åº¦é—´å­˜åœ¨æ˜¾è‘—çš„12%â€“34%é”™é…æƒè¡¡ï¼Œè¯´æ˜å•ä¸€ç»´åº¦å¯¹é½å¹¶ä¸è¶³å¤Ÿï¼ŒMisAlign-Profile èƒ½æ›´å…¨é¢æš´éœ²å’Œé‡åŒ–è¿™äº›è·¨ç»´åº¦å¯¹é½é—®é¢˜ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, å¯¹é½è¯„ä¼°, ä»·å€¼è§‚å¯¹é½, å®‰å…¨æ€§å¯¹é½, æ–‡åŒ–åå¥½å»ºæ¨¡, mechanistic interpretability, åŸºå‡†æ•°æ®é›†æ„å»º, å¤šç»´åº¦æƒè¡¡åˆ†æ, èŒƒå¼è¿ç§»åœºæ™¯, äººæœºä»·å€¼å†²çª, llm

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11091v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11091v1.pdf)

---

## [5. DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning](https://arxiv.org/abs/2602.11089v1)

**ä½œè€…**ï¼šYicheng Chen, Zerun Ma, Xinchen Xie ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šDataChef æå‡ºç”¨å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨ç”Ÿæˆç«¯åˆ°ç«¯æ•°æ®é…æ–¹ï¼Œä½¿å°å‹åŸºç¡€ LLM é€šè¿‡ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°æ¥è¿‘äººå·¥ç­–åˆ’æ–¹æ¡ˆçš„æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰ LLM çš„è®­ç»ƒæ•ˆæœé«˜åº¦ä¾èµ–æ•°æ®é…æ–¹è®¾è®¡ï¼Œä½†æ•´ä½“æ•°æ®æµç¨‹ä»ä¸»è¦é ä¸“å®¶æ‰‹å·¥åå¤è¯•é”™ï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ï¼Œéœ€è¦ä¸€ç§èƒ½è‡ªåŠ¨å‘ç°é«˜è´¨é‡æ•°æ®é…æ–¹çš„æœºåˆ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†â€œæ•°æ®é…æ–¹ç”Ÿæˆâ€å»ºæ¨¡ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œè®­ç»ƒ DataChef-32B åœ¨ç»™å®šåŸºåº§æ¨¡å‹ã€æ•°æ®æºæ± å’Œç›®æ ‡åŸºå‡†çš„æ¡ä»¶ä¸‹è¾“å‡ºå®Œæ•´æ•°æ®æµæ°´çº¿ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªé¢„æµ‹ä¸‹æ¸¸è¡¨ç°çš„ä»£ç†å¥–åŠ±è¿›è¡Œåœ¨çº¿ RL ä¼˜åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å…­ä¸ªæœªè§è¿‡çš„ä»»åŠ¡ä¸Šï¼ŒDataChef-32B è‡ªåŠ¨ç”Ÿæˆçš„é…æ–¹èƒ½è¾¾åˆ°ä¸äººç±»ä¸“å®¶ç›¸å½“çš„ä¸‹æ¸¸æ€§èƒ½ï¼ŒåŒ…æ‹¬å°† Qwen3-1.7B-Base é€‚é…åˆ°æ•°å­¦é¢†åŸŸå¹¶åœ¨ AIME'25 ä¸Šå–å¾— 66.7 åˆ†ï¼Œæ˜¾ç¤ºå‡ºè‡ªåŠ¨åŒ– LLM è®­ç»ƒå’Œè‡ªè¿›åŒ– AI ç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚

**å…³é”®è¯**ï¼šå¤§æ¨¡å‹, LLM, å¼ºåŒ–å­¦ä¹ , åœ¨çº¿å­¦ä¹ , å¥–åŠ±æ¨¡å‹, æ•°æ®é…æ–¹ç”Ÿæˆ, è®­ç»ƒæ•°æ®è‡ªåŠ¨åŒ–, ä»»åŠ¡è‡ªé€‚åº”, äººæœºååŒè¿­ä»£, æ•°å­¦é¢†åŸŸè¿ç§»

**è¯„åˆ†**ï¼š67

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11089v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11089v1.pdf)

---

## [6. SteuerLLM: Local specialized large language model for German tax law analysis](https://arxiv.org/abs/2602.11081v1)

**ä½œè€…**ï¼šSebastian Wind, Jeta Sopa, Laurin Schmid ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these challenges, as correct answers require exact statutory citation, structured legal argumentation, and numerical accuracy under rigid grading schemes. We algorithmically generate SteuerEx, the first open benchmark derived from authentic German university tax law examinations. SteuerEx comprises 115 expert-validated examination questions spanning six core tax law domains and multiple academic levels, and employs a statement-level, partial-credit evaluation framework that closely mirrors real examination practice. We further present SteuerLLM, a domain-adapted LLM for German tax law trained on a large-scale synthetic dataset generated from authentic examination material using a controlled retrieval-augmented pipeline. SteuerLLM (28B parameters) consistently outperforms general-purpose instruction-tuned models of comparable size and, in several cases, substantially larger systems, demonstrating that domain-specific data and architectural adaptation are more decisive than parameter scale for performance on realistic legal reasoning tasks. All benchmark data, training datasets, model weights, and evaluation code are released openly to support reproducible research in domain-specific legal artificial intelligence. A web-based demo of SteuerLLM is available at https://steuerllm.i5.ai.fau.de.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æ„å»ºäº†å¾·è¯­ç¨æ³•è€ƒè¯•åŸºå‡†SteuerExï¼Œå¹¶åŸºäºåˆæˆæ•°æ®è®­ç»ƒäº†ä¸“ç”¨ç¨æ³•å¤§æ¨¡å‹SteuerLLMï¼Œåœ¨çœŸå®æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºé€šç”¨LLMã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šé€šç”¨å¤§æ¨¡å‹åœ¨å—ä¸¥æ ¼å½¢å¼è§„åˆ™ã€ç²¾ç¡®æœ¯è¯­å’Œæ³•å¾‹ç»“æ„çº¦æŸçš„ç¨æ³•é¢†åŸŸè¡¨ç°æ¬ ä½³ï¼Œè€Œç°å®è€ƒè¯•éœ€è¦ç²¾ç¡®æ³•æ¡å¼•ç”¨ã€ç»“æ„åŒ–è®ºè¯å’Œæ•°å€¼å‡†ç¡®æ€§ï¼Œå› æ­¤éœ€è¦ä¸“é—¨çš„è¯„æµ‹åŸºå‡†å’Œé¢†åŸŸæ¨¡å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ä»çœŸå®å¾·å›½è¯­å¤§ç¨æ³•è€ƒè¯•ä¸­ç®—æ³•åŒ–ç”Ÿæˆå¹¶äººå·¥æ ¡éªŒSteuerExåŸºå‡†ï¼ˆ115é“è·¨6å¤§ç¨æ³•é¢†åŸŸçš„é¢˜ç›®ï¼‰ï¼Œè®¾è®¡æ¥è¿‘çœŸå®é˜…å·çš„åˆ†å¥éƒ¨åˆ†æ‰“åˆ†ä½“ç³»ï¼Œå¹¶åˆ©ç”¨çœŸå®è€ƒè¯•ææ–™+å—æ§RAGæµæ°´çº¿å¤§è§„æ¨¡ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ï¼Œè®­ç»ƒäº†28Bå‚æ•°çš„ä¸“ç”¨SteuerLLMã€‚

**ä¸»è¦ç»“è®º**ï¼šSteuerLLMåœ¨SteuerExä¸Šç¨³å®šä¼˜äºåŒè§„æ¨¡ç”šè‡³æ›´å¤§è§„æ¨¡çš„é€šç”¨æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œè¡¨æ˜åœ¨å¤æ‚æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸­ï¼Œé¢†åŸŸç‰¹å®šæ•°æ®ä¸æ¶æ„é€‚é…æ¯”å•çº¯å¢å¤§å‚æ•°è§„æ¨¡æ›´å…³é”®ï¼›è®ºæ–‡åŒæ—¶å¼€æºåŸºå‡†ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹æƒé‡å’Œè¯„æµ‹ä»£ç ï¼Œä¿ƒè¿›ç¨æ³•ä¸æ³•å¾‹AIçš„å¯å¤ç°ç ”ç©¶ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, LLM, æ£€ç´¢å¢å¼ºç”Ÿæˆ, é¢†åŸŸè‡ªé€‚åº”, æ³•å¾‹æ¨ç†, å¾·è¯­ç¨æ³•åˆ†æ, å¼€æ”¾åŸºå‡†æ•°æ®é›†, æ¨¡å‹è¯„æµ‹æ¡†æ¶, åˆæˆè®­ç»ƒæ•°æ®é›†, ç»“æ„åŒ–è®ºè¯ç”Ÿæˆ

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11081v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11081v1.pdf)

---

## [7. Simultaneous Speech-to-Speech Translation Without Aligned Data](https://arxiv.org/abs/2602.11072v1)

**ä½œè€…**ï¼šTom Labiausse, Romain Fabre, Yannick EstÃ¨ve ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.SD, eess.AS  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºæ— éœ€è¯çº§å¯¹é½æ•°æ®çš„åŒå£°ä¼ è¯‘è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹ Hibiki-Zeroï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨ä¿è¯è´¨é‡çš„åŒæ—¶ä¼˜åŒ–å»¶è¿Ÿï¼Œå¹¶åœ¨å¤šè¯­ç§åˆ°è‹±è¯­ä»»åŠ¡ä¸Šå–å¾—SOTAè¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŒå£°ä¼ è¯‘æ–¹æ³•ä¸¥é‡ä¾èµ–éš¾ä»¥å¤§è§„æ¨¡è·å–ä¸”éœ€è¯­è¨€ç‰¹å®šå¯å‘å¼çš„è¯çº§å¯¹é½æ•°æ®ï¼Œé™åˆ¶äº†å¯¹è¯­åºå·®å¼‚è¾ƒå¤§çš„å¤šè¯­è¨€æ‰©å±•ä¸ç³»ç»Ÿå¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆåˆ©ç”¨ä»…å¥çº§å¯¹é½çš„è¯­éŸ³-æ–‡æœ¬æ•°æ®è®­ç»ƒé«˜å»¶è¿Ÿçš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘èƒ½åŠ›ï¼Œå†é€šè¿‡åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç›´æ¥ä»¥å»¶è¿Ÿå’Œç¿»è¯‘è´¨é‡ä¸ºå¥–åŠ±è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œä»è€Œå»é™¤å¯¹è¯çº§å¯¹é½å’Œäººå·¥å¯å‘å¼çš„ä¾èµ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šHibiki-Zeroåœ¨äº”ä¸ªå¤šè¯­è¨€åˆ°è‹±è¯­çš„åŒå£°ä¼ è¯‘ä»»åŠ¡ä¸Šåœ¨ç¿»è¯‘å‡†ç¡®ç‡ã€å»¶è¿Ÿã€å£°éŸ³ä¿çœŸå’Œè‡ªç„¶åº¦æ–¹é¢å‡è¾¾åˆ°æˆ–è¶…è¶Šç°æœ‰ç³»ç»Ÿï¼Œå¹¶èƒ½åœ¨ä¸åˆ°1000å°æ—¶æ–°è¯­ç§è¯­éŸ³æ•°æ®ä¸‹å¿«é€Ÿé€‚é…ï¼ŒåŒæ—¶å¼€æ”¾æ¨¡å‹æƒé‡ã€æ¨ç†ä»£ç åŠ45å°æ—¶å¤šè¯­ç§è¯„æµ‹åŸºå‡†ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, å¼ºåŒ–å­¦ä¹ , GRPO, è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘, åŒå£°ä¼ è¯‘, å®æ—¶ç¿»è¯‘, è¯­éŸ³è¡¨ç¤ºå­¦ä¹ , å¤šè¯­è¨€å»ºæ¨¡, å»¶è¿Ÿä¼˜åŒ–, ml

**è¯„åˆ†**ï¼š42

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11072v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11072v1.pdf)

---

## [8. Conversational Behavior Modeling Foundation Model With Multi-Level Perception](https://arxiv.org/abs/2602.11065v1)

**ä½œè€…**ï¼šDingkun Zhou, Shuchang Pan, Jiachen Lian ç­‰ 14 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as multi-level perception, and then reasons over conversational behaviors via a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a high quality corpus that pairs controllable, event-rich dialogue data with human-annotated labels. The GoT framework structures streaming predictions as an evolving graph, enabling a transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºå¤šå±‚æ„ŸçŸ¥ä¸Graph-of-Thoughtsçš„å¯¹è¯è¡Œä¸ºå»ºæ¨¡åŸºç¡€æ¡†æ¶ï¼Œå¯åœ¨å…¨åŒå·¥å¯¹è¯ä¸­é¢„æµ‹è¯´è¯è¡Œä¸ºå¹¶ç»™å‡ºå¯è§£é‡Šçš„æ¨ç†é“¾ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¯¹è¯ç³»ç»Ÿéš¾ä»¥æ˜¾å¼å»ºæ¨¡äººç±»ä»æ„å›¾åˆ°å…·ä½“è¯´è¯è¡Œä¸ºçš„éšæ€§æ€ç»´é“¾ï¼Œå°¤å…¶åœ¨å…¨åŒå·¥åœºæ™¯ä¸‹éš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„æ—¶é—´ä¸å› æœç»“æ„ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æ„å»ºäº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨åˆ†å±‚æ ‡æ³¨å½¢å¼åŒ–â€œæ„å›¾â†’è¯´è¯è¡Œä¸ºâ€çš„è·¯å¾„ï¼Œå¹¶é€šè¿‡Graph-of-ThoughtsæŠŠæŒç»­çš„é¢„æµ‹ç»„ç»‡æˆåŠ¨æ€æ¼”åŒ–çš„å›¾ç»“æ„ï¼Œä½¿Transformeræ—¢èƒ½é¢„æµ‹ä¸‹ä¸€æ­¥è¯´è¯è¡Œä¸ºï¼Œåˆèƒ½ç”Ÿæˆç®€è¦å†³ç­–ç†ç”±å¹¶åœ¨çº¿ä¿®æ­£æ¨ç†ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨åˆæˆä¸çœŸå®å…¨åŒå·¥å¯¹è¯å®éªŒä¸­ï¼Œè¯¥æ¡†æ¶åœ¨è¡Œä¸ºæ£€æµ‹ä¸Šè¡¨ç°ç¨³å¥ï¼Œèƒ½äº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†é“¾ï¼Œå¹¶ä¸ºå…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿä¸­çš„å¯¹è¯æ¨ç†å»ºç«‹äº†ä¸€ä¸ªåŸºç¡€æ€§è¯„æµ‹ä¸å»ºæ¨¡æ–¹æ¡ˆã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, transformer, å¯¹è¯è¡Œä¸ºå»ºæ¨¡, å¤šå±‚æ¬¡æ„ŸçŸ¥, Graph-of-Thoughts, æ„å›¾è¯†åˆ«, è¯­éŸ³å¯¹è¯ç³»ç»Ÿ, å…¨åŒå·¥äº¤äº’, å› æœæ¨ç†

**è¯„åˆ†**ï¼š47

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11065v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11065v1.pdf)

---

## cs.CV

## [9. SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos](https://arxiv.org/abs/2602.11154v1)

**ä½œè€…**ï¼šYue Gao, Hong-Xing Yu, Sanghyeon Chang ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSurfPhaseæå‡ºä¸€ç§ç»“åˆåŠ¨æ€Gaussian surfelsä¸SDFå¹¶é…åˆè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œä»ä»…ä¸¤è·¯ç¨€ç–ç›¸æœºè§†é¢‘ä¸­é‡å»ºä¸¤ç›¸æµä¸­çš„ä¸‰ç»´ç•Œé¢åŠ¨åŠ›å­¦ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¸¤ç›¸æµç•Œé¢ä¸»å¯¼åŠ¨é‡ã€ä¼ çƒ­å’Œä¼ è´¨è¿‡ç¨‹ï¼Œä½†å®éªŒæµ‹é‡å›°éš¾ï¼šä¼ ç»Ÿæ–¹æ³•åœ¨è¿åŠ¨ç•Œé¢é™„è¿‘å—é™ï¼Œè€Œç°æœ‰ç¥ç»æ¸²æŸ“å¤šé’ˆå¯¹å•ç›¸æµã€è¾¹ç•Œæ¨¡ç³Šï¼Œéš¾ä»¥å¤„ç†å°–é”ä¸”å¯å˜å½¢çš„æ¶²-æ±½ç•Œé¢ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ–¹æ³•å°†åŠ¨æ€Gaussian surfelsä¸æœ‰ç¬¦å·è·ç¦»å‡½æ•°ç›¸ç»“åˆä»¥ä¿è¯å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œæ–°è§†è§’è§†é¢‘åˆæˆï¼Œä»ç¨€ç–è§†è§’è¡¥å……è§‚æµ‹ã€è¿­ä»£æå‡ç•Œé¢é‡å»ºä¸è¿åŠ¨ä¼°è®¡è´¨é‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨é«˜é€Ÿæ± æ²¸è…¾æ–°æ•°æ®é›†ä¸Šï¼ŒSurfPhaseä»…ä¾èµ–ä¸¤è·¯ç›¸æœºè§†è§’å³å¯å®ç°é«˜è´¨é‡çš„æ–°è§†ç‚¹åˆæˆå’Œç•Œé¢é€Ÿåº¦ä¼°è®¡ï¼Œè¯æ˜å…¶åœ¨å¤æ‚ä¸¤ç›¸æµä¸‰ç»´ç•Œé¢é‡å»ºä¸Šçš„æœ‰æ•ˆæ€§ä¸å®ç”¨æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, è§†é¢‘æ‰©æ•£æ¨¡å‹, ä¸‰ç»´é‡å»º, é«˜æ–¯surfels, æœ‰ç¬¦å·è·ç¦»å‡½æ•°, ç¨€ç–è§†è§’é‡å»º, ä¸¤ç›¸æµåŠ¨ç•Œé¢åŠ¨åŠ›å­¦, æ–°è§†è§’åˆæˆ, é€Ÿåº¦åœºä¼°è®¡, diffusion

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11154v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11154v1.pdf)

---

## [10. Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling](https://arxiv.org/abs/2602.11146v1)

**ä½œè€…**ï¼šGongye Liu, Bo Yang, Yida Zhi ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºé¢å‘æ‰©æ•£æ¨¡å‹çš„åŸç”Ÿæ½œç©ºé—´å¥–åŠ±æ¨¡å‹DiNa-LRMï¼Œåœ¨å¸¦å™ªå£°çš„æ½œå˜é‡ä¸Šç›´æ¥åšåå¥½å­¦ä¹ ï¼Œæ€§èƒ½æ¥è¿‘ç”šè‡³ä¼˜äºVLMå¥–åŠ±ä¸”è®¡ç®—æˆæœ¬æ˜¾è‘—æ›´ä½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¯¹æ‰©æ•£/æµåŒ¹é…æ¨¡å‹çš„åå¥½ä¼˜åŒ–å¤šä¾èµ–VLMå¥–åŠ±ï¼Œæ—¢è®¡ç®—/æ˜¾å­˜å¼€é”€å¤§ï¼Œåˆå­˜åœ¨åƒç´ ç©ºé—´å¥–åŠ±ä¸æ½œç©ºé—´ç”Ÿæˆä¹‹é—´çš„åŸŸä¸åŒ¹é…ï¼Œå½±å“å¯¹é½æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šDiNa-LRMåœ¨æ‰©æ•£è¿‡ç¨‹çš„å¸¦å™ªæ½œçŠ¶æ€ä¸Šå®šä¹‰å¥–åŠ±ï¼Œå¼•å…¥éšå™ªå£°æ°´å¹³å˜åŒ–çš„ä¸ç¡®å®šæ€§çš„å™ªå£°æ ¡å‡†Thurstoneä¼¼ç„¶ï¼Œå¹¶åœ¨é¢„è®­ç»ƒæ½œæ‰©æ•£éª¨å¹²ä¸Šæ·»åŠ æ—¶é—´æ­¥æ¡ä»¶çš„å¥–åŠ±å¤´ï¼Œç»“åˆæ¨ç†æ—¶å¤šå™ªå£°é›†æˆå®ç°æ‰©æ•£åŸç”Ÿçš„é²æ£’å¥–åŠ±ä¸å¯æ‰©å±•æ¨ç†ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å¤šç§å›¾åƒå¯¹é½åŸºå‡†ä¸Šï¼ŒDiNa-LRMæ˜¾è‘—ä¼˜äºç°æœ‰æ‰©æ•£ç±»å¥–åŠ±åŸºçº¿ï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶è¾¾åˆ°æ¥è¿‘ç”šè‡³åª²ç¾SOTA VLMå¥–åŠ±çš„æ•ˆæœï¼Œå¹¶åœ¨åå¥½ä¼˜åŒ–ä¸­å¸¦æ¥æ›´å¿«ã€æ›´é«˜æ•ˆçš„å¯¹é½åŠ¨åŠ›å­¦ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, ç”Ÿæˆå¼æ¨¡å‹, å¥–åŠ±æ¨¡å‹, åå¥½ä¼˜åŒ–, latentè¡¨ç¤º, å™ªå£°å»ºæ¨¡, å›¾åƒå¯¹é½, æ¨ç†åŠ é€Ÿ, å¤šæ¨¡æ€å¯¹é½, diffusion

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11146v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11146v1.pdf)

---

## [11. PhyCritic: Multimodal Critic Models for Physical AI](https://arxiv.org/abs/2602.11124v1)

**ä½œè€…**ï¼šTianyi Xiong, Shihao Wang, Guilin Liu ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPhyCritic æå‡ºé¢å‘ç‰©ç†åœºæ™¯çš„å¤šæ¨¡æ€è£åˆ¤æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–åå¥½å­¦ä¹ æ˜¾è‘—æå‡å¯¹ç‰©ç†ä»»åŠ¡å›ç­”çš„è¯„åˆ¤ä¸æŒ‡å¯¼èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ¨¡æ€è£åˆ¤æ¨¡å‹ä¸»è¦åŸºäºé€šç”¨è§†è§‰ä»»åŠ¡è®­ç»ƒï¼Œç¼ºä¹å¯¹æ¶‰åŠç‰©ç†æ„ŸçŸ¥ã€å› æœæ¨ç†å’Œè§„åˆ’ç­‰â€œç‰©ç†æ™ºèƒ½â€ä»»åŠ¡çš„å¯é è¯„ä¼°ä¸å¯¹é½èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé‡‡ç”¨ä¸¤é˜¶æ®µ RLVR æµç¨‹ï¼šå…ˆè¿›è¡Œâ€œç‰©ç†æŠ€èƒ½é¢„çƒ­â€ä»¥å¼ºåŒ–ç‰©ç†ç›¸å…³çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå†è¿›è¡Œè‡ªæŒ‡å¼è£åˆ¤å¾®è°ƒï¼Œè®©æ¨¡å‹å…ˆç”Ÿæˆè‡ªèº«å‚è€ƒç­”æ¡ˆå†å¯¹å€™é€‰å›ç­”æ‰“åˆ†ï¼Œä»è€Œæå‡è¯„åˆ¤ç¨³å®šæ€§å’Œç‰©ç†æ­£ç¡®æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šPhyCritic åœ¨å¤šä¸ªç‰©ç†å’Œé€šç”¨å¤šæ¨¡æ€è¯„æµ‹ä¸­å‡ä¼˜äºå¼€æ”¾æºåŸºçº¿ï¼Œå¹¶åœ¨ä½œä¸ºç­–ç•¥æ¨¡å‹ä½¿ç”¨æ—¶å¯è¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨ç‰©ç†ç¯å¢ƒä¸­çš„æ„ŸçŸ¥ä¸æ¨ç†è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹, ç‰©ç†AI, æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, å¥–åŠ±æ¨¡å‹, åå¥½å¯¹é½, è‡ªç›‘ç£è¯„ä¼°, å› æœæ¨ç†, è§„åˆ’æ¨ç†, ç‰©ç†åœºæ™¯ç†è§£, æ¨¡å‹ç”Ÿæˆå“åº”è¯„ä¼°

**è¯„åˆ†**ï¼š54

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11124v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11124v1.pdf)

---

## [12. HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion](https://arxiv.org/abs/2602.11117v1)

**ä½œè€…**ï¼šDi Chang, Ji Hou, Aljaz Bozic ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šHairWeaveræå‡ºä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç®¡çº¿ï¼Œèƒ½ä»å•å¼ äººåƒç”Ÿæˆå…·æœ‰çœŸå®ç»†è…»å¤´å‘åŠ¨æ€çš„åŠ¨ç”»è§†é¢‘ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰äººåƒåŠ¨ç”»æ–¹æ³•è™½èƒ½æ§åˆ¶èº«ä½“å§¿æ€ï¼Œå´ç¼ºä¹å¯¹å¤´å‘çš„ç²¾ç»†æ§åˆ¶ï¼Œå¯¼è‡´å¤´å‘è¿åŠ¨åƒµç¡¬ã€ä¸çœŸå®ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªæ—¢èƒ½ç²¾ç¡®æ§åˆ¶å¤´å‘è¿åŠ¨åˆä¿æŒäººç‰©ç…§ç‰‡çœŸå®æ„Ÿçš„ç”Ÿæˆæ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨è§†é¢‘æ‰©æ•£éª¨å¹²ç½‘ç»œä¸Šå¼•å…¥ä¸¤ä¸ªè½»é‡åŒ–LoRAæ¨¡å—ï¼šMotion-Context-LoRAç”¨äºèåˆä»CGä»¿çœŸå¾—åˆ°çš„å¤´å‘è¿åŠ¨æ¡ä»¶ï¼ŒSim2Real-Domain-LoRAç”¨äºåœ¨è™šæ‹Ÿåˆ°çœŸå®æ•°æ®åŸŸè¿ç§»æ—¶ä¿æŒäººç‰©çš„ç…§ç‰‡çº§å¤–è§‚ï¼Œå¹¶åˆ©ç”¨å¤§è§„æ¨¡CGåŠ¨æ€äººç±»æ•°æ®è¿›è¡Œè®­ç»ƒä»¥å­¦ä¼šç»†è‡´çš„å¤´å‘å“åº”ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒHairWeaveråœ¨å¤´å‘è¿åŠ¨çš„çœŸå®æ„Ÿå’Œç»†èŠ‚åŠ¨æ€ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆåœ¨è¿åŠ¨ä¸­è‡ªç„¶å“åº”çš„é«˜ä¿çœŸå¤´å‘åŠ¨ç”»ï¼Œåˆ·æ–°äº†äººåƒå¤´å‘åŠ¨ç”»çš„æ•ˆæœæ°´å¹³ã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, è§†é¢‘æ‰©æ•£, ç”Ÿæˆå¼, LoRAå¾®è°ƒ, å¤´å‘è¿åŠ¨åˆæˆ, Sim2Real, äººç‰©å›¾åƒåŠ¨ç”»åŒ–, ä¸‰ç»´å¤´å‘åŠ¨åŠ›å­¦å»ºæ¨¡, generative

**è¯„åˆ†**ï¼š33

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11117v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11117v1.pdf)

---

## [13. FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference](https://arxiv.org/abs/2602.11105v1)

**ä½œè€…**ï¼šDivya Jyoti Bajpai, Dhruv Bhardwaj, Soumya Roy ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFastFlow æå‡ºä¸€ç§æ— éœ€é‡è®­ç»ƒçš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ™ºèƒ½è·³è¿‡å†—ä½™å»å™ªæ­¥æ¥æ˜¾è‘—åŠ é€Ÿ Flow Matching ç”Ÿæˆæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒé«˜ç”Ÿæˆè´¨é‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŠ é€Ÿæ–¹æ³•ï¼ˆè’¸é¦ã€è½¨è¿¹æˆªæ–­ã€ä¸€è‡´æ€§æ¨¡å‹ï¼‰éœ€è¦é¢å¤–è®­ç»ƒä¸”ç¼ºä¹è·¨ä»»åŠ¡æ³›åŒ–ï¼Œæ— æ³•è§£å†³ Flow Matching æ¨¡å‹å› é¡ºåºå»å™ªå¯¼è‡´æ¨ç†ç¼“æ…¢çš„é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šFastFlow åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ£€æµ‹é‚£äº›å¯¹å»å™ªè·¯å¾„å½±å“å¾ˆå°çš„æ­¥éª¤ï¼Œç”¨å‰å‡ æ­¥çš„æœ‰é™å·®åˆ†é€Ÿåº¦ä¼°è®¡å¤–æ¨æœªæ¥çŠ¶æ€ä»¥è¿‘ä¼¼æ›´æ–°ï¼Œå¹¶å°†â€œè·³è¿‡å¤šå°‘æ­¥å†è°ƒç”¨ä¸€æ¬¡å®Œæ•´æ¨¡å‹â€å»ºæ¨¡ä¸ºå¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œåœ¨çº¿å­¦ä¹ æœ€ä¼˜è·³æ­¥ç­–ç•¥ï¼Œåœ¨å›¾åƒã€è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘æµæ°´çº¿ä¸­å³æ’å³ç”¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ FastFlow åœ¨å¤šç§ä»»åŠ¡ä¸Šå¯å®ç°è¶…è¿‡ 2.6 å€çš„åŠ é€Ÿï¼Œå¹¶åœ¨æ— éœ€é‡è®­ç»ƒçš„å‰æä¸‹ä¿æŒé«˜ä¿çœŸè¾“å‡ºï¼Œå±•ç¤ºäº†åŸºäº bandit çš„è‡ªé€‚åº”æ¨ç†ç­–ç•¥åœ¨ Flow Matching æ¨¡å‹åŠ é€Ÿä¸­çš„æœ‰æ•ˆæ€§ä¸é€šç”¨æ€§ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆå¼, æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, æ‰©æ•£æ¨¡å‹, flow matching, å¤šè‡‚è€è™æœº, è‡ªé€‚åº”æ¨ç†, å›¾åƒç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ, å›¾åƒç¼–è¾‘, æ¨ç†åŠ é€Ÿ, ml

**è¯„åˆ†**ï¼š32

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11105v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11105v1.pdf)

---

## [14. First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges](https://arxiv.org/abs/2602.11086v1)

**ä½œè€…**ï¼šRobyn Larracy, Eve MacDonald, Angkoon Phinyomark ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Biometric footstep recognition, based on a person's unique pressure patterns under their feet during walking, is an emerging field with growing applications in security and safety. However, progress in this area has been limited by the lack of large, diverse datasets necessary to address critical challenges such as generalization to new users and robustness to shifts in factors like footwear or walking speed. The recent release of the UNB StepUP-P150 dataset, the largest and most comprehensive collection of high-resolution footstep pressure recordings to date, opens new opportunities for addressing these challenges through deep learning. To mark this milestone, the First International StepUP Competition for Biometric Footstep Recognition was launched. Competitors were tasked with developing robust recognition models using the StepUP-P150 dataset that were then evaluated on a separate, dedicated test set designed to assess verification performance under challenging variations, given limited and relatively homogeneous reference data. The competition attracted global participation, with 23 registered teams from academia and industry. The top-performing team, Saeid_UCC, achieved the best equal error rate (EER) of 10.77% using a generative reward machine (GRM) optimization strategy. Overall, the competition showcased strong solutions, but persistent challenges in generalizing to unfamiliar footwear highlight a critical area for future work.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡å›´ç»•UNB StepUP-P150å¤§è§„æ¨¡è¶³è¿¹å‹åŠ›æ•°æ®é›†ä¸¾åŠé¦–å±ŠStepUPç«èµ›ï¼Œç³»ç»Ÿè¯„ä¼°å„å›¢é˜Ÿåœ¨å¤æ‚æ¡ä»¶ä¸‹çš„è¶³è¿¹ç”Ÿç‰©è¯†åˆ«æ€§èƒ½å¹¶æ­ç¤ºå…³é”®æŒ‘æˆ˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè¶³è¿¹ç”Ÿç‰©è¯†åˆ«è™½å…·å®‰å…¨ä¸å®‰é˜²åº”ç”¨æ½œåŠ›ï¼Œä½†é•¿æœŸå—é™äºæ•°æ®é›†è§„æ¨¡ä¸å¤šæ ·æ€§ï¼Œå°¤å…¶éš¾ä»¥è§£å†³å¯¹æ–°ç”¨æˆ·æ³›åŒ–å’Œå¯¹é‹ç±»ã€æ­¥é€Ÿå˜åŒ–çš„é²æ£’æ€§é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåŸºäºStepUP-P150æ„å»ºè®­ç»ƒä¸ç‹¬ç«‹æµ‹è¯•é›†ï¼Œç»„ç»‡23æ”¯å›¢é˜Ÿå¼€å‘å’Œæäº¤è¯†åˆ«æ¨¡å‹ï¼Œåœ¨åŒ…å«é‹ç±»ä¸æ­¥æ€å˜åŒ–çš„å¤æ‚åœºæ™¯ä¸‹è¯„ä¼°éªŒè¯æ€§èƒ½ï¼Œå…¶ä¸­æœ€ä½³å›¢é˜Ÿé‡‡ç”¨ç”Ÿæˆå¼å¥–åŠ±æœºå™¨ï¼ˆGRMï¼‰ä¼˜åŒ–ç­–ç•¥å–å¾—æœ€ä½EERã€‚

**ä¸»è¦ç»“è®º**ï¼šç«èµ›å±•ç¤ºäº†å½“å‰è¶³è¿¹è¯†åˆ«åœ¨åŒç±»æ¡ä»¶ä¸‹å·²èƒ½å–å¾—è¾ƒå¥½æ€§èƒ½ï¼ˆæœ€ä½³EERä¸º10.77%ï¼‰ï¼Œä½†åœ¨é™Œç”Ÿé‹ç±»ç­‰è·¨æ¡ä»¶æ³›åŒ–æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾æ€§èƒ½ç“¶é¢ˆï¼Œæ˜¯ä»Šåæ–¹æ³•è®¾è®¡å’Œæ•°æ®é›†å»ºè®¾çš„æ ¸å¿ƒæ–¹å‘ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , æœºå™¨å­¦ä¹ , ç¥ç»ç½‘ç»œ, ç”Ÿæˆå¼æ¨¡å‹, ç”Ÿç‰©ç‰¹å¾è¯†åˆ«, æ­¥æ€è¯†åˆ«, è¶³å‹ä¼ æ„Ÿæ•°æ®, å®‰å…¨éªŒè¯ç³»ç»Ÿ, é²æ£’æ€§ä¼˜åŒ–, å¯¹æŠ—æ ·æœ¬é˜²å¾¡, deep learning

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11086v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11086v1.pdf)

---

## [15. Chatting with Images for Introspective Visual Thinking](https://arxiv.org/abs/2602.11073v1)

**ä½œè€…**ï¼šJunfei Wu, Jian Guan, Qiang Liu ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºâ€œä¸å›¾å¯¹è¯â€çš„è§†è§‰æ€è€ƒæ¡†æ¶ï¼Œé€šè¿‡è¯­è¨€å¼•å¯¼çš„ç‰¹å¾è°ƒåˆ¶å®ç°å¤šæ¬¡ã€äº¤äº’å¼å›¾åƒç¼–ç ï¼Œä»è€Œæ˜¾è‘—æå‡å¤æ‚å¤šå›¾å’Œè§†é¢‘åœºæ™¯ä¸‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸åªåšä¸€æ¬¡æ€§è§†è§‰ç¼–ç å¹¶åœ¨æ–‡æœ¬åŸŸæ¨ç†ï¼Œå¯¼è‡´ç»†ç²’åº¦è§†è§‰ä¿¡æ¯ä¸¢å¤±ä¸”éš¾ä»¥åœ¨å¤šåŒºåŸŸ/å¤šå›¾åƒé—´è¿›è¡Œç²¾ç¡®è¯­ä¹‰å’Œå‡ ä½•å…³ç³»æ¨ç†ï¼›ç°æœ‰â€œthinking with imagesâ€æ–¹å¼åˆç¼ºä¹ä¸è¯­è¨€è¯­ä¹‰çš„ç´§å¯†å¯¹é½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…æå‡ºâ€œchatting with imagesâ€æ¡†æ¶ï¼Œå°†è§†è§‰æ“ä½œå½¢å¼åŒ–ä¸ºè¯­è¨€å¼•å¯¼çš„ç‰¹å¾è°ƒåˆ¶ï¼šåœ¨è¯­è¨€æç¤ºæŒ‡å¯¼ä¸‹å¯¹å¤šä¸ªå›¾åƒåŒºåŸŸè¿›è¡ŒåŠ¨æ€è”åˆé‡ç¼–ç ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†å¸¦åŠ¨æ€è§†è§‰ç¼–ç å™¨çš„ViLaVTæ¨¡å‹ï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒ+å¼ºåŒ–å­¦ä¹ ä¸¤é˜¶æ®µè®­ç»ƒä»¥å¡‘é€ æœ‰æ•ˆçš„äº¤äº’å¼è§†è§‰æ¨ç†è¡Œä¸ºã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å…«ä¸ªåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViLaVTåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šå–å¾—ç¨³å®šæå‡ï¼Œå°¤å…¶åœ¨å¤šå›¾åƒå’Œè§†é¢‘åœºæ™¯ä¸‹å¤æ‚ç©ºé—´æ¨ç†æ–¹é¢å¢å¹…æ˜¾è‘—ï¼ŒéªŒè¯äº†è¯­è¨€å¼•å¯¼çš„åŠ¨æ€è§†è§‰é‡ç¼–ç å¯¹è·¨æ¨¡æ€å¯¹é½ä¸ç²¾ç»†è§†è§‰æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹, å¤šæ¨¡æ€æ¨ç†, è§†è§‰ç‰¹å¾è°ƒåˆ¶, è¯­è¨€å¼•å¯¼é‡ç¼–ç , è·¨å›¾åƒç©ºé—´å…³ç³»ç†è§£, äº¤äº’å¼è§†è§‰æ€ç»´, å¼ºåŒ–å­¦ä¹ è®­ç»ƒ, agent

**è¯„åˆ†**ï¼š41

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11073v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11073v1.pdf)

---

## [16. PuriLight: A Lightweight Shuffle and Purification Framework for Monocular Depth Estimation](https://arxiv.org/abs/2602.11066v1)

**ä½œè€…**ï¼šYujie Chen, Li Zhang, Xiaomeng Chu ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We propose PuriLight, a lightweight and efficient framework for self-supervised monocular depth estimation, to address the dual challenges of computational efficiency and detail preservation. While recent advances in self-supervised depth estimation have reduced reliance on ground truth supervision, existing approaches remain constrained by either bulky architectures compromising practicality or lightweight models sacrificing structural precision. These dual limitations underscore the critical need to develop lightweight yet structurally precise architectures. Our framework addresses these limitations through a three-stage architecture incorporating three novel modules: the Shuffle-Dilation Convolution (SDC) module for local feature extraction, the Rotation-Adaptive Kernel Attention (RAKA) module for hierarchical feature enhancement, and the Deep Frequency Signal Purification (DFSP) module for global feature purification. Through effective collaboration, these modules enable PuriLight to achieve both lightweight and accurate feature extraction and processing. Extensive experiments demonstrate that PuriLight achieves state-of-the-art performance with minimal training parameters while maintaining exceptional computational efficiency. Codes will be available at https://github.com/ishrouder/PuriLight.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPuriLightæå‡ºä¸€ä¸ªç”±Shuffle-Dilationå·ç§¯ã€æ—‹è½¬è‡ªé€‚åº”æ³¨æ„åŠ›å’Œé¢‘åŸŸå‡€åŒ–ç»„æˆçš„è½»é‡çº§è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½å‚æ•°é‡ä¸è®¡ç®—æˆæœ¬ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡è¦ä¹ˆä¾èµ–åºå¤§ç½‘ç»œã€éš¾ä»¥éƒ¨ç½²ï¼Œè¦ä¹ˆåœ¨è½»é‡åŒ–æ—¶ä¸¢å¤±ç»“æ„ç»†èŠ‚ï¼Œå› æ­¤éœ€è¦ä¸€ç§åŒæ—¶å…¼é¡¾è½»é‡ä¸ç»“æ„ç²¾åº¦çš„æ–°æ¶æ„ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µç»“æ„ï¼šé€šè¿‡Shuffle-Dilation Convolutionè¿›è¡Œå±€éƒ¨ç‰¹å¾æå–ï¼Œåˆ©ç”¨Rotation-Adaptive Kernel Attentionè¿›è¡Œåˆ†å±‚/æ–¹å‘æ„ŸçŸ¥ç‰¹å¾å¢å¼ºï¼Œæœ€åç”¨Deep Frequency Signal Purificationåœ¨é¢‘åŸŸå¯¹å…¨å±€ç‰¹å¾è¿›è¡Œå‡€åŒ–ï¼Œä»¥æå‡æ·±åº¦å›¾çš„ç»†èŠ‚ä¸ä¸€è‡´æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¯æ˜PuriLightåœ¨è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šä»¥æå°‘çš„è®­ç»ƒå‚æ•°å’Œè¾ƒä½è®¡ç®—å¼€é”€è¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå±•ç°å‡ºè‰¯å¥½çš„éƒ¨ç½²å‹å¥½æ€§ä¸ç²¾åº¦æƒè¡¡ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, è‡ªç›‘ç£å­¦ä¹ , å•ç›®æ·±åº¦ä¼°è®¡, è½»é‡åŒ–æ¨¡å‹, ç‰¹å¾æå–, æ³¨æ„åŠ›æœºåˆ¶, é¢‘åŸŸä¿¡å·å¤„ç†, å·ç§¯ç½‘ç»œ, è®¡ç®—æ•ˆç‡ä¼˜åŒ–, agent

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11066v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11066v1.pdf)

---

## cs.LG

## [17. Diffusion-Pretrained Dense and Contextual Embeddings](https://arxiv.org/abs/2602.11151v1)

**ä½œè€…**ï¼šSedigheh Eslami, Maksim Gaiduk, Markus Krimmel ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.CL, cs.IR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºåŸºäºæ‰©æ•£é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€æ£€ç´¢å‘é‡æ¨¡å‹pplx-embedï¼Œé€šè¿‡å¤šé˜¶æ®µå¯¹æ¯”å­¦ä¹ åœ¨å¤šé¡¹å…¬å¼€ä¸çœŸå®å¤§è§„æ¨¡æ£€ç´¢ä»»åŠ¡ä¸Šå–å¾—å¼ºæ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ£€ç´¢å‘é‡æ¨¡å‹åœ¨é•¿æ–‡æ¡£å…¨å±€è¯­ä¹‰å»ºæ¨¡å’Œå¤šè¯­è¨€/å¤§è§„æ¨¡çœŸå®æœç´¢åœºæ™¯ä¸­çš„é²æ£’æ€§ä¸è¶³ï¼Œéœ€è¦æ›´å¥½åˆ©ç”¨åŒå‘ä¸Šä¸‹æ–‡å¹¶å…¼é¡¾æ•ˆæœä¸æ•ˆç‡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥æ‰©æ•£é¢„è®­ç»ƒçš„åŒå‘è¯­è¨€æ¨¡å‹ä¸ºéª¨å¹²ï¼Œé€šè¿‡å¤šé˜¶æ®µå¯¹æ¯”å­¦ä¹ è®­ç»ƒä¸¤ç±»æ¨¡å‹ï¼šæ ‡å‡†æ£€ç´¢å‘é‡æ¨¡å‹pplx-embed-v1ï¼Œä»¥åŠæ˜¾å¼æ³¨å…¥å…¨å±€æ–‡æ¡£ä¸Šä¸‹æ–‡çš„contextç‰ˆæœ¬pplx-embed-context-v1ï¼Œå¹¶ç»“åˆå‡å€¼æ± åŒ–ä¸late chunkingç­–ç•¥ä»¥æ›´å¥½ä¿ç•™é•¿æ–‡æ¡£å…¨å±€è¯­ä¹‰ã€‚

**ä¸»è¦ç»“è®º**ï¼špplx-embed-v1åœ¨å¤šè¯­è¨€MTEBã€MIRACLç­‰å¤šé¡¹æ£€ç´¢åŸºå‡†ä¸Šè¡¨ç°æœ‰ç«äº‰åŠ›ï¼Œpplx-embed-context-v1åœ¨ConTEBä¸Šåˆ·æ–°çºªå½•ï¼ŒåŒæ—¶åœ¨å†…éƒ¨æ•°åƒä¸‡çº§æ–‡æ¡£æ£€ç´¢è¯„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜è¯¥æ‰©æ•£é¢„è®­ç»ƒ+å¯¹æ¯”å­¦ä¹ æ–¹æ¡ˆåœ¨å¤§è§„æ¨¡ç”Ÿäº§æ£€ç´¢åœºæ™¯ä¸­æœ‰æ•ˆä¸”é«˜æ•ˆã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, è¯­ä¹‰æœç´¢, æ£€ç´¢å¢å¼ºç”ŸæˆRAG, æ–‡æœ¬å‘é‡è¡¨ç¤º, å¤šè¯­è¨€åµŒå…¥, å¯¹æ¯”å­¦ä¹ , æ‰©æ•£é¢„è®­ç»ƒ, åŒå‘æ³¨æ„åŠ›, é•¿æ–‡æ¡£æ£€ç´¢, ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤º

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11151v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11151v1.pdf)

---

## [18. GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144v1)

**ä½œè€…**ï¼šRuichuan An, Sihan Yang, Ziyu Guo ç­‰ 11 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šGENIUSæå‡ºä¸€ä¸ªä¸“é—¨è¯„ä¼°ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨¡å‹â€œç”Ÿæˆæµä½“æ™ºåŠ›â€ï¼ˆGFIï¼‰çš„åŸºå‡†å¥—ä»¶ï¼Œå¹¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨éœ€å³åœºæ¨ç†ä¸é€‚åº”çš„æ–°ä»»åŠ¡ä¸Šå­˜åœ¨æ˜æ˜¾çŸ­æ¿ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è§†è§‰ä¸å¤šæ¨¡æ€åŸºå‡†å¤§å¤šæµ‹çš„æ˜¯â€œæ™¶ä½“æ™ºåŠ›â€ï¼ˆä¾èµ–è®°å¿†ä¸æ—¢æœ‰çŸ¥è¯†ï¼‰ï¼Œéš¾ä»¥è¡¡é‡æ¨¡å‹åœ¨é™Œç”Ÿæƒ…å¢ƒä¸­å½’çº³æ¨¡å¼ã€æ»¡è¶³ä¸´æ—¶çº¦æŸå’Œåˆ©ç”¨ä¸Šä¸‹æ–‡è¿›è¡Œå³æ—¶æ¨ç†çš„èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªç³»ç»ŸåŒ–è¯„ä¼°GFIçš„æ–°åŸºå‡†ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å°†GFIå½¢å¼åŒ–ä¸ºä¸‰ç±»åŸå§‹èƒ½åŠ›ï¼šéšå¼æ¨¡å¼å½’çº³ã€ä¸´æ—¶çº¦æŸæ‰§è¡Œä¸æƒ…å¢ƒçŸ¥è¯†é€‚åº”ï¼Œæ®æ­¤æ„é€ GENIUSä»»åŠ¡é›†ï¼Œå¯¹12ä¸ªä»£è¡¨æ€§ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹è¿›è¡Œç³»ç»Ÿæµ‹è¯•ï¼Œå¹¶é€šè¿‡è¯Šæ–­åˆ†æåŒºåˆ†â€œç”Ÿæˆèƒ½åŠ›ä¸è¶³â€ä¸â€œä¸Šä¸‹æ–‡ç†è§£ä¸è¶³â€çš„ä¸åŒå¤±è´¥ç±»å‹ï¼ŒåŒæ—¶æå‡ºä¸€ç§å…è®­ç»ƒçš„æ³¨æ„åŠ›å¹²é¢„ç­–ç•¥ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒæ˜¾ç¤ºå½“å‰ä¸»æµæ¨¡å‹åœ¨GENIUSä¸Šçš„è¡¨ç°æ˜¾è‘—ä¸è¶³ï¼Œä¸»è¦é—®é¢˜æ¥æºäºä¸Šä¸‹æ–‡ç†è§£å’Œçº¦æŸéµå®ˆè€Œéç”Ÿæˆè´¨é‡æœ¬èº«ï¼›GENIUSä¸ºç”Ÿæˆæµä½“æ™ºåŠ›æä¾›äº†æ›´ä¸¥æ ¼å’Œç»†ç²’åº¦çš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶è¡¨æ˜é€šè¿‡ç®€å•çš„æ³¨æ„åŠ›å¹²é¢„å³å¯åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¿™äº›ç¼ºé™·ï¼Œä¸ºåç»­æ¨¡å‹è®¾è®¡ä¸è®­ç»ƒæŒ‡æ˜äº†æ”¹è¿›æ–¹å‘ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , å¤šæ¨¡æ€æ¨¡å‹, ç”Ÿæˆå¼æ™ºèƒ½, ç”Ÿæˆå¼æ¨ç†, ä¸Šä¸‹æ–‡ç†è§£, æ³¨æ„åŠ›æœºåˆ¶å¹²é¢„, è§†è§‰ç”Ÿæˆè¯„æµ‹åŸºå‡†, å›¾åƒåå¥½å»ºæ¨¡, æŠ½è±¡éšå–»å¯è§†åŒ–, åç›´è§‰ç‰©ç†æ¨¡æ‹Ÿ, generative

**è¯„åˆ†**ï¼š37

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11144v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11144v1.pdf)

---

## [19. TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139v1)

**ä½œè€…**ï¼šJingang Qu, David HolzmÃ¼ller, GaÃ«l Varoquaux ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šTabICLv2 æ˜¯ä¸€ä¸ªé¢å‘è¡¨æ ¼æ•°æ®çš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡åˆæˆæ•°æ®é¢„è®­ç»ƒã€æ¶æ„æ”¹è¿›å’Œä¼˜åŒ–è®­ç»ƒç­–ç•¥ï¼Œåœ¨æ— éœ€è°ƒå‚çš„æƒ…å†µä¸‹è¶…è¶Šç°æœ‰æœ€å¼ºæ¨¡å‹å¹¶å…·å¤‡æ›´é«˜æ•ˆç‡ä¸å¯æ‰©å±•æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŸºäºè¡¨æ ¼çš„åŸºç¡€æ¨¡å‹è™½å·²è¶…è¿‡ä¼ ç»Ÿæ¢¯åº¦æå‡æ ‘ï¼Œä½†åœ¨é¢„è®­ç»ƒæ•°æ®å¤šæ ·æ€§ã€å¯¹å¤§è§„æ¨¡æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€è®­ç»ƒä¸æ¨ç†æ•ˆç‡ä»¥åŠå¼€æ”¾æ€§æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º TabICLv2ï¼Œä»ä¸‰æ–¹é¢æ”¹è¿›ï¼šæ„å»ºé«˜å¤šæ ·æ€§åˆæˆæ•°æ®ç”Ÿæˆå¼•æ“è¿›è¡Œé¢„è®­ç»ƒï¼›åœ¨æ¶æ„ä¸Šå¼•å…¥å¯æ‰©å±• softmax ç­‰æ³¨æ„åŠ›æ”¹è¿›ä»¥æ”¯æŒæ›´é•¿åºåˆ—ä¸å¤§æ•°æ®é›†ï¼›åœ¨è®­ç»ƒä¸Šé‡‡ç”¨ Muon ç­‰ä¼˜åŒ–ç­–ç•¥æ›¿ä»£ AdamW å¹¶ç³»ç»Ÿä¼˜åŒ–é¢„è®­ç»ƒåè®®ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ TabArena å’Œ TALENT åŸºå‡†ä¸Šï¼ŒTabICLv2 åœ¨é›¶è°ƒå‚è®¾ç½®ä¸‹å³è¶…è¿‡ç»è¿‡è°ƒå‚ã€é›†æˆå¹¶åœ¨çœŸå®æ•°æ®ä¸Šå¾®è°ƒçš„ RealTabPFN-2.5ï¼Œåœ¨é€‚ä¸­ç®—åŠ›ä¸‹å³å¯æ¨å¹¿åˆ°ç™¾ä¸‡çº§æ•°æ®é›†ä¸”æ˜¾è‘—æ›´å¿«ï¼ŒåŒæ—¶ä»¥å¼€æºå½¢å¼æä¾›æ¨ç†ä»£ç ä¸æ¨¡å‹æƒé‡å¹¶é…å¥—è¯¦å°½æ¶ˆèå®éªŒã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, åŸºç¡€æ¨¡å‹, è¡¨æ ¼æ•°æ®å»ºæ¨¡, ä¸Šä¸‹æ–‡å­¦ä¹ , æ³¨æ„åŠ›æœºåˆ¶, åˆæˆæ•°æ®ç”Ÿæˆ, è‡ªç›‘ç£é¢„è®­ç»ƒ, ä¼˜åŒ–å™¨Muon, å›å½’ä¸åˆ†ç±»é¢„æµ‹, context

**è¯„åˆ†**ï¼š45

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11139v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11139v1.pdf)

---

## [20. Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137v1)

**ä½œè€…**ï¼šTessa Han, Sebastian Bordt, Hanlin Zhang ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, cs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæ–‡ç« è¡¨æ˜ï¼šåœ¨å¤§æ¨¡å‹é¢„è®­ç»ƒä¸­ä½¿ç”¨æ›´å¤§çš„weight decayä¼šé™ä½åŸºæ¨¡è¡¨é¢æŒ‡æ ‡ï¼Œä½†æ˜¾è‘—æå‡å…¶åœ¨ä¸‹æ¸¸å¾®è°ƒæ—¶çš„å¯å¡‘æ€§ä¸æ”¶ç›Šã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰é¢„è®­ç»ƒè¶…å‚å’Œç¼©æ”¾å¾‹ç ”ç©¶å‡ ä¹åªçœ‹åŸºåº§æ¨¡å‹çš„éªŒè¯æŸå¤±ï¼Œå¿½è§†äº†æ¨¡å‹åœ¨åç»­å¾®è°ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€‚åº”èƒ½åŠ›ï¼Œæœ¬å·¥ä½œæƒ³ä»â€œå¯å¡‘æ€§â€è§’åº¦é‡æ–°å®¡è§†é¢„è®­ç»ƒè¶…å‚ï¼Œå°¤å…¶æ˜¯weight decayã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ç³»ç»Ÿåœ°åœ¨é¢„è®­ç»ƒé˜¶æ®µæ‰«ä¸åŒçš„weight decayè®¾ç½®ï¼Œæ¯”è¾ƒè¿™äº›åŸºæ¨¡åœ¨é¢„è®­ç»ƒç»“æŸæ—¶çš„è¡¨ç°ä¸åç»­åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒåçš„æ€§èƒ½ï¼Œå¹¶ç»“åˆè¡¨ç¤ºçº¿æ€§å¯åˆ†æ€§ã€æ³¨æ„åŠ›çŸ©é˜µç»“æ„ä¸è¿‡æ‹Ÿåˆç¨‹åº¦ç­‰æŒ‡æ ‡åšæœºåˆ¶åˆ†æã€‚

**ä¸»è¦ç»“è®º**ï¼šè¾ƒå¤§çš„weight decayè™½ä¼šç•¥æŸåŸºæ¨¡çš„é¢„è®­ç»ƒæ€§èƒ½ï¼Œä½†èƒ½æå‡è¡¨ç¤ºçš„çº¿æ€§å¯åˆ†æ€§ã€æ­£åˆ™åŒ–æ³¨æ„åŠ›ç»“æ„ã€å‡å°‘è¿‡æ‹Ÿåˆï¼Œä»è€Œæ˜¾è‘—å¢å¼ºä¸‹æ¸¸å¾®è°ƒæ”¶ç›Šä¸æ¨¡å‹å¯å¡‘æ€§ï¼Œè¯´æ˜è¶…å‚ä¼˜åŒ–ä¸åº”åªç›¯äº¤å‰ç†µæŸå¤±ï¼Œè€Œè¦çº³å…¥ä¸‹æ¸¸é€‚åº”æ€§ç­‰æ›´ä¸°å¯ŒæŒ‡æ ‡ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, LLM, æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, Transformer, å¾®è°ƒ, æƒé‡è¡°å‡, æ¨¡å‹å¯å¡‘æ€§, è¡¨ç¤ºå­¦ä¹ , ä¸‹æ¸¸ä»»åŠ¡é€‚åº”, æ­£åˆ™åŒ–, æ³¨æ„åŠ›æœºåˆ¶

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11137v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11137v1.pdf)

---

## [21. From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers](https://arxiv.org/abs/2602.11130v1)

**ä½œè€…**ï¼šMaximilian Plattner, Fabian Paischer, Johannes Brandstetter ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡å‘ç°3Dæ‰©æ•£Transformeråœ¨ç¨€ç–ç‚¹äº‘æ¡ä»¶ä¸‹å­˜åœ¨æ˜“è¢«å¾®å°æ‰°åŠ¨è§¦å‘çš„â€œç†”æ¯â€ç¢ç‰‡åŒ–å¤±è´¥æ¨¡å¼ï¼Œå¹¶é€šè¿‡åŠ¨åŠ›å­¦ä¸æœºåˆ¶è§£é‡Šæå‡ºç®€å•æµ‹è¯•æ—¶æ§åˆ¶æ–¹æ³•å®ç°ç¨³å®šåŒ–ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°å®åº”ç”¨ä¸­éœ€è¦ä»ç¨€ç–ç‚¹äº‘å¯é é‡å»ºè¿ç»­è¡¨é¢ï¼Œä½†ç°æœ‰3Dæ‰©æ•£Transformerè™½ç„¶æ€§èƒ½SOTAï¼Œå´åœ¨å¾®å°è¾“å…¥æ‰°åŠ¨ä¸‹ä¼šäº§ç”Ÿä¸¥é‡ä¸”éš¾ä»¥é¢„æœŸçš„è¾“å‡ºç¢ç‰‡åŒ–é£é™©ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åˆ©ç”¨æœºæ¢°å¯è§£é‡Šæ€§ä¸­çš„æ¿€æ´»è¡¥ä¸æŠ€æœ¯ï¼Œå°†â€œç†”æ¯â€ç°è±¡å®šä½åˆ°æ—©æœŸå»å™ªé˜¶æ®µçš„å•ä¸€äº¤å‰æ³¨æ„åŠ›æ¿€æ´»ï¼Œå¹¶é€šè¿‡åˆ†æå…¶å¥‡å¼‚å€¼è°±çš„è°±ç†µä½œä¸ºç¢ç‰‡åŒ–ä»£ç†æŒ‡æ ‡ï¼Œè¿›è€Œä»æ‰©æ•£åŠ¨åŠ›å­¦è§†è§’å°†å…¶è§£é‡Šä¸ºåå‘è¿‡ç¨‹çš„å¯¹ç§°ç ´ç¼ºåˆ†å²”ï¼Œå¹¶åŸºäºæ­¤è®¾è®¡äº†æµ‹è¯•æ—¶æ§åˆ¶æ–¹æ³•PowerRemapæ¥é‡å¡‘è¯¥æ¿€æ´»çš„è°±ç‰¹æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šMeltdownç°è±¡åœ¨å¤šç§SOTAæ¶æ„ã€æ•°æ®é›†å’Œå»å™ªç­–ç•¥ä¸­æ™®éå­˜åœ¨ï¼Œè€ŒPowerRemapå¯åœ¨æµ‹è¯•æ—¶æ˜¾è‘—æŠ‘åˆ¶è¯¥å¤±è´¥æ¨¡å¼ï¼Œç¨³å®šç‡æœ€é«˜è¾¾98.3%ï¼Œå±•ç¤ºäº†ä»ç”µè·¯çº§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶åˆ°æ‰©æ•£åŠ¨åŠ›å­¦è½¨è¿¹åˆ†å²”çš„å¯è§£é‡Šåˆ†æï¼Œèƒ½å¤Ÿåå‘æŒ‡å¯¼æ¨¡å‹çš„ç¨³å¥æ€§å¢å¼ºã€‚

**å…³é”®è¯**ï¼šæ‰©æ•£æ¨¡å‹, transformer, 3Dç‚¹äº‘ç”Ÿæˆ, è·¨æ³¨æ„åŠ›æœºåˆ¶, æœºåˆ¶å¯è§£é‡Šæ€§, å¯¹æŠ—é²æ£’æ€§, è¡¨é¢é‡å»º, æµ‹è¯•æ—¶æ§åˆ¶

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11130v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11130v1.pdf)

---

## [22. Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128v1)

**ä½œè€…**ï¼šReinhard Heckel, Mahdi Soltanolkotabi, Christos Thramboulidis  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§åœ¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ä¸­å¯¹â€œä½æˆåŠŸç‡â€æç¤ºè¯è¿›è¡Œéå¯¹ç§°åŠ æƒçš„æ–°ç­–ç•¥ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°æå‡æ¨¡å‹ä»ä½å‡†ç¡®ç‡åˆ°ç›®æ ‡å‡†ç¡®ç‡çš„æ”¶æ•›é€Ÿåº¦ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¦‚GRPOã€DAPOã€RLOOç­‰ç®—æ³•ä¸»è¦èšç„¦äºæˆåŠŸç‡ä¸­ç­‰çš„â€œæ¨¡ç³Šæç¤ºâ€ï¼Œå¯¹ç‰¹åˆ«ç®€å•å’Œç‰¹åˆ«å›°éš¾çš„æç¤ºé™æƒï¼Œå¯¼è‡´åœ¨ä»é›¶å¼€å§‹è®­ç»ƒï¼ˆå¦‚R1-Zeroï¼‰è¿™ç±»â€œä½æˆåŠŸç‡é˜¶æ®µâ€æ—¶æ•ˆç‡è¾ƒä½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¼•å…¥å¯¹æç¤ºçš„éå¯¹ç§°æƒé‡åˆ†é…æœºåˆ¶ï¼Œåˆ»æ„æé«˜å¯¹ç»éªŒæˆåŠŸç‡å¾ˆä½ç”šè‡³ä¸ºé›¶çš„æç¤ºçš„æ¢¯åº¦æƒé‡ï¼Œå¹¶ä»ç†è®ºä¸Šæ¨å¯¼åœ¨å›ºå®šæ›´æ–°é¢„ç®—ä¸‹ã€æœ€å°åŒ–ä»åˆå§‹åˆ°ç›®æ ‡æˆåŠŸç‡æ—¶é—´çš„æœ€ä¼˜æç¤ºåŠ æƒå½¢å¼ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜éå¯¹ç§°åŠ æƒåœ¨from-scratch RLæƒ…æ™¯ï¼ˆå¦‚R1-Zeroï¼‰æ˜¾è‘—åŠ é€Ÿæ€§èƒ½æå‡ï¼Œè€Œåœ¨å·²SFTåå†åšRLæ—¶æ”¶ç›Šè¾ƒå°ï¼›ç†è®ºåˆ†æè¿›ä¸€æ­¥è¯´æ˜åœ¨ä½æˆåŠŸç‡ã€å“åº”æ˜‚è´µçš„åœºæ™¯ä¸­ï¼Œæœ€ä¼˜æƒé‡åº”åå‘ä½æˆåŠŸæ¦‚ç‡æç¤ºï¼Œä»è€ŒåŠ å¿«æœ‰æ•ˆè®­ç»ƒæ—¶é—´çš„æ”¶æ•›ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , å¤§è¯­è¨€æ¨¡å‹, LLMåè®­ç»ƒ, å¯éªŒè¯å¥–åŠ±, from-scratchRL, å¥–åŠ±æ¨¡å‹, ç­–ç•¥ä¼˜åŒ–, æç¤ºåŠ æƒ, æ¨ç†èƒ½åŠ›æå‡

**è¯„åˆ†**ï¼š33

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11128v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11128v1.pdf)

---

## [23. The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126v1)

**ä½œè€…**ï¼šStephanie Holly, Alexandru-Ciprian ZÄƒvoianu, Siegfried Silber ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æŒ‡å‡ºç¦»çº¿å¤šç›®æ ‡ä¼˜åŒ–ä¸­çš„ç”Ÿæˆå¼æ–¹æ³•åœ¨éè¶…ä½“ç§¯æŒ‡æ ‡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå…¶æ ¹æºæ˜¯ç¦»çº¿æ•°æ®ä¸çœŸå®å¸•ç´¯æ‰˜å‰æ²¿çš„åˆ†å¸ƒåç§»ï¼ˆoffline-frontier shiftï¼‰ï¼Œå¯¼è‡´ç”Ÿæˆæ¨¡å‹è¿‡äºä¿å®ˆã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å·¥ä½œå¤šç”¨è¶…ä½“ç§¯è¯„ä¼°ç¦»çº¿å¤šç›®æ ‡ä¼˜åŒ–ä¸­çš„ç”Ÿæˆå¼æ–¹æ³•ï¼Œå› æ­¤éš¾ä»¥ç†è§£å®ƒä»¬åœ¨å…¶ä»–å…³é”®MOOæŒ‡æ ‡ï¼ˆå¦‚generational distanceï¼‰ä¸Šçš„çœŸå®è¡¨ç°åŠå¤±è´¥åŸå› ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…ç³»ç»Ÿæ¯”è¾ƒç”Ÿæˆå¼æ–¹æ³•ä¸è¿›åŒ–ç®—æ³•åœ¨å¤šç§MOOæŒ‡æ ‡ä¸Šçš„è¡¨ç°ï¼Œæå‡ºå¹¶å½¢å¼åŒ–â€œoffline-frontier shiftâ€è¿™ä¸€åˆ†å¸ƒåç§»è§†è§’ï¼Œå¹¶é€šè¿‡ç§¯åˆ†æ¦‚ç‡åº¦é‡åˆ†æå’Œå®è¯å±•ç¤ºç”Ÿæˆæ¨¡å‹åœ¨ç›®æ ‡ç©ºé—´ä¸­ä¿æŒä¸ç¦»çº¿æ•°æ®åˆ†å¸ƒâ€œä¿å®ˆæ¥è¿‘â€ã€‚

**ä¸»è¦ç»“è®º**ï¼šç¦»çº¿å¤šç›®æ ‡ä¼˜åŒ–æœ¬è´¨ä¸Šå—åˆ°åˆ†å¸ƒåç§»é™åˆ¶ï¼šè¦é€¼è¿‘çœŸå®å¸•ç´¯æ‰˜å‰æ²¿å¿…é¡»åœ¨ç›®æ ‡ç©ºé—´è¿›è¡Œåˆ†å¸ƒå¤–é‡‡æ ·ï¼Œè€Œå½“å‰ç”Ÿæˆå¼ä¼˜åŒ–æ–¹æ³•æ™®éåšä¸åˆ°è¿™ä¸€ç‚¹ï¼Œå› æ­¤åœ¨å¤šç§æŒ‡æ ‡ä¸Šè½åäºè¿›åŒ–æ–¹æ³•ï¼›è¯¥è§†è§’ä¸ºè¯Šæ–­ä½•æ—¶åŠä¸ºä½•ç”Ÿæˆå¼ä¼˜åŒ–å¤±è´¥æä¾›äº†æ–°çš„åˆ†æå·¥å…·ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆå¼æ¨¡å‹, å¤šç›®æ ‡ä¼˜åŒ–, ç¦»çº¿ä¼˜åŒ–, æ‰©æ•£æ¨¡å‹, åˆ†å¸ƒåç§», å¸•ç´¯æ‰˜å‰æ²¿, è¶…ä½“ç§¯æŒ‡æ ‡, ç”Ÿæˆå¼ä¼˜åŒ–, generative

**è¯„åˆ†**ï¼š27

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11126v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11126v1.pdf)

---

## [24. From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent](https://arxiv.org/abs/2602.11123v1)

**ä½œè€…**ï¼šGenmao Zhuang, Amir Barati Farimani  
**åˆ†ç±»**ï¼šcs.LG, cond-mat.mtrl-sci  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºææ–™çŸ¥è¯†å¯¼èˆªæ™ºèƒ½ä½“ MKNAï¼Œå¯å°†è‡ªç„¶è¯­è¨€ç§‘ç ”æ„å›¾è‡ªåŠ¨è½¬åŒ–ä¸ºæ•°æ®åº“æ£€ç´¢ã€æ€§è´¨é¢„æµ‹ã€ç»“æ„ç”Ÿæˆä¸ç¨³å®šæ€§è¯„ä¼°æ“ä½œï¼Œç”¨äºè‡ªä¸»å‘ç°é«˜æ€§èƒ½ææ–™å¹¶æ€»ç»“å¯è§£é‡Šè®¾è®¡å‡†åˆ™ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿé«˜æ€§èƒ½ææ–™å‘ç°ä¾èµ–ä¸“å®¶ç»éªŒå’Œé«˜æˆæœ¬è®¡ç®—æ¨¡æ‹Ÿï¼Œæ•ˆç‡ä½ä¸”éš¾ä»¥ç³»ç»ŸåŒ–ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½ä»è‡ªç„¶è¯­è¨€æ–‡çŒ®ä¸æ•°æ®åº“ä¸­è‡ªåŠ¨æç‚¼è®¾è®¡è§„åˆ™å¹¶æŒ‡å¯¼æœç´¢çš„æ–°å‹æ™ºèƒ½ç³»ç»Ÿã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºè¯­è¨€é©±åŠ¨çš„ Materials Knowledge Navigation Agentï¼ˆMKNAï¼‰ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€è§£æç§‘ç ”æ„å›¾ï¼Œè‡ªåŠ¨è°ƒç”¨ææ–™æ•°æ®åº“ã€æ€§è´¨é¢„æµ‹ä¸ç»“æ„ç”Ÿæˆ/ç¨³å®šæ€§è¯„ä¼°å·¥å…·ï¼Œå¹¶ä»æ–‡çŒ®å’Œæ•°æ®ä¸­è‡ªä¸»æŠ½å–å®šé‡é˜ˆå€¼ä¸åŒ–å­¦è®¾è®¡æ¨¡å¼ä»¥å½¢æˆæ•°æ®æ”¯æ’‘çš„å‡è®¾ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨é«˜å¾·æ‹œæ¸©é™¶ç“·æœç´¢ä»»åŠ¡ä¸­ï¼ŒMKNAè‡ªåŠ¨å‘ç°å¹¶éªŒè¯äº†Theta_D > 800 Kçš„ç­›é€‰å‡†åˆ™ï¼Œé‡æ–°å‘ç°é’»çŸ³ã€SiCã€SiNã€BeOç­‰ç»å…¸è¶…é«˜åˆšåº¦ææ–™ï¼Œå¹¶æå‡ºå¤šç§çƒ­åŠ›å­¦ç¨³å®šä¸”æ­¤å‰æœªæŠ¥é“çš„å¯Œ Be-C åŒ–åˆç‰©ï¼Œå±•ç¤ºå‡ºå…¶åœ¨å¯è§£é‡Šã€å¯æ³›åŒ–çš„è¯­è¨€å¼•å¯¼ææ–™æ¢ç´¢ä¸­çš„æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šå¤šæ™ºèƒ½ä½“, è‡ªä¸»ä»£ç†, è¯­è¨€é©±åŠ¨agent, ææ–™å‘ç°, å±æ€§é¢„æµ‹, ç»“æ„ç”Ÿæˆ, æ•°æ®åº“æ£€ç´¢, ç¨³å®šæ€§è¯„ä¼°, æ–‡çŒ®æŒ–æ˜, é«˜æ€§èƒ½é™¶ç“·, è®¾è®¡å¯å‘é‡æ„

**è¯„åˆ†**ï¼š63

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11123v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11123v1.pdf)

---

## [25. MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092v1)

**ä½œè€…**ï¼šCassandre Notton, Benjamin Stott, Philippe Schoeb ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.PL, quant-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMerLin æ˜¯ä¸€ä¸ªé¢å‘å…‰å­ä¸æ··åˆé‡å­æœºå™¨å­¦ä¹ çš„å¼€æºå‘ç°å¼•æ“ï¼Œå°†å¯å¾®åˆ†çš„çº¿æ€§å…‰å­¦ç”µè·¯ä»¿çœŸæ— ç¼é›†æˆè¿› PyTorch / scikit-learn ç”Ÿæ€ï¼Œç”¨äºç³»ç»ŸåŒ–åŸºå‡†æµ‹è¯•ä¸æ¨¡å‹å¤ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰é‡å­æœºå™¨å­¦ä¹ ç¼ºä¹è·¨æ¨¡å‹ã€æ•°æ®é›†å’Œç¡¬ä»¶çº¦æŸçš„ç³»ç»Ÿæ€§å®è¯æ¢ç´¢ï¼Œéš¾ä»¥åˆ¤æ–­é‡å­æ¨¡å‹åœ¨è¿‘ä¸­æœŸä½•å¤„çœŸæ­£å…·å¤‡å®ç”¨ä¼˜åŠ¿ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªå…¼å®¹ä¸»æµ ML å·¥å…·é“¾ã€å¯å¤ç°å’Œå¯æ‰©å±•çš„ç»Ÿä¸€å®éªŒå¹³å°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡º MerLin æ¡†æ¶ï¼Œå°†é«˜æ•ˆå¼ºæ¨¡æ‹Ÿçš„çº¿æ€§å…‰å­¦ç”µè·¯å°è£…ä¸ºå¯ç«¯åˆ°ç«¯åå‘ä¼ æ’­è®­ç»ƒçš„é‡å­å±‚ï¼Œèå…¥ PyTorch / scikit-learn å·¥ä½œæµï¼Œå¹¶ä»¥æ¨¡å—åŒ–å®éªŒå½¢å¼å¤ç° 18 ç¯‡å…‰å­/æ··åˆ QML å·¥ä½œï¼Œæ”¯æŒç¡¬ä»¶æ„ŸçŸ¥é…ç½®ä¸ç»å…¸-é‡å­æ··åˆæµç¨‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šMerLin è¯æ˜äº†å…‰å­ä¸æ··åˆé‡å­æ¨¡å‹å¯ä»¥è¢«è‡ªç„¶åµŒå…¥ç°æœ‰ ML ç”Ÿæ€ï¼Œå»ºç«‹äº†å¯å¤ç”¨çš„åŸºå‡†ä¸å¤ç°åŸºçº¿ï¼Œä¸ºåç»­åœ¨çœŸå®ä¸ç†æƒ³åŒ–ç¡¬ä»¶æ¡ä»¶ä¸‹å¼€å±•ç³»ç»ŸåŒ–å¯¹æ¯”ã€æ¶ˆèä¸ååŒç®—æ³•-ç¡¬ä»¶å…±è®¾è®¡æä¾›äº†é€šç”¨ç ”ç©¶åŸºç¡€ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç”Ÿæˆå¼æ¨¡å‹, é‡å­æœºå™¨å­¦ä¹ , æ··åˆé‡å­ç»å…¸æ¨¡å‹, å…‰å­é‡å­ç”µè·¯, PyTorché›†æˆ, å¯å¾®åˆ†è®­ç»ƒ, åŸºå‡†æµ‹è¯•æ¡†æ¶, artificial intelligence

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11092v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11092v1.pdf)

---

## [26. General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087v1)

**ä½œè€…**ï¼šJianxun Wang, Grant C. Forbes, Leonardo Villalobos-Arias ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§åŸºäºæ›´ä¸€èˆ¬çº¿æ€§è§„åˆ’å½¢å¼å’Œå‡¸å…±è½­ç†è®ºçš„â€œå¯è°ƒèŠ‚ f-divergenceâ€çº¦æŸï¼Œå®ç°å¯¹ç¦»çº¿RLä¸­ç­–ç•¥åç¦»è¡Œä¸ºæ•°æ®åˆ†å¸ƒçš„è‡ªé€‚åº”æ§åˆ¶ï¼Œä»è€Œåœ¨å›°éš¾æ•°æ®é›†ä¸Šå–å¾—æ›´å¥½æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°å®ç¦»çº¿RLæ•°æ®å¾€å¾€æ¢ç´¢åº¦ä½ã€è½¨è¿¹å¤šæ ·æ€§å·®ä¸”ç”±ä¸åŒæ°´å¹³çš„è¡Œä¸ºç­–ç•¥æ··åˆç”Ÿæˆï¼Œä¼ ç»Ÿæ–¹æ³•è¦ä¹ˆå› ä¼°è®¡Q/Vä¸å‡†è€Œé€€åŒ–ï¼Œè¦ä¹ˆå› å¯¹æ‰€æœ‰è¡Œä¸ºç­–ç•¥ä¸€åˆ€åˆ‡çº¦æŸè€Œè¿‡äºä¿å®ˆï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½åœ¨â€œæå‡å›æŠ¥â€å’Œâ€œéµå®ˆæ•°æ®æ”¯æŒâ€ä¹‹é—´è‡ªé€‚åº”æƒè¡¡çš„æ¡†æ¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…å…ˆå°†RLç›®æ ‡å†™æˆæ›´ä¸€èˆ¬çš„çº¿æ€§è§„åˆ’å½¢å¼ï¼Œå¹¶åˆ©ç”¨å‡¸å…±è½­å»ºç«‹f-divergenceä¸å¯¹Bellmanæ®‹å·®çº¦æŸä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œè¿›è€Œè®¾è®¡ä¸€ç±»å¯çµæ´»é€‰æ‹©å’Œè°ƒæ•´çš„f-divergenceå‡½æ•°ï¼Œä½¿çº¦æŸå¼ºåº¦å¯æ ¹æ®ç¦»çº¿æ•°æ®çš„éšæœºæ€§å’Œè¡Œä¸ºç­–ç•¥å¤šæ ·æ€§è‡ªé€‚åº”å˜åŒ–ï¼Œå¹¶å°†å…¶åµŒå…¥å…¼å®¹çš„çº¦æŸä¼˜åŒ–ç®—æ³•ä¸­ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨MuJoCoã€Fetchå’ŒAdroitHandç­‰ç¦»çº¿ä»»åŠ¡ä¸Šå®éªŒè¡¨æ˜ï¼Œè¯¥LPç†è®ºå½¢å¼ä¸f-divergenceâ€”Bellmanæ®‹å·®çš„è”ç³»æ˜¯æ­£ç¡®ä¸”å¯å®ç°çš„ï¼Œå¯è°ƒèŠ‚f-divergenceåœ¨ä½éšæœºæ€§ã€è¡Œä¸ºç­–ç•¥å¤šæ ·çš„æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†ç¦»çº¿RLæ€§èƒ½ï¼Œç›¸æ¯”å›ºå®šçº¦æŸç­–ç•¥æ›´æœ‰æ•ˆåœ°å¹³è¡¡äº†å®‰å…¨æ€§ä¸æ¢ç´¢æ€§ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , ç¦»çº¿RL, æœºå™¨å­¦ä¹ , ç­–ç•¥çº¦æŸ, è¡Œä¸ºç­–ç•¥å¤šæ ·æ€§, è´å°”æ›¼æ®‹å·®ä¼˜åŒ–, fæ•£åº¦, çº¿æ€§è§„åˆ’å½¢å¼åŒ–, MuJoCoå®éªŒ, llm

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11087v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11087v1.pdf)

---

## [27. GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084v1)

**ä½œè€…**ï¼šYuheng Luo, Shuyan Li, Zhong Cao  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model via SHAP, then enforces structured sparsity through group $L_{21}$ regularized logistic regression, yielding stable and interpretable selections. Extensive comparisons with LASSO, SHAP, and deep learning based methods show that GRASP consistently delivers comparable or superior predictive accuracy, while identifying fewer, less redundant, and more stable features.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šGRASP æå‡ºä¸€ç§ç»“åˆ Shapley å½’å› ä¸ç»„ç¨€ç–æ­£åˆ™çš„ç‰¹å¾é€‰æ‹©æ¡†æ¶ï¼Œåœ¨åŒ»ç–—é¢„æµ‹ä¸­å®ç°æ›´ç¨³å®šã€ç´§å‡‘ä¸”å¯è§£é‡Šçš„ç‰¹å¾é›†ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¦‚ LASSO ç­‰ç‰¹å¾é€‰æ‹©æ–¹æ³•åœ¨åŒ»ç–—é¢„æµ‹ä¸­å¸¸è¡¨ç°å‡ºé²æ£’æ€§å·®ã€ç‰¹å¾å†—ä½™é«˜å’Œå¯è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œéœ€è¦ä¸€ç§èƒ½åŒæ—¶å…¼é¡¾é¢„æµ‹æ€§èƒ½ä¸ä¸´åºŠå¯è§£é‡Šæ€§çš„ç‰¹å¾é€‰æ‹©æ–¹æ¡ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆåˆ©ç”¨é¢„è®­ç»ƒæ ‘æ¨¡å‹å’Œ SHAP è®¡ç®—ç‰¹å¾ç»„çš„é‡è¦æ€§ï¼Œå†åœ¨é€»è¾‘å›å½’ä¸­æ–½åŠ ç»„ L21 æ­£åˆ™å®ç°ç»“æ„åŒ–ç¨€ç–ï¼Œæœ€ç»ˆé€‰å‡ºç´§å‡‘ä¸”éå†—ä½™çš„ç‰¹å¾å­é›†ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ä¸ LASSOã€SHAP åŠå¤šç§æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ¯”è¾ƒä¸­ï¼ŒGRASP åœ¨ä¿æŒæˆ–æå‡é¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œé€‰å‡ºçš„ç‰¹å¾æ•°é‡æ›´å°‘ã€ç›¸å…³æ€§æ›´ä½ä¸”é€‰æ‹©ç»“æœæ›´åŠ ç¨³å®šï¼Œä½“ç°å‡ºæ›´å¥½çš„å®ç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç‰¹å¾é€‰æ‹©, åŒ»å­¦é¢„æµ‹, Shapleyå€¼, SHAPè§£é‡Š, ç¨€ç–æ­£åˆ™åŒ–, é€»è¾‘å›å½’æ¨¡å‹, ç‰¹å¾é‡è¦æ€§, æ¨¡å‹å¯è§£é‡Šæ€§, deep learning

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11084v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11084v1.pdf)

---

## [28. In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079v1)

**ä½œè€…**ï¼šFrank Xiao, Santiago Aranguri  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§åŸºäºæ¿€æ´»è¡¨ç¤ºçš„æ•°æ®å½’å› æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡LLMåè®­ç»ƒä¸­å®šä½å¼•å‘ç‰¹å®šæ¶Œç°è¡Œä¸ºçš„è®­ç»ƒæ ·æœ¬ï¼Œå¹¶é€šè¿‡å®é™…é‡è®­éªŒè¯ä¸ç¼“è§£ä¸è‰¯è¡Œä¸ºã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰LLMåœ¨ç”Ÿäº§ç¯å¢ƒåè®­ç»ƒé˜¶æ®µå¯èƒ½å‡ºç°éš¾ä»¥è§£é‡Šå’Œå‘ç°çš„ä¸è‰¯æ¶Œç°è¡Œä¸ºï¼Œç°æœ‰å½’å› æ–¹æ³•è¦ä¹ˆæˆæœ¬é«˜æ˜‚è¦ä¹ˆæ•ˆæœæœ‰é™ï¼Œå› æ­¤éœ€è¦ä¸€ç§å¯æ‰©å±•ã€å¯é ä¸”å»‰ä»·çš„æœºåˆ¶æ¥è¿½è¸ªâ€œé—®é¢˜è¡Œä¸ºâ€åˆ°å…·ä½“è®­ç»ƒæ•°æ®ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¯¹æµ‹è¯•æç¤ºä¸åå¥½æ•°æ®ï¼ˆDPOæ ·æœ¬ï¼‰çš„ä¸­é—´æ¿€æ´»è®¡ç®—â€œæ¿€æ´»å·®åˆ†å‘é‡â€ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦å°†ç‰¹å®šè¡Œä¸ºä¸è®­ç»ƒæ ·æœ¬è¿›è¡ŒåŒ¹é…ä¸æ’åºï¼Œå¹¶è¿›ä¸€æ­¥å¯¹è¡Œä¸º-æ ·æœ¬ç›¸ä¼¼åº¦çŸ©é˜µèšç±»ä»¥æ— ç›‘ç£å‘ç°æ¶Œç°è¡Œä¸ºï¼Œå†é€šè¿‡ä¿®æ”¹/è¿‡æ»¤è¿™äº›æ ·æœ¬å¹¶é‡è®­æ¥è¿›è¡Œå› æœéªŒè¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨OLMo 2çš„ç”Ÿäº§DPOè®­ç»ƒä¸Šï¼Œè¯¥æ–¹æ³•å‘ç°å¹¶ç•Œå®šäº†â€œå¹²æ‰°é¡¹è§¦å‘çš„å±é™©è¯·æ±‚åˆè§„â€è¿™ä¸€ä¸è‰¯è¡Œä¸ºï¼Œè¡¨æ˜å…¶æºäºè¢«æ±¡æŸ“çš„åå¥½æ•°æ®ï¼›é€šè¿‡è¿‡æ»¤æˆ–åè½¬é«˜å½’å› æ ·æœ¬æ ‡ç­¾å¯åˆ†åˆ«å°†è¯¥è¡Œä¸ºé™ä½63%å’Œ78%ï¼Œåœ¨æ•ˆæœå’Œæˆæœ¬ä¸Šå‡ä¼˜äºæ¢¯åº¦å½’å› ä¸LLMè¯„å®¡åŸºçº¿ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªç°å®çš„å®‰å…¨åŸºå‡†åœºæ™¯ã€‚

**å…³é”®è¯**ï¼šå¤§è¯­è¨€æ¨¡å‹, æ·±åº¦å­¦ä¹ , neuralnetwork, æ•°æ®å½’å› , æ¿€æ´»å·®åˆ†å‘é‡, åå¥½å¯¹é½, DPOå¾®è°ƒ, æ¨¡å‹å®‰å…¨, æœ‰å®³è¡Œä¸ºç¼“è§£, æ— ç›‘ç£è¡Œä¸ºå‘ç°

**è¯„åˆ†**ï¼š43

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11079v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11079v1.pdf)

---

## [29. Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064v1)

**ä½œè€…**ï¼šFiras Darwish, George Nicholson, Aiden Doherty ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Synthetic data offers a compelling path to scalable pretraining when real-world data is scarce, but models pretrained on synthetic data often fail to transfer reliably to deployment settings. We study this problem in full-body human motion, where large-scale data collection is infeasible but essential for wearable-based Human Activity Recognition (HAR), and where synthetic motion can be generated from motion-capture-derived representations. We pretrain motion time-series models using such synthetic data and evaluate their transfer across diverse downstream HAR tasks. Our results show that synthetic pretraining improves generalisation when mixed with real data or scaled sufficiently. We also demonstrate that large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals, clarifying key sim-to-real challenges and the limits and opportunities of synthetic motion data for transferable HAR representations.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ç³»ç»Ÿç ”ç©¶äº†åˆ©ç”¨åˆæˆäººä½“è¿åŠ¨æ•°æ®é¢„è®­ç»ƒå¯ç©¿æˆ´è®¾å¤‡HARæ¨¡å‹çš„å¯è¡Œæ€§ï¼Œå‘ç°åˆæˆæ•°æ®åœ¨è§„æ¨¡è¶³å¤Ÿæˆ–ä¸çœŸå®æ•°æ®æ··åˆæ—¶èƒ½æå‡æ³›åŒ–ï¼Œä½†ç›´æ¥ç”¨å¤§è§„æ¨¡åŠ¨ä½œæ•æ‰æ•°æ®é¢„è®­ç»ƒæ”¶ç›Šæœ‰é™ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¯ç©¿æˆ´äººä½“æ´»åŠ¨è¯†åˆ«éœ€è¦å¤§è§„æ¨¡å¸¦æ ‡æ³¨çš„çœŸå®è¿åŠ¨æ—¶åºæ•°æ®ï¼Œä½†é‡‡é›†æˆæœ¬é«˜ä¸”éš¾ä»¥è¦†ç›–å¤šæ ·åœºæ™¯ï¼Œå› æ­¤ä½œè€…å¸Œæœ›åˆ©ç”¨åŸºäºåŠ¨ä½œæ•æ‰ç”Ÿæˆçš„åˆæˆè¿åŠ¨æ•°æ®æ¥è§£å†³æ•°æ®åŒ®ä¹å’Œè¿ç§»æ³›åŒ–é—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥å…¨èº«äººä½“è¿åŠ¨ä¸ºç ”ç©¶å¯¹è±¡ï¼Œä½¿ç”¨ç”±åŠ¨ä½œæ•æ‰è¡ç”Ÿçš„åˆæˆè¿åŠ¨æ—¶åºæ•°æ®å¯¹HARæ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°åœ¨å¤šç§ä¸‹æ¸¸HARä»»åŠ¡ä¸Šçš„è¿ç§»æ€§èƒ½ï¼Œæ¯”è¾ƒâ€œçº¯åˆæˆâ€â€œåˆæˆ+çœŸå®æ··åˆâ€å’Œâ€œä»…å¤§è§„æ¨¡åŠ¨ä½œæ•æ‰é¢„è®­ç»ƒâ€ç­‰è®¾ç½®ã€‚

**ä¸»è¦ç»“è®º**ï¼šï¼ˆ1ï¼‰åˆæˆæ•°æ®åœ¨è§„æ¨¡è¶³å¤Ÿå¤§æˆ–ä¸çœŸå®æ•°æ®æ··åˆæ—¶èƒ½æ˜¾è‘—æå‡HARæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰å•çº¯æ‰©å¤§åŠ¨ä½œæ•æ‰åŸŸé¢„è®­ç»ƒè§„æ¨¡å¯¹å¯ç©¿æˆ´ä¼ æ„Ÿå™¨ä¿¡å·çš„è¿ç§»æ”¶ç›Šæœ‰é™ï¼Œå­˜åœ¨æ˜æ˜¾åŸŸåå·®ï¼›ï¼ˆ3ï¼‰å·¥ä½œæ¾„æ¸…äº†è¿åŠ¨åˆæˆæ•°æ®åœ¨sim-to-realä¸­çš„å±€é™æ€§ä¸æœºä¼šï¼Œä¸ºæ„å»ºå¯è¿ç§»çš„HARè¡¨ç¤ºæä¾›äº†å®è·µæŒ‡å—ã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , æ—¶åºè¡¨ç¤ºå­¦ä¹ , äººä½“åŠ¨ä½œè¯†åˆ«, å¯ç©¿æˆ´ä¼ æ„Ÿå™¨, åˆæˆæ•°æ®é¢„è®­ç»ƒ, è·¨åŸŸè¿ç§», sim-to-realåå·®, agent

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11064v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11064v1.pdf)

---

## [30. Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057v1)

**ä½œè€…**ï¼šXinyu Yuan, Yan Qiao, Zonghui Wang ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-11

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered "agent", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºå¤šæ™ºèƒ½ä½“ä¼˜åŒ–å™¨ï¼Œåˆ†è§£å¹¶åè°ƒå¤šå“ç§ç½‘ç»œæµé—®é¢˜ï¼Œåœ¨æ¥è¿‘æœ€ä¼˜è§£çš„å‰æä¸‹æ˜¾è‘—åŠ é€Ÿæ±‚è§£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿçº¿æ€§è§„åˆ’ç­‰ä¼˜åŒ–å¼•æ“åœ¨å¤§è§„æ¨¡å¤šå“ç§æµç½‘ç»œä¸­é¢ä¸´â€œæ±‚è§£è´¨é‡ vs è®¡ç®—æ—¶é—´â€çš„æƒè¡¡ï¼Œéš¾ä»¥æ»¡è¶³å®é™…äº¤é€šã€é€šä¿¡å’Œç‰©æµç³»ç»Ÿå¯¹é«˜æ•ˆã€é²æ£’ã€å¯æ‰©å±•åˆ†é…æ–¹æ¡ˆçš„éœ€æ±‚ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºæ¡†æ¶Pramï¼šå…ˆå°†å…¨å±€å¤šå“ç§æµé—®é¢˜åˆ’åˆ†ä¸ºå±€éƒ¨å­é—®é¢˜ï¼Œç”±å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½ä½“å¿«é€Ÿç»™å‡ºå±€éƒ¨åˆ†é…ï¼Œå†é€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å¯¹è¿™äº›å±€éƒ¨è§£è¿›è¡Œåè°ƒä¸ä¸€è‡´åŒ–ï¼Œä½¿æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ è¿‘ä¼¼æ¢¯åº¦ä¸‹é™å¹¶ä¿è¯æ”¶æ•›ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨çœŸå®æ•°æ®é›†å’Œå…¬å¼€ç½‘ç»œæ‹“æ‰‘ä¸Šï¼ŒPramåœ¨è§£è´¨é‡ä¸Šå¯åª²ç¾ç”šè‡³éƒ¨åˆ†è¶…è¶Šçº¿æ€§è§„åˆ’æ±‚è§£å™¨ï¼ŒåŒæ—¶è¿è¡Œæ—¶é—´å¿«1â€“2ä¸ªæ•°é‡çº§ï¼Œå¹¶åœ¨é“¾è·¯æ•…éšœå’Œçªå‘æµé‡ä¸‹æ€§èƒ½ä¸‹é™å°äº10%ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–ä¸é²æ£’æ€§ä¸”æ˜“äºé›†æˆåˆ°ä¸»æµèµ„æºåˆ†é…ç³»ç»Ÿä¸­ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹, æœºå™¨å­¦ä¹ , ç¥ç»ç½‘ç»œ, agent, å¤šæ™ºèƒ½ä½“, å¼ºåŒ–å­¦ä¹ , ä¸Šä¸‹æ–‡æ¢¯åº¦ä¸‹é™, ç½‘ç»œæµä¼˜åŒ–, å¤šå•†å“æµ, èµ„æºåˆ†é…ç³»ç»Ÿ, é²æ£’ä¼˜åŒ–

**è¯„åˆ†**ï¼š57

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.11057v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.11057v1.pdf)

---

