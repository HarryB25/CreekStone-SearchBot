# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-28

> å…± 30 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.CV](#csCV) (10 ç¯‡)
- [cs.LG](#csLG) (11 ç¯‡)
- [cs.AI](#csAI) (7 ç¯‡)
- [cs.CL](#csCL) (2 ç¯‡)

---

## cs.AI

## [1. Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330v1)

**ä½œè€…**ï¼šKunihiro Miyazaki, Takanobu Kawahara, Stephen Roberts ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, q-fin.TR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§å°†æŠ•èµ„åˆ†ææ‹†è§£ä¸ºç»†ç²’åº¦äº¤æ˜“ä»»åŠ¡çš„å¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼Œåœ¨æ—¥æœ¬è‚¡ç¥¨æ•°æ®çš„é˜²æ³„æ¼å›æµ‹ä¸­æ˜¾è‘—æå‡é£é™©è°ƒæ•´åæ”¶ç›Šï¼Œå¹¶å¯é€šè¿‡ç»„åˆä¼˜åŒ–è¿›ä¸€æ­¥å¢ç›Šã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ™ºèƒ½ä½“äº¤æ˜“ç³»ç»Ÿå¤šç”¨â€œåˆ†æå¸ˆ/ç»ç†â€å¼çš„ç²—ç²’åº¦æŒ‡ä»¤ï¼Œå¿½ç•¥çœŸå®æŠ•ç ”æµç¨‹ç»†èŠ‚ï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½ä¸‹é™ä¸”å†³ç­–è¿‡ç¨‹ä¸é€æ˜ã€‚ä½œè€…å¸Œæœ›é€šè¿‡æ›´è´´è¿‘å·¥ä½œæµçš„ä»»åŠ¡æ‹†è§£ï¼Œæé«˜å¯æ§æ€§ä¸æ”¶ç›Šè¡¨ç°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºå¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œå°†æŠ•ç ”è¿‡ç¨‹æ˜¾å¼åˆ†è§£ä¸ºæ›´ç»†çš„å­ä»»åŠ¡å¹¶ä¸²è”äº§å‡ºä¸­é—´ç»“æœï¼Œåœ¨ä»·æ ¼ã€è´¢æŠ¥ã€æ–°é—»ä¸å®è§‚ç­‰æ—¥æœ¬å¸‚åœºæ•°æ®ä¸Šè¿›è¡Œé˜²ä¿¡æ¯æ³„æ¼çš„å›æµ‹è¯„ä¼°ã€‚è¿›ä¸€æ­¥åˆ†æå„ä»£ç†ä¸­é—´è¾“å‡ºä¸ä¸‹æ¸¸å†³ç­–åå¥½çš„å¯¹é½ç¨‹åº¦ï¼Œå¹¶ç»“åˆä¼ ç»ŸæŠ•èµ„ç»„åˆä¼˜åŒ–åˆ©ç”¨ä¸æŒ‡æ•°çš„ä½ç›¸å…³æ€§åŠå„ç³»ç»Ÿè¾“å‡ºæ–¹å·®æ¥æå‡è¡¨ç°ã€‚

**ä¸»è¦ç»“è®º**ï¼šç»†ç²’åº¦ä»»åŠ¡åˆ†è§£ç›¸æ¯”ç²—ç²’åº¦è®¾è®¡èƒ½æ˜¾è‘—æ”¹å–„é£é™©è°ƒæ•´åæ”¶ç›Šï¼›ç³»ç»Ÿæ€§èƒ½çš„å…³é”®é©±åŠ¨ä¹‹ä¸€æ˜¯â€œä¸­é—´åˆ†æè¾“å‡ºâ€ä¸â€œä¸‹æ¸¸å†³ç­–åå¥½â€çš„å¯¹é½ã€‚ç»“åˆæ ‡å‡†ç»„åˆä¼˜åŒ–å¯è¿›ä¸€æ­¥è·å¾—æ›´ä¼˜çš„æ•´ä½“è¡¨ç°ï¼Œæç¤ºåœ¨å®ç›˜åº”ç”¨ä¸­ä»£ç†ç»“æ„ä¸ä»»åŠ¡é…ç½®è‡³å…³é‡è¦ã€‚

**å…³é”®è¯**ï¼šå¤šæ™ºèƒ½ä½“LLMäº¤æ˜“, æŠ•èµ„åˆ†æä»»åŠ¡åˆ†è§£, ç»†ç²’åº¦äº¤æ˜“ä»»åŠ¡, æ³„æ¼æ§åˆ¶å›æµ‹, é£é™©è°ƒæ•´æ”¶ç›Š, ä¸­é—´è¾“å‡ºå¯¹é½, å†³ç­–åå¥½å¯¹é½, æŠ•èµ„ç»„åˆä¼˜åŒ–, ä½ç›¸å…³æ€§ç­–ç•¥, æ—¥æœ¬è‚¡ç¥¨æ•°æ®

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23330v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23330v1.pdf)

---

## [2. LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329v1)

**ä½œè€…**ï¼šChen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus ç­‰ 19 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL, cs.CR, cs.CY, cs.HC  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šç ”ç©¶å‘ç°ï¼šåœ¨å¤šé¡¹å…·ç”Ÿç‰©å®‰å…¨åŒé‡ç”¨é€”é£é™©çš„ç”Ÿç‰©ä»»åŠ¡ä¸Šï¼Œç»™äºˆLLMè®¿é—®ä¼šæ˜¾è‘—æå‡æ–°æ‰‹è¡¨ç°ï¼Œç”šè‡³åœ¨éƒ¨åˆ†åŸºå‡†ä¸Šè¶…è¿‡ä»…ç”¨äº’è”ç½‘çš„ä¸“å®¶ï¼ŒåŒæ—¶æš´éœ²å‡ºé˜²æŠ¤æªæ–½æ˜“è¢«ç»•è¿‡çš„é—®é¢˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå°½ç®¡LLMåœ¨ç”Ÿç‰©åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æå‡ï¼Œä½†å°šä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦çœŸæ­£â€œæ‰¶èµ·æ–°æ‰‹â€å¹¶å¸¦æ¥ç§‘å­¦åŠ é€Ÿä¸åŒé‡ç”¨é€”é£é™©ä¸Šå‡ã€‚ä¸ºè¯„ä¼°ç°å®å½±å“ï¼Œéœ€è¦æ¯”è¾ƒâ€œLLMè¾…åŠ©çš„äººâ€ä¸â€œä»…äº’è”ç½‘çš„äººâ€çš„ä»»åŠ¡å®Œæˆæ•ˆæœã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…è¿›è¡Œå¤šæ¨¡å‹ã€å¤šåŸºå‡†çš„äººç±»æå‡ï¼ˆupliftï¼‰å®éªŒï¼šè®©æ–°æ‰‹åœ¨8å¥—ç”Ÿç‰©å®‰å…¨ç›¸å…³ä»»åŠ¡ä¸Šå®Œæˆå¤æ‚é—®é¢˜ï¼Œå¯¹ç…§ç»„ä»…å¯ç”¨äº’è”ç½‘ï¼Œå®éªŒç»„å¯ç”¨LLMä¸”ç»™äºˆå……è¶³æ—¶é—´ï¼ˆæœ€å¤æ‚ä»»åŠ¡æœ€é«˜13å°æ—¶ï¼‰ï¼Œå¹¶åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šä¸ä»…äº’è”ç½‘çš„ä¸“å®¶åŸºçº¿å¯¹æ¯”ã€‚

**ä¸»è¦ç»“è®º**ï¼šLLMæ˜¾è‘—æå‡æ–°æ‰‹å‡†ç¡®ç‡ï¼ˆçº¦4.16å€ï¼‰ï¼Œä¸”åœ¨æœ‰ä¸“å®¶åŸºçº¿çš„4ä¸ªåŸºå‡†ä¸­ï¼Œæ–°æ‰‹+LLMåœ¨å…¶ä¸­3ä¸ªè¶…è¿‡ä¸“å®¶ï¼›åŒæ—¶å•ç‹¬LLMå¸¸ä¼˜äºâ€œLLMè¾…åŠ©æ–°æ‰‹â€ï¼Œæ˜¾ç¤ºç”¨æˆ·æœªå……åˆ†å‘æŒ¥æ¨¡å‹èƒ½åŠ›ï¼Œä¸”å¤šæ•°å‚ä¸è€…èƒ½è¾ƒè½»æ¾è·å–åŒé‡ç”¨é€”ç›¸å…³ä¿¡æ¯ï¼Œæç¤ºéœ€è¦æŒç»­çš„äº¤äº’å¼æå‡è¯„ä¼°ä¸æ›´å¼ºçš„å®‰å…¨æ²»ç†ã€‚

**å…³é”®è¯**ï¼šæ–°æ‰‹èƒ½åŠ›æå‡è¯„æµ‹, äººç±»-LLMåä½œ, ç”Ÿç‰©å®‰å…¨ä»»åŠ¡, ç”Ÿç‰©å­¦åŸºå‡†æµ‹è¯•, å¤šæ¨¡å‹å¯¹æ¯”è¯„æµ‹, äººç±»å‚ä¸å®éªŒ, ä¸“å®¶-æ–°æ‰‹å¯¹ç…§, äº¤äº’å¼è¯„æµ‹æ¡†æ¶, å®‰å…¨æŠ¤æ æœ‰æ•ˆæ€§, ä¿¡æ¯è·å–å¯å¾—æ€§

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23329v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23329v1.pdf)

---

## [3. Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315v1)

**ä½œè€…**ï¼šSha Hu  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šé€šè¿‡å¯¹åŒä¸€è¾“å…¥æ–½åŠ å¤šç§ä¸å˜å˜æ¢å¹¶è¿›è¡Œâ€œé‡é‡‡æ ·å¼â€å¤šæ¬¡æ¨ç†ä¸èšåˆï¼Œå¯åˆ©ç”¨éƒ¨åˆ†ç‹¬ç«‹çš„è®¤çŸ¥ä¸ç¡®å®šæ€§è¯¯å·®æ¥æå‡æœ€ç»ˆæ¨ç†å‡†ç¡®ç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå³ä½¿è®­ç»ƒè‰¯å¥½çš„æ¨¡å‹ä¹Ÿä¼šå› å¶ç„¶ä¸ç¡®å®šæ€§ä¸è®¤çŸ¥ä¸ç¡®å®šæ€§äº§ç”Ÿæ¨ç†é”™è¯¯ï¼›ä½œè€…è§‚å¯Ÿåˆ°å¯¹è¾“å…¥åšä¸å˜å˜æ¢åï¼Œå¤šæ¬¡æ¨ç†çš„è¯¯å·®åœ¨è®¤çŸ¥ä¸ç¡®å®šæ€§å±‚é¢å¯èƒ½å‘ˆéƒ¨åˆ†ç‹¬ç«‹ï¼Œä»è€Œå­˜åœ¨å¯è¢«é›†æˆæ¶ˆé™¤çš„ç©ºé—´ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¯¹å•ä¸ªè¾“å…¥ç”Ÿæˆå¤šä¸ªæ»¡è¶³ä»»åŠ¡ä¸å˜æ€§çš„å˜æ¢ç‰ˆæœ¬ï¼ˆå¦‚æ—‹è½¬/ç¼©æ”¾/æ‰°åŠ¨ç­‰ï¼‰ï¼Œåœ¨åŒä¸€å·²è®­ç»ƒæ¨¡å‹ä¸Šåˆ†åˆ«æ¨ç†å¾—åˆ°å¤šç»„è¾“å‡ºï¼›å†é€šè¿‡èšåˆï¼ˆå¦‚æŠ•ç¥¨/å¹³å‡/ç½®ä¿¡åº¦èåˆç­‰ï¼‰å½¢æˆæ›´ç¨³å¥ã€æ›´å‡†ç¡®çš„æœ€ç»ˆé¢„æµ‹ï¼Œä»¥â€œæ¨ç†é˜¶æ®µé‡é‡‡æ ·â€æ–¹å¼é™ä½è®¤çŸ¥ä¸ç¡®å®šæ€§å½±å“ã€‚

**ä¸»è¦ç»“è®º**ï¼šå¤šå˜æ¢é‡é‡‡æ ·ä¸è¾“å‡ºèšåˆèƒ½åœ¨ä¸æ”¹åŠ¨æˆ–å¢å¤§æ¨¡å‹çš„æƒ…å†µä¸‹å‡å°‘ç”±è®¤çŸ¥ä¸ç¡®å®šæ€§å¯¼è‡´çš„æ¨ç†é”™è¯¯ã€æå‡å‡†ç¡®ç‡ï¼Œå¹¶ä¸ºåœ¨æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´è¿›è¡Œæƒè¡¡æä¾›äº†ä¸€ç§æ¨ç†æœŸç­–ç•¥ã€‚

**å…³é”®è¯**ï¼šè®¤çŸ¥ä¸ç¡®å®šæ€§, å¶ç„¶ä¸ç¡®å®šæ€§, ä¸ç¡®å®šæ€§åˆ†è§£, æµ‹è¯•æ—¶å¢å¼º, ä¸å˜æ€§å˜æ¢, é‡é‡‡æ ·æ¨ç†, å¤šæ ·æœ¬æ¨æ–­, è¾“å‡ºèšåˆ, é›†æˆæ¨ç†, æ¨ç†é²æ£’æ€§, è¯¯å·®ç‹¬ç«‹æ€§, æ¨¡å‹è§„æ¨¡-æ€§èƒ½æƒè¡¡

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23315v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23315v1.pdf)

---

## [4. The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302v1)

**ä½œè€…**ï¼šGiacomo Bonanno  
**åˆ†ç±»**ï¼šcs.AI, cs.LO, math.LO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡å°†KMä¿¡å¿µæ›´æ–°ä¸AGMä¿¡å¿µä¿®æ­£çš„å…¬ç†è½¬ä¸ºåŒä¸€å¥—æ¨¡æ€é€»è¾‘æ¡†æ¶åè¯æ˜ï¼šç”±AGMå¾—åˆ°çš„é€»è¾‘ä½“ç³»åŒ…å«KMä½“ç³»ï¼Œå› æ­¤AGMä¿®æ­£å¯è§†ä¸ºKMæ›´æ–°çš„ç‰¹ä¾‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šKMæ›´æ–°ä¸AGMä¿®æ­£æ˜¯ä¸¤ç±»ç»å…¸ä¿¡å¿µå˜æ›´ç†è®ºï¼Œä½†å®ƒä»¬çš„å½¢å¼å…³ç³»ä¸æ¸…æ™°ï¼›ä½œè€…å¸Œæœ›ç”¨ç»Ÿä¸€çš„é€»è¾‘è¯­è¨€ç²¾ç¡®æ¯”è¾ƒä¸¤è€…çš„è¡¨è¾¾åŠ›ä¸å…¬ç†å¼ºå¼±ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæŠŠKMä¸AGMå„è‡ªçš„å…¬ç†é€æ¡ç¿»è¯‘ä¸ºå«ä¸‰ä¸ªæ¨¡æ€ç®—å­ï¼ˆä¿¡å¿µBã€æ¡ä»¶ç®—å­>ã€å¿…ç„¶æ€§â–¡ï¼‰çš„æ¨¡æ€é€»è¾‘å…¬ç†ä½“ç³»ï¼Œåˆ†åˆ«æ„é€ $\mathcal L_{KM}$ä¸$\mathcal L_{AGM}$å¹¶è¿›è¡Œå¯å¯¼æ€§/åŒ…å«æ€§è¯æ˜ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯æ˜$\mathcal L_{KM}$çš„æ¯æ¡å…¬ç†éƒ½æ˜¯$\mathcal L_{AGM}$çš„å®šç†ï¼Œå³AGMé€»è¾‘ä¸¥æ ¼åŒ…å«KMé€»è¾‘ï¼Œä»è€ŒAGMä¿®æ­£å¯è¢«çœ‹ä½œKMæ›´æ–°çš„ç‰¹æ®Šæƒ…å½¢ï¼›å¯¹â€œå¼ºKMæ›´æ–°â€ï¼Œä¸¤è€…å·®å¼‚å¯ç¼©å‡ä¸ºä»…ä¸€ä¸ªå…³äºâ€œéæ„å¤–ä¿¡æ¯ï¼ˆæœ€åˆæœªè¢«ä¸ä¿¡ï¼‰â€çš„å…¬ç†ã€‚

**å…³é”®è¯**ï¼šä¿¡å¿µæ›´æ–°, ä¿¡å¿µä¿®æ­£, KM ä¿¡å¿µæ›´æ–°, AGM ä¿¡å¿µä¿®æ­£, æ¨¡æ€é€»è¾‘å…¬ç†åŒ–, ä¿¡å¿µç®—å­, æ¡ä»¶ç®—å­, å¿…ç„¶æ€§ç®—å­, é€»è¾‘åŒ…å«å…³ç³», å®šç†å¯å¯¼æ€§

**è¯„åˆ†**ï¼š6

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23302v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23302v1.pdf)

---

## [5. ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285v1)

**ä½œè€…**ï¼šHaohui Jia, Zheng Chen, Lingwei Zhu ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šODEBrainé€šè¿‡å°†EEGæ„é€ æˆè°±å›¾èŠ‚ç‚¹å¹¶ç”¨Neural ODEå»ºæ¨¡è¿ç»­æ—¶é—´æ½œåœ¨åŠ¨åŠ›å­¦ï¼Œå®ç°æ›´ç¨³å¥ã€æ›´å‡†ç¡®çš„EEGåŠ¨æ€é¢„æµ‹ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿç¦»æ•£æ—¶é—´çš„å¾ªç¯å¼æ½œå˜é‡æ¨¡å‹éœ€è¦æ—¶é—´ç¦»æ•£åŒ–ï¼Œæ˜“äº§ç”Ÿè¯¯å·®ç´¯ç§¯ä¸”éš¾æ•æ‰EEGç¬æ—¶çš„éçº¿æ€§åŠ¨æ€ç‰¹å¾ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆèåˆæ—¶ç©º-é¢‘ç‡ç‰¹å¾æ„å»ºè°±å›¾èŠ‚ç‚¹è¡¨ç¤ºè„‘ç½‘ç»œï¼Œå†åœ¨æ½œåœ¨ç©ºé—´ä¸­å¼•å…¥Neural ODEå¯¹è¿ç»­æ—¶é—´åŠ¨åŠ›å­¦è¿›è¡Œå»ºæ¨¡ä¸é¢„æµ‹ï¼Œä»¥åœ¨ä»»æ„æ—¶é—´ç‚¹åˆ»ç”»å¤æ‚è„‘çŠ¶æ€çš„éšæœºå˜åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒODEBrainåœ¨EEGåŠ¨æ€é¢„æµ‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚

**å…³é”®è¯**ï¼šEEGåŠ¨æ€é¢„æµ‹, è¿ç»­æ—¶é—´å»ºæ¨¡, æ½œå˜é‡åŠ¨æ€æ¨¡å‹, åŠ¨æ€å›¾ç¥ç»ç½‘ç»œ, è„‘ç½‘ç»œå»ºæ¨¡, è°±å›¾èŠ‚ç‚¹è¡¨ç¤º, æ—¶ç©ºé¢‘ç‰¹å¾èåˆ, éçº¿æ€§è„‘ä¿¡å·å»ºæ¨¡, éšæœºæ½œåœ¨çŠ¶æ€è¡¨ç¤º

**è¯„åˆ†**ï¼š21

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23285v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23285v1.pdf)

---

## [6. CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276v1)

**ä½œè€…**ï¼šHyungyung Lee, Hangyul Yoon, Edward Choi  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šCXReasonAgent é€šè¿‡å°†LLMä¸ä¸´åºŠè¯Šæ–­å·¥å…·ç»“åˆï¼Œåœ¨èƒ¸ç‰‡å¤šæ­¥æ¨ç†ä¸­ç”Ÿæˆå¯ç”±å›¾åƒè¯æ®æ”¯æ’‘ã€å¯éªŒè¯çš„è¯Šæ–­å¯¹è¯ç»“æœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰LVLMåœ¨èƒ¸ç‰‡è§£è¯»ä¸­å®¹æ˜“äº§å‡ºâ€œçœ‹ä¼¼åˆç†ä½†ç¼ºä¹è¯æ®æ”¯æ’‘â€çš„å›ç­”ï¼Œä¸”å¯è§†åŒ–è¯æ®ä¸è¶³ä»¥æ ¸éªŒï¼›åŒæ—¶ä¸ºæ–°ä»»åŠ¡é€‚é…å¸¸éœ€æ˜‚è´µå†è®­ç»ƒï¼Œå½±å“ä¸´åºŠå¯é æ€§ä¸å¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºCXReasonAgentï¼šç”¨LLMä½œä¸ºæ¨ç†ä¸å¯¹è¯ä¸­æ¢ï¼Œè°ƒç”¨åŸºäºå›¾åƒçš„è¯Šæ–­/è§†è§‰è¯æ®æå–å·¥å…·æ¥æ”¯æ’‘æ¯ä¸€æ­¥è¯Šæ–­æ¨ç†ï¼›å¹¶æ„å»ºå¤šè½®å¯¹è¯è¯„æµ‹é›†CXReasonDialï¼ˆ12é¡¹ä»»åŠ¡ã€1,946æ®µå¯¹è¯ï¼‰è¯„ä¼°è¯æ®æ‰æ ¹ä¸å¯éªŒè¯æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨CXReasonDialä¸Šï¼ŒCXReasonAgent ç›¸æ¯”ä¼ ç»ŸLVLMèƒ½ç”Ÿæˆæ›´å¿ å®äºå›¾åƒè¯æ®çš„å›ç­”å¹¶æä¾›å¯æ ¸éªŒçš„è§†è§‰ä¾æ®ï¼Œè¡¨æ˜å°†ä¸´åºŠè¯Šæ–­å·¥å…·èå…¥LLMä»£ç†å¯¹å®‰å…¨å…³é”®åŒ»ç–—åœºæ™¯å°¤ä¸ºé‡è¦ã€‚

**å…³é”®è¯**ï¼šèƒ¸éƒ¨Xå…‰, èƒ¸éƒ¨å½±åƒè¯Šæ–­, å¤šæ­¥è¯Šæ–­æ¨ç†, è¯æ®æ”¯æ’‘æ¨ç†, è§†è§‰è¯æ®å®šä½, å¹»è§‰æŠ‘åˆ¶, å¤šè½®åŒ»å­¦å¯¹è¯è¯„æµ‹, å®‰å…¨å…³é”®åŒ»ç–—AI

**è¯„åˆ†**ï¼š50

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23276v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23276v1.pdf)

---

## [7. Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271v1)

**ä½œè€…**ï¼šHaotian Zhai, Elias Stengel-Eskin, Pratik Patil ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç³»ç»Ÿæ€§è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†ï¼ˆDRAï¼‰åœ¨åŒä¸€é—®é¢˜ä¸‹è¾“å‡ºçš„éšæœºæ€§æ¥æºä¸å½±å“ï¼Œå¹¶æå‡ºé™ä½éšæœºæ€§ä¸”ä¸æŸè´¨é‡çš„ç¼“è§£ç­–ç•¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå°½ç®¡DRAçš„å¹³å‡ç ”ç©¶è´¨é‡åœ¨æå‡ï¼Œä½†åœ¨çœŸå®éƒ¨ç½²ä¸­åŒæ ·è¾“å…¥å¤šæ¬¡è¿è¡Œä¼šäº§ç”Ÿæ˜¾è‘—ä¸åŒçš„ç»“è®ºã€ä¾æ®ä¸å¼•ç”¨ï¼Œå¯¼è‡´ç»“æœä¸ç¨³å®šã€éš¾ä»¥ä¿¡ä»»ä¸å¤ç°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†DRAå»ºæ¨¡ä¸ºä¿¡æ¯è·å–çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæå‡ºé‡åŒ–è¾“å‡ºæ–¹å·®çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å°†éšæœºæ€§å½’å› åˆ°ä¿¡æ¯è·å–ã€ä¿¡æ¯å‹ç¼©ä¸æ¨ç†ä¸‰æ¨¡å—ï¼Œéšåé€šè¿‡å—æ§å®éªŒåˆ†æä¸åŒå†³ç­–æ­¥ï¼ˆå°¤å…¶æ—©æœŸï¼‰éšæœºæ€§å¯¹æœ€ç»ˆæ–¹å·®çš„è´¡çŒ®ï¼›æœ€åç”¨ç»“æ„åŒ–è¾“å‡ºä¸åŸºäºé›†æˆçš„æŸ¥è¯¢ç”Ÿæˆæ¥ç¼“è§£éšæœºæ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šé™ä½éšæœºæ€§é€šå¸¸èƒ½æå‡ç ”ç©¶è¾“å‡ºè´¨é‡ï¼Œå…¶ä¸­æ¨ç†é˜¶æ®µä¸æ—©æœŸæ­¥éª¤çš„éšæœºæ€§å¯¹æ–¹å·®è´¡çŒ®æœ€å¤§ï¼›åœ¨DeepSearchQAä¸Šï¼Œæ‰€ææ–¹æ³•åœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶å°†å¹³å‡éšæœºæ€§é™ä½çº¦22%ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦ç ”ç©¶ä»£ç†, éšæœºæ€§è¯„ä¼°, è¾“å‡ºæ–¹å·®, è¯„æµ‹æ¡†æ¶, ä¿¡æ¯è·å–MDP, é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹, ä¿¡æ¯è·å–, ä¿¡æ¯å‹ç¼©, æ¨ç†éšæœºæ€§, ç»“æ„åŒ–è¾“å‡º, é›†æˆå¼æŸ¥è¯¢ç”Ÿæˆ

**è¯„åˆ†**ï¼š40

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23271v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23271v1.pdf)

---

## cs.CL

## [8. A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations](https://arxiv.org/abs/2602.23300v1)

**ä½œè€…**ï¼šSoumya Dutta, Smruthi Balaji, Sriram Ganapathy  
**åˆ†ç±»**ï¼šcs.CL, eess.AS  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºMiSTER-Eæ··åˆä¸“å®¶æ¡†æ¶ï¼Œå°†è¯­éŸ³/æ–‡æœ¬å„è‡ªå»ºæ¨¡ä¸è·¨æ¨¡æ€èåˆè§£è€¦ï¼Œç”¨é—¨æ§åŠ¨æ€èåˆä¸‰ç±»ä¸“å®¶ä»¥æå‡å¯¹è¯æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¯¹è¯æƒ…æ„Ÿè¯†åˆ«éœ€è¦åŒæ—¶å¤„ç†å¤šè½®ä¸Šä¸‹æ–‡çš„æ—¶åºä¾èµ–ä¸å¤šæ¨¡æ€ä¿¡æ¯èåˆï¼Œç°æœ‰æ–¹æ³•å¸¸å°†ä¸¤è€…è€¦åˆå¯¼è‡´å»ºæ¨¡ä¸å¤Ÿçµæ´»ä¸”å¯¹é½ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç”¨åˆ†åˆ«é’ˆå¯¹è¯­éŸ³ä¸æ–‡æœ¬å¾®è°ƒçš„LLMç”Ÿæˆè¯è¯­çº§è¡¨ç¤ºï¼Œç»å·ç§¯-å¾ªç¯ä¸Šä¸‹æ–‡å±‚å¢å¼ºï¼›æ„å»ºè¯­éŸ³ä¸“å®¶ã€æ–‡æœ¬ä¸“å®¶ä¸è·¨æ¨¡æ€ä¸“å®¶ä¸‰è·¯é¢„æµ‹ï¼Œå¹¶ç”¨å­¦ä¹ åˆ°çš„gateåŠ æƒèåˆï¼ŒåŒæ—¶åŠ å…¥è¯­éŸ³-æ–‡æœ¬ç›‘ç£å¯¹æ¯”æŸå¤±ä¸è·¨ä¸“å®¶é¢„æµ‹çš„KLæ­£åˆ™ä»¥ä¿ƒè¿›å¯¹é½ä¸€è‡´ï¼Œä¸”ä¸ä½¿ç”¨è¯´è¯äººèº«ä»½ä¿¡æ¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨IEMOCAP/MELD/MOSIä¸Šåˆ†åˆ«è¾¾åˆ°70.9/69.5/87.9çš„åŠ æƒF1å¹¶ä¼˜äºå¤šç§åŸºçº¿ï¼Œæ¶ˆèå®éªŒè¡¨æ˜MoEé—¨æ§èåˆä¸å¯¹æ¯”/KLæ­£åˆ™å¯¹æ€§èƒ½æå‡å…³é”®ã€‚

**å…³é”®è¯**ï¼šå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«, è¯­éŸ³-æ–‡æœ¬èåˆ, æ··åˆä¸“å®¶æ¨¡å‹, é—¨æ§æœºåˆ¶, æ¨¡æ€ç‰¹å®šä¸Šä¸‹æ–‡å»ºæ¨¡, å·ç§¯-å¾ªç¯ä¸Šä¸‹æ–‡ç¼–ç , ç›‘ç£å¯¹æ¯”å­¦ä¹ , KLæ•£åº¦æ­£åˆ™åŒ–, è¯è¯­çº§è¡¨ç¤ºå­¦ä¹ , æ— è¯´è¯äººèº«ä»½å»ºæ¨¡

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23300v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23300v1.pdf)

---

## [9. SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables](https://arxiv.org/abs/2602.23286v1)

**ä½œè€…**ï¼šSungho Park, Jueun Kim, Wook-Shin Han  
**åˆ†ç±»**ï¼šcs.CL, cs.AI, cs.DB, cs.IR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºSPARTAï¼Œä¸€ä¸ªå¯æ‰©å±•ã€åŸåˆ™åŒ–çš„è‡ªåŠ¨åŒ–æ„å»ºæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤§è§„æ¨¡ã€æ·±å±‚å¤šè·³ä¸”åŒ…å«èšåˆ/åˆ†ç»„ç­‰æ“ä½œçš„è¡¨æ ¼-æ–‡æœ¬è”åˆé—®ç­”åŸºå‡†ï¼Œå¹¶æ­ç¤ºç°æœ‰æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„æ˜¾è‘—èƒ½åŠ›ç¼ºå£ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è¡¨æ ¼-æ–‡æœ¬QAåŸºå‡†è§„æ¨¡å°ä¸”äººå·¥æ„å»ºæ˜“å‡ºé”™ï¼Œé—®é¢˜å¤šä¸ºæµ…å±‚æ¨ç†ï¼Œè¾ƒå°‘è¦†ç›–>2è·³ä¸èšåˆã€åˆ†ç»„ç­‰å¤æ‚åˆ†ææ“ä½œï¼Œéš¾ä»¥çœŸå®è¯„æµ‹è·¨æ–‡æœ¬ä¸è¡¨æ ¼çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆå°†æºè¡¨ä¸æ–‡æœ¬æ®µè½å¯¹é½ï¼Œè‡ªåŠ¨ä»æ–‡æœ¬æŠ½å–åŸå­äº‹å®å½¢æˆâ€œgrounding tablesâ€ï¼Œæ„å»ºå¯æŸ¥è¯¢çš„å‚è€ƒäº‹å®åº“ï¼›å†æŒ‰ç›®æ ‡hopæ•°åˆæˆåµŒå¥—SQL/æŸ¥è¯¢å›¾ï¼Œå¹¶ç”¨provenance-based refinementä¿è¯æŸ¥è¯¢å¯æ‰§è¡Œä¸”éç©ºã€ç”¨realistic-structure enforcementé™åˆ¶ä¸ºæŸ¥è¯¢å›¾ååºéå†ä»¥ç”Ÿæˆæ›´è‡ªç„¶æµç•…çš„é—®é¢˜ï¼Œè¾…ä»¥è½»é‡äººå·¥éªŒè¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šSPARTAç”Ÿæˆäº†è¦†ç›–æ·±å¤šè·³ä¸èšåˆ/åˆ†ç»„ç­‰æ“ä½œçš„é«˜ä¿çœŸQAæ•°æ®é›†ï¼Œæ ‡æ³¨æˆæœ¬çº¦ä¸ºHybridQAçš„å››åˆ†ä¹‹ä¸€ï¼›åœ¨è¯¥åŸºå‡†ä¸ŠSOTAæ¨¡å‹ç›¸å¯¹HybridQA/OTT-QAçš„è¡¨ç°ä¸‹é™30+ F1ï¼Œè¡¨æ˜å½“å‰è·¨è¡¨-æ–‡æ¨ç†æ¨¡å‹ä»å­˜åœ¨æ ¹æœ¬æ€§ä¸è¶³ã€‚

**å…³é”®è¯**ï¼šè¡¨æ ¼-æ–‡æœ¬é—®ç­”, å¤šè·³æ¨ç†, æ ‘ç»“æ„æŸ¥è¯¢, é—®ç­”åŸºå‡†æ„å»º, è‡ªåŠ¨æ•°æ®ç”Ÿæˆ, è½»é‡äººå·¥éªŒè¯, å‚è€ƒäº‹å®æ•°æ®åº“, åŸå­äº‹å®æŠ½å–, SQLå¯æ‰§è¡Œæ€§æ ¡éªŒ, æº¯æºå¼æŸ¥è¯¢æ”¹å†™, æŸ¥è¯¢å›¾ååºéå†, èšåˆä¸åˆ†ç»„æ“ä½œ

**è¯„åˆ†**ï¼š33

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23286v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23286v1.pdf)

---

## cs.CV

## [10. MediX-R1: Open Ended Medical Reinforcement Learning](https://arxiv.org/abs/2602.23363v1)

**ä½œè€…**ï¼šSahal Shaji Mullappilly, Mohammed Irfan Kurpath, Omair Mohamed ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šMediX-R1æå‡ºé¢å‘åŒ»å­¦å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¼€æ”¾å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¤šä¿¡å·å¥–åŠ±ä¸LLMè¯„ä¼°ï¼Œå®ç°æ¯”é€‰æ‹©é¢˜æ›´è´´è¿‘ä¸´åºŠçš„è‡ªç”±æ–‡æœ¬å›ç­”ä¸æ¨ç†æå‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŒ»å­¦RLè®­ç»ƒä¸è¯„æµ‹å¸¸ä¾èµ–å¯éªŒè¯ç­”æ¡ˆæˆ–MCQï¼Œéš¾ä»¥ä¸ºå¼€æ”¾å¼ä¸´åºŠå›ç­”æä¾›ç¨³å®šã€æœ‰æ•ˆçš„åé¦ˆä¸å¯é çš„è¯­ä¹‰è¯„ä¼°ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨è§†è§‰-è¯­è¨€éª¨å¹²ä¸Šè¿›è¡ŒGroup-based RLå¾®è°ƒï¼Œè®¾è®¡å¤åˆå¥–åŠ±ï¼šLLMä¸¥æ ¼YES/NOè¯­ä¹‰æ­£ç¡®æ€§å¥–åŠ±ã€åŒ»å­¦embeddingè¯­ä¹‰å¥–åŠ±ï¼ˆè¦†ç›–åŒä¹‰æ”¹å†™/æœ¯è¯­å˜ä½“ï¼‰ã€ä»¥åŠæ ¼å¼ä¸æ¨¡æ€è¯†åˆ«å¥–åŠ±ï¼›åŒæ—¶æå‡ºç»Ÿä¸€è¯„æµ‹ï¼Œç”¨reference-based LLM-as-judgeæ›¿ä»£è„†å¼±çš„å­—ç¬¦ä¸²åŒ¹é…æŒ‡æ ‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šä»…ç”¨çº¦51KæŒ‡ä»¤æ•°æ®ï¼ŒMediX-R1åœ¨æ–‡æœ¬ä¸å›¾æ–‡åŒ»å­¦åŸºå‡†ä¸Šä¼˜äºå¼ºå¼€æºåŸºçº¿ï¼Œå°¤å…¶åœ¨å¼€æ”¾å¼ä¸´åºŠä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ï¼Œè¡¨æ˜å¼€æ”¾å¼RL+å¤åˆå¥–åŠ±+LLMè¯„ä¼°æ˜¯æå‡å¤šæ¨¡æ€åŒ»å­¦æ¨ç†å¯é æ€§çš„å¯è¡Œè·¯å¾„ã€‚

**å…³é”®è¯**ï¼šå¼€æ”¾å¼å¼ºåŒ–å­¦ä¹ , åŒ»å­¦å¤šæ¨¡æ€å¤§æ¨¡å‹, è§†è§‰è¯­è¨€æ¨¡å‹, ä¸´åºŠæ¨ç†, ç»„å¼å¼ºåŒ–å­¦ä¹ , å¤åˆå¥–åŠ±å‡½æ•°, å¥–åŠ±å»ºæ¨¡, åŒ»å­¦è¯­ä¹‰åµŒå…¥, å‚è€ƒç­”æ¡ˆè¯„æµ‹, å¤šæ¨¡æ€è¯†åˆ«å¥–åŠ±, æ ¼å¼çº¦æŸå¥–åŠ±

**è¯„åˆ†**ï¼š39

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23363v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23363v1.pdf)

---

## [11. VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale](https://arxiv.org/abs/2602.23361v1)

**ä½œè€…**ï¼šSven Elflein, Ruilong Li, SÃ©rgio Agostinho ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šVGG-TÂ³é€šè¿‡å°†å¯å˜é•¿åº¦çš„å‡ ä½•KVè¡¨ç¤ºåœ¨æµ‹è¯•æ—¶è’¸é¦ä¸ºå›ºå®šå¤§å°MLPï¼Œä½¿ç¦»çº¿å‰é¦ˆ3Dé‡å»ºåœ¨è¾“å…¥è§†è§’æ•°ä¸Šå®ç°çº¿æ€§æ‰©å±•å¹¶æ˜¾è‘—åŠ é€Ÿã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰ç¦»çº¿å‰é¦ˆ3Dé‡å»ºæ–¹æ³•å› åŸºäºsoftmaxæ³¨æ„åŠ›çš„å…¨å±€èšåˆå¯¼è‡´è®¡ç®—ä¸å†…å­˜éšå›¾åƒæ•°é‡äºŒæ¬¡å¢é•¿ï¼Œéš¾ä»¥æ‰©å±•åˆ°ä¸Šåƒå¼ å›¾åƒçš„åœºæ™¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ´å¯Ÿç“¶é¢ˆæ¥è‡ªå¯å˜é•¿åº¦çš„Key-Valueå‡ ä½•è¡¨ç¤ºï¼Œæå‡ºVisual Geometry Grounded Test Time Trainingï¼šåœ¨æµ‹è¯•é˜¶æ®µè®­ç»ƒ/è’¸é¦ä¸€ä¸ªå›ºå®šå®¹é‡çš„MLPæ¥æ‰¿è½½å…¨å±€åœºæ™¯ä¿¡æ¯ï¼Œä»è€Œåœ¨ä¿æŒå…¨å±€èšåˆèƒ½åŠ›çš„åŒæ—¶å°†å¤æ‚åº¦é™ä¸ºçº¿æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•å¯åœ¨54ç§’å†…é‡å»º1kå›¾åƒé›†åˆï¼Œç›¸æ¯”æ³¨æ„åŠ›åŸºçº¿æé€Ÿ11.6Ã—ï¼Œå¹¶åœ¨ç‚¹å›¾é‡å»ºè¯¯å·®ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–çº¿æ€§æ—¶é—´æ–¹æ³•ï¼ŒåŒæ—¶æ”¯æŒç”¨æœªè§å›¾åƒæŸ¥è¯¢åœºæ™¯è¡¨ç¤ºè¿›è¡Œè§†è§‰å®šä½ã€‚

**å…³é”®è¯**ï¼šå¤šè§†å›¾3Dé‡å»º, ç¦»çº¿å‰é¦ˆé‡å»º, çº¿æ€§æ—¶é—´æ‰©å±•, æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰, KVç©ºé—´è’¸é¦, å›ºå®šå¤§å°åœºæ™¯è¡¨ç¤º, å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰, å…¨å±€åœºæ™¯èšåˆ, è½¯æœ€å¤§æ³¨æ„åŠ›æ›¿ä»£, ç‚¹å›¾ï¼ˆpoint mapï¼‰é‡å»º

**è¯„åˆ†**ï¼š33

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23361v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23361v1.pdf)

---

## [12. Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training](https://arxiv.org/abs/2602.23357v1)

**ä½œè€…**ï¼šAheli Saha, RenÃ© Schuster, Didier Stricker  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡ç ”ç©¶äº‹ä»¶ç›¸æœºçš„å†…åœ¨ä¼ æ„Ÿå‚æ•°å¦‚ä½•å½±å“äº‹ä»¶æµç›®æ ‡æ£€æµ‹ï¼Œå¹¶é€šè¿‡è”åˆåˆ†å¸ƒè®­ç»ƒæå‡æ¨¡å‹å¯¹ä¸åŒä¼ æ„Ÿå™¨/å‚æ•°è®¾ç½®çš„é²æ£’æ³›åŒ–èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šäº‹ä»¶ç›¸æœºæ•°æ®å½¢æ€æ–°é¢–ä¸”å¯ç”¨æ•°æ®ä¸å‚æ•°åˆ†æä¸è¶³ï¼Œå¯¼è‡´æ¨¡å‹å¯¹ä¼ æ„Ÿå™¨å‚æ•°å˜åŒ–æ•æ„Ÿã€è·¨è®¾å¤‡/é…ç½®æ³›åŒ–å·®ï¼›å› æ­¤éœ€è¦ç³»ç»Ÿç†è§£å‚æ•°å½±å“å¹¶æ„å»ºä¼ æ„Ÿå™¨æ— å…³çš„ç¨³å¥æ£€æµ‹å™¨ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç³»ç»Ÿè€ƒå¯Ÿäº‹ä»¶ç›¸æœºä¿¡å·çš„å…³é”®å†…å‚ï¼ˆå¦‚è§¦å‘é˜ˆå€¼ã€å™ªå£°/äº‹ä»¶ç‡ç›¸å…³è®¾ç½®ç­‰ï¼‰å¯¹æ£€æµ‹æ€§èƒ½çš„ä½œç”¨è§„å¾‹ï¼Œå¹¶æ®æ­¤åœ¨è®­ç»ƒæ—¶å¯¹å¤šç§å‚æ•°åˆ†å¸ƒè¿›è¡Œâ€œè”åˆåˆ†å¸ƒè®­ç»ƒâ€ï¼Œè®©æ¨¡å‹åœ¨å¤šé…ç½®æ•°æ®ä¸Šå…±åŒå­¦ä¹ ä»¥è·å¾—é€‚åº”æ€§ä¸ä¼ æ„Ÿå™¨æ³›åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šä¸åŒå†…å‚ä¼šæ˜¾è‘—æ”¹å˜äº‹ä»¶åˆ†å¸ƒå¹¶å½±å“æ£€æµ‹ç²¾åº¦ä¸ç¨³å®šæ€§ï¼›é‡‡ç”¨è”åˆåˆ†å¸ƒè®­ç»ƒå¯æå‡å¯¹å‚æ•°æ¼‚ç§»ä¸è·¨ä¼ æ„Ÿå™¨å·®å¼‚çš„é²æ£’æ€§ï¼Œä½¿ä¸‹æ¸¸ç›®æ ‡æ£€æµ‹æ›´æ¥è¿‘ä¼ æ„Ÿå™¨æ— å…³çš„æ³›åŒ–è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šäº‹ä»¶ç›¸æœº, äº‹ä»¶è§†è§‰, äº‹ä»¶æµç›®æ ‡æ£€æµ‹, è‡ªé€‚åº”æ„ŸçŸ¥, ä¼ æ„Ÿå™¨æ³›åŒ–, ä¼ æ„Ÿå™¨æ— å…³é²æ£’æ€§, å†…å‚æ•æ„Ÿæ€§åˆ†æ, è”åˆåˆ†å¸ƒè®­ç»ƒ, é¢†åŸŸæ³›åŒ–, å¼‚æ­¥æˆåƒ

**è¯„åˆ†**ï¼š32

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23357v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23357v1.pdf)

---

## [13. Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?](https://arxiv.org/abs/2602.23339v1)

**ä½œè€…**ï¼šTilemachos Aravanis, Vladan StojniÄ‡, Bill Psomas ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§â€œæ£€ç´¢+æµ‹è¯•æ—¶é€‚é…â€çš„å°‘æ ·æœ¬å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡èåˆæ–‡æœ¬æç¤ºä¸å°‘é‡åƒç´ æ ‡æ³¨æ”¯æŒæ ·æœ¬ï¼Œæ˜¾è‘—ç¼©å°é›¶æ ·æœ¬ä¸å…¨ç›‘ç£åˆ†å‰²çš„æ€§èƒ½å·®è·ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¼€æ”¾è¯æ±‡åˆ†å‰²ä¾èµ–VLMçš„å›¾åƒçº§å¼±ç›‘ç£ä¸”è‡ªç„¶è¯­è¨€è¯­ä¹‰æ˜“æ­§ä¹‰ï¼Œå¯¼è‡´åƒç´ çº§åˆ†å‰²æ€§èƒ½è½åäºå…¨ç›‘ç£ï¼›ä½œè€…å¸Œæœ›ç”¨å°‘é‡åƒç´ æ ‡æ³¨ç¤ºä¾‹åœ¨ä¸ä¸¢å¤±å¼€æ”¾è¯æ±‡èƒ½åŠ›çš„å‰æä¸‹è¡¥è¶³ç›‘ç£ç¼ºå£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šåœ¨æµ‹è¯•æ—¶ä»å¯æ‰©å±•çš„æ”¯æŒé›†æ£€ç´¢ä¸å½“å‰æŸ¥è¯¢ç›¸å…³çš„åƒç´ æ ‡æ³¨æ ·æœ¬ï¼Œå¹¶ç”¨è½»é‡çº§é€‚é…å™¨ä¸ºâ€œæ¯å¼ å›¾â€å­¦ä¹ ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå°†æ–‡æœ¬ç‰¹å¾ä¸æ”¯æŒå›¾åƒçš„è§†è§‰ç‰¹å¾è¿›è¡Œå¯å­¦ä¹ çš„ã€æŒ‰æŸ¥è¯¢åŠ¨æ€çš„èåˆï¼ˆè€Œéæ‰‹å·¥æ™šæœŸèåˆï¼‰ï¼Œå†è¾“å‡ºåˆ†å‰²ç»“æœã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•åœ¨ä¿æŒå¼€æ”¾è¯æ±‡æ³›åŒ–çš„åŒæ—¶æ˜¾è‘—æå‡åˆ†å‰²ç²¾åº¦ã€ç¼©å°ä¸å…¨ç›‘ç£æ–¹æ³•çš„å·®è·ï¼›å¹¶æ”¯æŒæŒç»­æ‰©å……æ”¯æŒé›†ï¼Œé€‚ç”¨äºç»†ç²’åº¦/ä¸ªæ€§åŒ–åˆ†å‰²ç­‰åœºæ™¯ã€‚

**å…³é”®è¯**ï¼šå¼€æ”¾è¯æ±‡åˆ†å‰², å°‘æ ·æœ¬åˆ†å‰², é›¶æ ·æœ¬åˆ†å‰², è§†è§‰è¯­è¨€æ¨¡å‹, æ£€ç´¢å¢å¼º, æµ‹è¯•æ—¶é€‚é…, åƒç´ çº§æ ‡æ³¨, å¤šæ¨¡æ€ç‰¹å¾èåˆ, é€æŸ¥è¯¢èåˆ, ä¸ªæ€§åŒ–åˆ†å‰²

**è¯„åˆ†**ï¼š31

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23339v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23339v1.pdf)

---

## [14. ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding](https://arxiv.org/abs/2602.23306v1)

**ä½œè€…**ï¼šYiran Guan, Sifan Tu, Dingkang Liang ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šThinkOmniæå‡ºä¸€ç§æ— éœ€è®­ç»ƒã€æ— éœ€æ•°æ®çš„è§£ç æ¡†æ¶ï¼Œç”¨ç°æˆå¤§æ¨ç†æ¨¡å‹æŒ‡å¯¼å…¨æ¨¡æ€å¤§æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å…¨æ¨¡æ€å¤§æ¨¡å‹æ„ŸçŸ¥å¼ºä½†æ¨ç†å¼±ï¼Œè€Œé€šè¿‡å†è®­ç»ƒå¢å¼ºæ¨ç†éœ€è¦é«˜è´¨é‡æ•°æ®ã€ä»»åŠ¡é€‚é…ä¸é«˜ç®—åŠ›ï¼Œæˆæœ¬å’Œé—¨æ§›éƒ½å¾ˆé«˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ¡†æ¶åŒ…å«ä¸¤éƒ¨åˆ†ï¼šç”¨â€œLRM-as-a-Guideâ€åœ¨è§£ç æ—¶å¼•å…¥ç°æˆLRMçš„æ¨ç†å¼•å¯¼ä¿¡å·ï¼›å¹¶ç”¨â€œStepwise Contrastive Scalingâ€æŒ‰æ­¥éª¤è‡ªé€‚åº”å¹³è¡¡æ„ŸçŸ¥ä¸æ¨ç†è´¡çŒ®ï¼Œé¿å…æ‰‹åŠ¨è°ƒå‚ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨å…­ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šæ–¹æ³•ç¨³å®šæå‡æ€§èƒ½ï¼Œä¸»ç»“æœåœ¨MathVistaè¾¾70.2ã€MMAUè¾¾75.5ï¼Œè¯æ˜è®­ç»ƒ/æ•°æ®è‡ªç”±çš„å¼•å¯¼å¼è§£ç å¯æœ‰æ•ˆå°†æ–‡æœ¬æ¨ç†è¿ç§»åˆ°å…¨æ¨¡æ€åœºæ™¯ã€‚

**å…³é”®è¯**ï¼šå…¨æ¨¡æ€æ¨ç†, å¤šæ¨¡æ€æ¨ç†, å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆOLLMï¼‰, å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰, LRM å¼•å¯¼è§£ç , æ„ŸçŸ¥-æ¨ç†æƒè¡¡, å¤šæ¨¡æ€æ¨ç†åŸºå‡†, ThinkOmni

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23306v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23306v1.pdf)

---

## [15. PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM](https://arxiv.org/abs/2602.23297v1)

**ä½œè€…**ï¼šYiqing Wang, Chunming He, Ming-Chen Lu ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPRIMAé€šè¿‡å°†é£é™©-ç–¾ç—…å…ˆéªŒçŸ¥è¯†æ³¨å…¥æ–‡æœ¬ç¼–ç å™¨å¹¶ä¸å›¾åƒç‰¹å¾è¿›è¡Œå¤šç²’åº¦å¯¹é½é¢„è®­ç»ƒï¼Œå†ç”¨LLMèåˆå®ç°æ›´å‡†ç¡®ä¸”æ›´é²æ£’çš„åŒ»å­¦è¯Šæ–­åˆ†ç±»ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤šæ¨¡æ€è¯Šæ–­æ–¹æ³•å¾€å¾€æŠŠä¸´åºŠå…ƒæ•°æ®å½“ä½œç¦»æ•£æ ‡ç­¾ï¼Œéš¾ä»¥åˆ©ç”¨ä¸´åºŠæè¿°ä¸­è•´å«çš„é£é™©ä¸è¯Šæ–­è¯­ä¹‰å…ˆéªŒï¼Œå¯¼è‡´å›¾æ–‡/å…ƒæ•°æ®å¯¹é½ä¸è¶³ä¸æ³›åŒ–å—é™ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆç”¨RAGæ„å»ºé£é™©-ç–¾ç—…ç›¸å…³çš„ä¸“å®¶è¯­æ–™ä»¥ç²¾ç‚¼Clinical ModernBERTï¼ˆæ³¨å…¥è¯Šæ–­å…ˆéªŒï¼‰ï¼Œå†ä»¥DINOv3+ç²¾ç‚¼BERTçš„åŒç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ç”¨å››ç§äº’è¡¥æŸå¤±å®ç°å¤šç²’åº¦è¯­ä¹‰å¯¹é½ä¸è½¯æ ‡ç­¾å¤„ç†ç›¸å…³æ€§æ­§ä¹‰ï¼›æœ€åç”¨Qwen-3èåˆå¯¹é½åçš„ç‰¹å¾è¿›è¡Œç–¾ç—…åˆ†ç±»ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜PRIMAåœ¨åˆ†ç±»æ€§èƒ½ä¸é²æ£’æ€§ä¸Šæ˜¾è‘—ä¼˜äºSOTAï¼Œèƒ½å¤ŸæŠŠåƒç´ çº§è§†è§‰çº¿ç´¢ä¸æŠ½è±¡ä¸´åºŠçŸ¥è¯†æœ‰æ•ˆç»Ÿä¸€ï¼ŒåŒæ—¶ä¸ä¾èµ–è¶…å¤§è§„æ¨¡æ•°æ®æˆ–æé«˜ç®—åŠ›ã€‚

**å…³é”®è¯**ï¼šåŒ»å­¦å½±åƒè¯Šæ–­, ä¸´åºŠå…ƒæ•°æ®, å›¾æ–‡å¯¹é½, å¤šæ¨¡æ€é¢„è®­ç»ƒ, åŒç¼–ç å™¨, æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰, é£é™©-ç–¾ç—…å…³è”, è¯Šæ–­å…ˆéªŒ, è½¯æ ‡ç­¾, å¤šç²’åº¦è¯­ä¹‰å¯¹é½, ç‰¹å¾èåˆ

**è¯„åˆ†**ï¼š35

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23297v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23297v1.pdf)

---

## [16. ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2602.23295v1)

**ä½œè€…**ï¼šAyush Roy, Wei-Yang Alex Lee, Rudrasis Chakraborty ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šManifoldGDæå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ‰©æ•£å¼æ•°æ®è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¯ä¸ªå»å™ªæ­¥åŠ å…¥â€œæµå½¢ä¸€è‡´â€çš„å‡ ä½•çº¦æŸï¼Œå¼•å¯¼ç”Ÿæˆæ›´ä»£è¡¨æ€§ä¸”æ›´é«˜ä¿çœŸçš„å°å‹åˆæˆæ•°æ®é›†ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰training-freeæ‰©æ•£è’¸é¦å¤šä¸ºæ— å¼•å¯¼å»å™ªæˆ–ä»…å‘ç±»åŸå‹/IPCè´¨å¿ƒåšç®€å•æ¨¡å¼å¯¹é½ï¼Œå¯¼è‡´å¼•å¯¼è¿‡äºç²—ç³™ã€éš¾ä»¥å…¼é¡¾è¯­ä¹‰ä¸€è‡´æ€§ä¸ç±»å†…å¤šæ ·æ€§ã€‚ä½œè€…å¸Œæœ›åˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆå…ˆéªŒçš„åŒæ—¶ï¼Œå¼•å…¥æ›´ç²¾ç»†çš„å‡ ä½•ç»“æ„çº¦æŸæ¥æå‡è’¸é¦è´¨é‡è€Œä¸å†è®­ç»ƒæ¨¡å‹ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆåœ¨VAEæ½œç‰¹å¾ä¸Šåšå±‚æ¬¡åŒ–ï¼ˆdivisiveï¼‰èšç±»å¾—åˆ°å¤šå°ºåº¦IPC coresetsï¼Œè¦†ç›–ç²—ç²’åº¦è¯­ä¹‰æ¨¡æ€ä¸ç»†ç²’åº¦ç±»å†…å˜åŒ–ï¼›åœ¨æ‰©æ•£æ¯ä¸ªå»å™ªæ—¶åˆ»ç”¨IPCå±€éƒ¨é‚»åŸŸä¼°è®¡æ½œç©ºé—´å±€éƒ¨æµå½¢ï¼Œå¹¶å°†â€œæ¨¡å¼å¯¹é½å‘é‡â€æŠ•å½±åˆ°è¯¥æµå½¢çš„å±€éƒ¨åˆ‡ç©ºé—´ä¸Šï¼Œä½¿é‡‡æ ·è½¨è¿¹æ²¿æµå½¢å‰è¿›ã€ä¿æŒè¯­ä¹‰ä¸€è‡´ä¸”ä¸åç¦»æ•°æ®å‡ ä½•ç»“æ„ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ— éœ€ä»»ä½•æ¨¡å‹é‡è®­ç»ƒçš„å‰æä¸‹ï¼ŒManifoldGDç›¸è¾ƒç°æœ‰training-freeä¸training-basedè’¸é¦åŸºçº¿ï¼Œåœ¨FIDã€çœŸå®/åˆæˆåµŒå…¥çš„l2è·ç¦»ä»¥åŠä¸‹æ¸¸åˆ†ç±»ç²¾åº¦ä¸Šå‡å–å¾—ç¨³å®šæå‡ï¼Œè¡¨æ˜å‡ ä½•æ„ŸçŸ¥çš„æµå½¢å¼•å¯¼èƒ½æ˜¾è‘—å¢å¼ºè’¸é¦æ•°æ®çš„ä»£è¡¨æ€§ã€å¤šæ ·æ€§ä¸ä¿çœŸåº¦ã€‚

**å…³é”®è¯**ï¼šæ•°æ®é›†è’¸é¦, æ— è®­ç»ƒè’¸é¦, Diffusion, åˆ†æ•°åŒ¹é…æ‰©æ•£, æµå½¢å¼•å¯¼, æ½œç©ºé—´æµå½¢, åˆ‡ç©ºé—´æŠ•å½±, å®ä¾‹åŸå‹ä¸­å¿ƒï¼ˆIPCï¼‰, å±‚æ¬¡èšç±», VAE æ½œç‰¹å¾, å¤šå°ºåº¦æ ¸å¿ƒé›†ï¼ˆcoresetï¼‰

**è¯„åˆ†**ï¼š28

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23295v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23295v1.pdf)

---

## [17. Towards Long-Form Spatio-Temporal Video Grounding](https://arxiv.org/abs/2602.23294v1)

**ä½œè€…**ï¼šXin Gu, Bing Fan, Jiali Yao ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºé¢å‘é•¿è§†é¢‘çš„è‡ªå›å½’Transformeræ¡†æ¶ART-STVGï¼Œä»¥æµå¼é€å¸§å¤„ç†ä¸è®°å¿†æœºåˆ¶å®ç°é•¿æ—¶ç©ºè§†é¢‘ç›®æ ‡å®šä½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰STVGæ–¹æ³•ä¸»è¦é’ˆå¯¹çŸ­è§†é¢‘å¹¶å¸¸éœ€ä¸€æ¬¡æ€§å¤„ç†å…¨å¸§ï¼Œéš¾ä»¥åº”å¯¹é•¿è§†é¢‘ä¸­æ›´é•¿æ—¶é—´è·¨åº¦ä¸å¤§é‡æ— å…³å†…å®¹å¸¦æ¥çš„è®¡ç®—ä¸å®šä½å›°éš¾ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šART-STVGå°†è§†é¢‘è§†ä¸ºæµå¼è¾“å…¥è¿›è¡Œè‡ªå›å½’é¡ºåºå»ºæ¨¡ï¼Œè®¾è®¡ç©ºé—´/æ—¶é—´è®°å¿†åº“ä¸ºè§£ç å™¨æä¾›è·¨å¸§ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡è®°å¿†é€‰æ‹©ç­–ç•¥è¿‡æ»¤ä¸ç›¸å…³å†å²ä¿¡æ¯ï¼›åŒæ—¶é‡‡ç”¨çº§è”æ—¶ç©ºå®šä½ç»“æ„ï¼Œç”¨ç©ºé—´è§£ç å™¨çš„ç»†ç²’åº¦çº¿ç´¢è¾…åŠ©æ›´å¤æ‚çš„æ—¶é—´å®šä½ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨æ–°æ‰©å±•çš„é•¿è§†é¢‘LF-STVGæ•°æ®é›†ä¸Šï¼ŒART-STVGç›¸è¾ƒSOTAå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ä¼ ç»ŸçŸ­è§†é¢‘STVGä»»åŠ¡ä¸Šä¿æŒæœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒéªŒè¯äº†æµå¼è‡ªå›å½’ä¸è®°å¿†é€‰æ‹©/çº§è”è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šé•¿è§†é¢‘æ—¶ç©ºè§†é¢‘å®šä½, æ—¶ç©ºè§†é¢‘å®šä½, è§†é¢‘ç›®æ ‡å®šä½, æµå¼è§†é¢‘å¤„ç†, è®°å¿†åº“æœºåˆ¶, è®°å¿†é€‰æ‹©ç­–ç•¥, çº§è”æ—¶ç©ºè§£ç , ç©ºé—´è§£ç å™¨, æ—¶é—´è§£ç å™¨, é•¿æ—¶åºå»ºæ¨¡, é•¿è§†é¢‘æ•°æ®é›†æ‰©å±•

**è¯„åˆ†**ï¼š26

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23294v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23294v1.pdf)

---

## [18. PGVMS: A Prompt-Guided Unified Framework for Virtual Multiplex IHC Staining with Pathological Semantic Learning](https://arxiv.org/abs/2602.23292v1)

**ä½œè€…**ï¼šFuqiang Chen, Ranran Zhang, Wanming Hu ç­‰ 9 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Immunohistochemical (IHC) staining enables precise molecular profiling of protein expression, with over 200 clinically available antibody-based tests in modern pathology. However, comprehensive IHC analysis is frequently limited by insufficient tissue quantities in small biopsies. Therefore, virtual multiplex staining emerges as an innovative solution to digitally transform H&E images into multiple IHC representations, yet current methods still face three critical challenges: (1) inadequate semantic guidance for multi-staining, (2) inconsistent distribution of immunochemistry staining, and (3) spatial misalignment across different stain modalities. To overcome these limitations, we present a prompt-guided framework for virtual multiplex IHC staining using only uniplex training data (PGVMS). Our framework introduces three key innovations corresponding to each challenge: First, an adaptive prompt guidance mechanism employing a pathological visual language model dynamically adjusts staining prompts to resolve semantic guidance limitations (Challenge 1). Second, our protein-aware learning strategy (PALS) maintains precise protein expression patterns by direct quantification and constraint of protein distributions (Challenge 2). Third, the prototype-consistent learning strategy (PCLS) establishes cross-image semantic interaction to correct spatial misalignments (Challenge 3).

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPGVMSæå‡ºä¸€ç§ç”±ç—…ç†è§†è§‰è¯­è¨€æ¨¡å‹æç¤ºå¼•å¯¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä»…ç”¨å•æŸ“ï¼ˆuniplexï¼‰è®­ç»ƒæ•°æ®å³å¯ä»H&Eå›¾åƒç”Ÿæˆå¤šç§IHCè™šæ‹ŸæŸ“è‰²ï¼Œå¹¶æå‡è¯­ä¹‰ä¸€è‡´æ€§ã€è›‹ç™½åˆ†å¸ƒå‡†ç¡®æ€§ä¸è·¨æ¨¡æ€å¯¹é½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¸´åºŠIHCæ£€æµ‹è™½ä¸°å¯Œï¼Œä½†å°æ´»æ£€ç»„ç»‡é‡ä¸è¶³é™åˆ¶äº†å¤šé‡IHCåˆ†æï¼›ç°æœ‰è™šæ‹Ÿå¤šé‡æŸ“è‰²æ–¹æ³•åœ¨è¯­ä¹‰æŒ‡å¯¼ä¸è¶³ã€æŸ“è‰²åˆ†å¸ƒä¸ä¸€è‡´ä»¥åŠä¸åŒæŸ“è‰²æ¨¡æ€ç©ºé—´é”™ä½ä¸Šä»å­˜åœ¨å…³é”®ç“¶é¢ˆã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ¡†æ¶ä»¥è‡ªé€‚åº”æç¤ºæœºåˆ¶ä¸ºæ ¸å¿ƒï¼Œå€ŸåŠ©ç—…ç†è§†è§‰è¯­è¨€æ¨¡å‹åŠ¨æ€è°ƒæ•´æŸ“è‰²æç¤ºä»¥æä¾›å¤šæŸ“è¯­ä¹‰å¼•å¯¼ï¼›åŒæ—¶å¼•å…¥PALSç›´æ¥é‡åŒ–å¹¶çº¦æŸè›‹ç™½è¡¨è¾¾åˆ†å¸ƒä»¥ç¨³å®šå…ç–«åŒ–å­¦æŸ“è‰²å¼ºåº¦/æ¨¡å¼ï¼Œå¹¶ç”¨PCLSé€šè¿‡è·¨å›¾åƒè¯­ä¹‰äº¤äº’ä¸åŸå‹ä¸€è‡´æ€§å­¦ä¹ çº æ­£ä¸åŒæŸ“è‰²æ¨¡æ€é—´çš„ç©ºé—´ä¸å¯¹é½ã€‚

**ä¸»è¦ç»“è®º**ï¼šPGVMSåœ¨ä»…ä½¿ç”¨å•æŸ“æ•°æ®è®­ç»ƒçš„æ¡ä»¶ä¸‹ï¼Œèƒ½å¤Ÿæ›´å¯é åœ°ç”Ÿæˆå¤šç§IHCè™šæ‹ŸæŸ“è‰²ç»“æœï¼Œå¹¶åœ¨è¯­ä¹‰å¯æ§æ€§ã€è›‹ç™½è¡¨è¾¾åˆ†å¸ƒä¸€è‡´æ€§ä¸è·¨æ¨¡æ€ç©ºé—´å¯¹é½æ–¹é¢ç¼“è§£äº†ç°æœ‰æ–¹æ³•çš„ä¸‰ç±»æ ¸å¿ƒé—®é¢˜ã€‚

**å…³é”®è¯**ï¼šè™šæ‹Ÿå¤šé‡IHCæŸ“è‰², æ•°å­—ç—…ç†, æç¤ºå¼•å¯¼å­¦ä¹ , ç—…ç†è§†è§‰è¯­è¨€æ¨¡å‹, å•é‡æŸ“è‰²è®­ç»ƒ, è›‹ç™½è¡¨è¾¾åˆ†å¸ƒçº¦æŸ, è›‹ç™½æ„ŸçŸ¥å­¦ä¹ , åŸå‹ä¸€è‡´æ€§å­¦ä¹ , è·¨æŸ“è‰²æ¨¡æ€é…å‡†, å¤šæŸ“è‰²è¯­ä¹‰å¼•å¯¼

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23292v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23292v1.pdf)

---

## [19. LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction](https://arxiv.org/abs/2602.23290v1)

**ä½œè€…**ï¼šZhengyang Wei, Renzhi Jing, Yiyi He ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šLineGraph2Roadé€šè¿‡å°†é“è·¯è¿é€šæ€§é¢„æµ‹è½¬åŒ–ä¸ºçº¿å›¾ä¸Šçš„ç»“æ„åŒ–å›¾æ¨ç†ï¼Œç”¨Graph Transformeræ›´å‡†ç¡®åœ°æ¢å¤å¤æ‚é“è·¯æ‹“æ‰‘ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰â€œå…³é”®ç‚¹+è¿é€šæ€§â€èŒƒå¼å¾€å¾€éš¾ä»¥å»ºæ¨¡é•¿ç¨‹ä¾èµ–ä¸å¤æ‚æ‹“æ‰‘ï¼ˆå¦‚å¤šåˆ†æ”¯ã€äº¤å‰ï¼‰ï¼Œä¸”å¯¹è¾¹ç«¯ç‚¹ç‰¹å¾èåˆåœ¨åŒæ„é“¾æ¥ä¸Šè¡¨è¾¾å—é™ï¼Œå¯¼è‡´è¿æ¥é¢„æµ‹ä¸ç¨³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå…ˆä»åˆ†å‰²æ©ç æå–å…³é”®ç‚¹ï¼Œå¹¶åœ¨å…¨å±€ä½†ç¨€ç–çš„æ¬§æ°å›¾ä¸­è¿æ¥è·ç¦»é˜ˆå€¼å†…çš„ç‚¹å¯¹å½¢æˆå€™é€‰é“è·¯è¾¹ï¼Œå†å°†è¯¥å›¾è½¬æ¢ä¸ºçº¿å›¾ï¼ˆæŠŠâ€œè¾¹â€å˜æˆâ€œç‚¹â€ï¼‰å¹¶åœ¨å…¶ä¸Šç”¨Graph Transformerè¿›è¡ŒäºŒåˆ†ç±»ä»¥åˆ¤æ–­è¾¹æ˜¯å¦ä¸ºçœŸå®é“è·¯è¿æ¥ï¼›åŒæ—¶åŠ å…¥ç«‹äº¤/ä¸‹ç©¿åˆ¤åˆ«å¤´å¤„ç†å¤šå±‚äº¤å‰ï¼Œå¹¶ç”¨è€¦åˆNMSä¿ç•™å…³é”®è¿æ¥ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨City-scaleã€SpaceNetä¸Global-scaleä¸‰å¥—åŸºå‡†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨TOPO-F1ä¸APLSç­‰æŒ‡æ ‡ä¸Šå–å¾—ï¼ˆéƒ¨åˆ†ï¼‰SOTAè¡¨ç°ï¼Œå¹¶èƒ½æ›´å¥½ä¿ç•™çœŸå®åœºæ™¯ä¸­ç»†ç²’åº¦é“è·¯ç»†èŠ‚ä¸æ‹“æ‰‘è¿é€šæ€§ã€‚

**å…³é”®è¯**ï¼šé“è·¯ç½‘ç»œæå–, å«æ˜Ÿå½±åƒ, æ‹“æ‰‘æ„ŸçŸ¥åˆ†å‰², å…³é”®ç‚¹æ£€æµ‹, è¿é€šæ€§é¢„æµ‹, æ¬§æ°å›¾, ç»“æ„åŒ–å›¾æ¨ç†, å¤šå±‚äº¤å‰ï¼ˆç«‹äº¤ï¼‰å»ºæ¨¡, éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰

**è¯„åˆ†**ï¼š25

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23290v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23290v1.pdf)

---

## cs.LG

## [20. Model Agreement via Anchoring](https://arxiv.org/abs/2602.23360v1)

**ä½œè€…**ï¼šEric Eaton, Surbhi Goel, Marcel Hussing ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.   We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºä¸€ç§â€œé”šå®š(anchoring)â€åˆ†ææŠ€å·§æ¥ä¸Šç•Œç‹¬ç«‹è®­ç»ƒæ¨¡å‹ä¹‹é—´çš„é¢„æµ‹åˆ†æ­§ï¼Œå¹¶è¯æ˜åœ¨å¤šç§å¸¸ç”¨ç®—æ³•ä¸­åˆ†æ­§å¯éšå…³é”®è®­ç»ƒå‚æ•°å¢å¤§è€Œè¶‹è¿‘äº0ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨ä¸åè°ƒè®­ç»ƒè¿‡ç¨‹ã€ä»…ä¾èµ–ç‹¬ç«‹é‡‡æ ·è®­ç»ƒçš„å‰æä¸‹ï¼Œä½œè€…å¸Œæœ›ç”¨å¯é€‚ç”¨äºç°æœ‰è®­ç»ƒæ–¹æ³•çš„åˆ†æï¼Œè§£é‡Šå¹¶æ§åˆ¶ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ä¸ä¸€è‡´ï¼ˆåˆ†æ­§ï¼‰çš„ç¨‹åº¦ï¼Œç†æƒ³æƒ…å†µä¸‹å¯å°†åˆ†æ­§é©±åŠ¨åˆ°0ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»¥å›å½’ä»»åŠ¡ä¸­â€œä¸¤ä¸ªç‹¬ç«‹è®­ç»ƒæ¨¡å‹é¢„æµ‹ä¹‹å·®çš„æœŸæœ›å¹³æ–¹â€ä½œä¸ºåˆ†æ­§åº¦é‡ï¼Œåœ¨è¯æ˜ä¸­å°†ä¸¤ä¸ªæ¨¡å‹é”šå®šåˆ°å…¶å¹³å‡æ¨¡å‹ä»¥è·å¾—ä¸€èˆ¬æ€§åˆ†æ­§ä¸Šç•Œï¼›å…ˆåœ¨ä¸€ç»´å¹³æ–¹æŸå¤±ä¸‹æ¨å¯¼ï¼Œå†æ¨å¹¿åˆ°å¤šç»´å›å½’ä¸ä»»æ„å¼ºå‡¸æŸå¤±ï¼Œå¹¶åˆ†åˆ«å®ä¾‹åŒ–åˆ°stackingã€gradient boostingã€å¸¦æ¶æ„æœç´¢çš„ç¥ç»ç½‘ç»œè®­ç»ƒã€å›ºå®šæ·±åº¦å›å½’æ ‘è®­ç»ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šä½œè€…ç»™å‡ºå››ç±»ç®—æ³•çš„åˆ†æ­§æ”¶æ•›ç»“è®ºï¼šstackingéšé›†æˆæ¨¡å‹æ•°kå¢å¤§ã€boostingéšè¿­ä»£æ¬¡æ•°kå¢å¤§ã€æ¶æ„æœç´¢éšå€™é€‰æ¶æ„è§„æ¨¡nå¢å¤§ã€å›å½’æ ‘éšæ·±åº¦då¢å¤§ï¼Œæ¨¡å‹åˆ†æ­§å‡å¯è¢«é©±åŠ¨è¶‹è¿‘0ï¼›ä¸”è¿™äº›ç»“æœä¸å±€é™äºå¹³æ–¹æŸå¤±çš„ä¸€ç»´è®¾å®šï¼Œå¯æ¨å¹¿åˆ°æ›´ä¸€èˆ¬çš„å¼ºå‡¸æŸå¤±å¤šç»´å›å½’ã€‚

**å…³é”®è¯**ï¼šæ¨¡å‹åˆ†æ­§, ç‹¬ç«‹è®­ç»ƒ, é¢„æµ‹å·®å¼‚å¹³æ–¹æœŸæœ›, åˆ†æ­§ä¸Šç•Œ, ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰, å›å½’æ ‘æ·±åº¦, å¼ºå‡¸æŸå¤±, å¤šç»´å›å½’

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23360v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23360v1.pdf)

---

## [21. A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358v1)

**ä½œè€…**ï¼šElad Kimchi Shoshani, Leeyam Gabay, Yedid Hoshen  
**åˆ†ç±»**ï¼šcs.LG, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šPLADA é€šè¿‡åªä¼ è¾“å‚è€ƒæ•°æ®é›†ä¸­ç‰¹å®šå›¾ç‰‡çš„ç±»åˆ«æ ‡ç­¾ï¼ˆä¸ä¼ åƒç´ ï¼‰ï¼Œåœ¨å°äº 1MB çš„é€šä¿¡å¼€é”€ä¸‹å®ç°è·¨ä»»åŠ¡çš„é«˜ç²¾åº¦åˆ†ç±»çŸ¥è¯†ä¼ é€’ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ•°æ®é›†æœåŠ¡å™¨å‘å¤šå®¢æˆ·ç«¯åˆ†å‘å¤§è§„æ¨¡åŸå§‹æ•°æ®é€šä¿¡æˆæœ¬æé«˜ï¼Œè€Œç›´æ¥ä¸‹å‘é¢„è®­ç»ƒæ¨¡å‹åˆå› å®¢æˆ·ç«¯è½¯ç¡¬ä»¶å·®å¼‚å¸¸ä¸å¯è¡Œï¼›ç°æœ‰æ•°æ®è’¸é¦æ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡ä¸”å‹ç¼©ç‡ä¸è¶³ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå‡è®¾å®¢æˆ·ç«¯å·²é¢„ç½®ä¸€ä¸ªå¤§å‹é€šç”¨æ— æ ‡æ³¨å‚è€ƒæ•°æ®é›†ï¼ŒæœåŠ¡å™¨ç”¨â€œä¼ªæ ‡ç­¾å³æ•°æ®â€çš„æ–¹å¼ä»…å‘é€éƒ¨åˆ†å‚è€ƒå›¾åƒçš„ç±»åˆ«æ ‡ç­¾æ¥è¡¨è¾¾æ–°ä»»åŠ¡ï¼›å¹¶ç”¨å‰ªæ/ç­›é€‰æœºåˆ¶é€‰å‡ºä¸ç›®æ ‡ä»»åŠ¡è¯­ä¹‰æœ€ç›¸å…³çš„å‚è€ƒå­é›†ä»¥ç¼“è§£åˆ†å¸ƒä¸åŒ¹é…å¹¶è¿›ä¸€æ­¥é™ä½ä¼ è¾“é‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ 10 ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šï¼ŒPLADA ä»¥ä½äº 1MB çš„è½½è·å®ç°æœ‰æ•ˆä»»åŠ¡è¿ç§»å¹¶ä¿æŒè¾ƒé«˜åˆ†ç±»å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºé¢å‘é«˜æ•ˆæ•°æ®é›†æœåŠ¡çš„å®ç”¨æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šæ•°æ®é›†åˆ†å‘, é€šä¿¡å¼€é”€å‹ç¼©, æ ‡ç­¾ä¼ è¾“, ä¼ªæ ‡ç­¾æ•°æ®åŒ–ï¼ˆPLADAï¼‰, å‚è€ƒæ•°æ®é›†é¢„åŠ è½½, è¯­ä¹‰ç›¸å…³æ ·æœ¬ç­›é€‰, æ•°æ®é›†å‰ªæ, åˆ†å¸ƒåç§»é€‚é…, æ•°æ®é›†è’¸é¦, é«˜åˆ†è¾¨ç‡æ•°æ®å‹ç¼©, æœ¬åœ°è®­ç»ƒ

**è¯„åˆ†**ï¼š38

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23358v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23358v1.pdf)

---

## [22. SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353v1)

**ä½œè€…**ï¼šSimon Roschmann, Paul Krzakala, Sonia Mazelet ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSOTAlignæå‡ºä¸€ç§åŠç›‘ç£å¯¹é½æ¡†æ¶ï¼Œç”¨å°‘é‡å›¾æ–‡é…å¯¹æ•°æ®ç»“åˆå¤§é‡éé…å¯¹æ•°æ®ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“åœ¨å†»ç»“çš„è§†è§‰/è¯­è¨€ç¼–ç å™¨é—´å­¦ä¹ ç¨³å¥çš„è”åˆåµŒå…¥ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¯¹é½æ–¹æ³•å¤šä¾èµ–å¯¹æ¯”å­¦ä¹ ä¸æµ·é‡é…å¯¹å›¾æ–‡æ ·æœ¬ï¼Œç›‘ç£æˆæœ¬é«˜ï¼›ä½œè€…æ¢è®¨èƒ½å¦åœ¨æ˜¾è‘—æ›´å°‘é…å¯¹ç›‘ç£ä¸‹ä»è·å¾—æœ‰æ„ä¹‰çš„è·¨æ¨¡æ€å¯¹é½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä¸¤é˜¶æ®µï¼šå…ˆç”¨å°‘é‡é…å¯¹æ•°æ®è®­ç»ƒçº¿æ€§â€œæ•™å¸ˆâ€æ¢å¤ç²—å…±äº«å‡ ä½•å¹¶åˆå§‹åŒ–å¯¹é½ï¼›å†ç”¨åŸºäºæœ€ä¼˜ä¼ è¾“çš„æ•£åº¦åœ¨éé…å¯¹å›¾åƒä¸æ–‡æœ¬ä¸Šè¿›è¡Œç»†åŒ–ï¼Œå¯¹é½å…³ç³»ç»“æ„è€Œä¸è¿‡åº¦çº¦æŸç›®æ ‡è¡¨ç¤ºç©ºé—´ã€‚

**ä¸»è¦ç»“è®º**ï¼šSOTAlignèƒ½æœ‰æ•ˆåˆ©ç”¨éé…å¯¹æ•°æ®ï¼Œåœ¨ä¸åŒæ•°æ®é›†ä¸ä¸åŒç¼–ç å™¨ç»„åˆä¸Šå­¦ä¹ æ›´é²æ£’çš„è”åˆè¡¨ç¤ºï¼Œå¹¶æ˜¾è‘—ä¼˜äºçº¯ç›‘ç£åŠå…¶ä»–åŠç›‘ç£åŸºçº¿æ–¹æ³•ã€‚

**å…³é”®è¯**ï¼šåŠç›‘ç£å¯¹é½, è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹é½, å•æ¨¡æ€ç¼–ç å™¨, å†»ç»“é¢„è®­ç»ƒæ¨¡å‹, å›¾æ–‡å¯¹é½, éé…å¯¹æ•°æ®, å°‘æ ·æœ¬ç›‘ç£, æœ€ä¼˜ä¼ è¾“, æœ€ä¼˜ä¼ è¾“æ•£åº¦, æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶, è”åˆåµŒå…¥ç©ºé—´

**è¯„åˆ†**ï¼š29

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23353v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23353v1.pdf)

---

## [23. FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349v1)

**ä½œè€…**ï¼šJose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.   We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.   Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šFlashOptim é€šè¿‡æ”¹è¿›ä¸»æƒé‡åˆ‡åˆ†ä¸8-bitä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–ï¼Œå°†æ··åˆç²¾åº¦è®­ç»ƒçš„æ¯å‚æ•°å†…å­˜å ç”¨é™ä½50%ä»¥ä¸Šä¸”åŸºæœ¬ä¸æŸå¤±æ¨¡å‹æ•ˆæœã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ ‡å‡†æ··åˆç²¾åº¦è®­ç»ƒé™¤å‚æ•°å¤–è¿˜éœ€ä¿å­˜æ¢¯åº¦ä¸ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå¯¼è‡´æ¯å‚æ•°å†…å­˜å¼€é”€å¾ˆé«˜ï¼Œä½¿å¾—ä¸­å¤§å‹æ¨¡å‹åœ¨<100GBæ˜¾å­˜ä¸‹è®­ç»ƒ/å¾®è°ƒå›°éš¾ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼šåˆ©ç”¨æ›´ç´§çš„é‡åŒ–è¯¯å·®ä¸Šç•Œæ”¹è¿›master weight splittingï¼Œä»¥åŠè®¾è®¡compandingï¼ˆå‹æ‰©ï¼‰å‡½æ•°ä»¥æ˜¾è‘—é™ä½8-bitä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–è¯¯å·®ï¼›é…åˆ16-bitæ¢¯åº¦å°†AdamWä»16B/å‚æ•°é™è‡³7Bï¼ˆæ¢¯åº¦é‡Šæ”¾å¯åˆ°5Bï¼‰ï¼Œå¹¶å‡å°‘checkpointä½“ç§¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨SGDã€AdamWä¸Lionä¸Šï¼ŒFlashOptimåœ¨å¤šç§è§†è§‰ä¸è¯­è¨€åŸºå‡†ï¼ˆå«Llama-3.1-8Bå¾®è°ƒï¼‰ä¸­æœªè§‚å¯Ÿåˆ°å¯æµ‹çš„è´¨é‡é€€åŒ–ï¼ŒåŒæ—¶æ˜¾è‘—èŠ‚çœè®­ç»ƒä¸å­˜å‚¨å†…å­˜ã€‚

**å…³é”®è¯**ï¼šæ··åˆç²¾åº¦è®­ç»ƒ, è®­ç»ƒå†…å­˜ä¼˜åŒ–, æ¯å‚æ•°å†…å­˜å¼€é”€, ä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–, 8-bit é‡åŒ–, ä¸»æƒé‡åˆ†è£‚, é‡åŒ–è¯¯å·®ç•Œ, 16-bit æ¢¯åº¦, æ¢¯åº¦é‡Šæ”¾, æ¨¡å‹æ£€æŸ¥ç‚¹å‹ç¼©

**è¯„åˆ†**ï¼š37

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23349v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23349v1.pdf)

---

## [24. Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341v1)

**ä½œè€…**ï¼šAlkis Kalavasis, Anay Mehrotra, Manolis Zampetakis ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.DS, math.ST, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡ç ”ç©¶åœ¨â€œåªè§‚å¯Ÿåˆ°æ ·æœ¬è½åœ¨å“ªä¸ªåˆ†åŒºé›†åˆä¸­â€çš„ç²—ç²’åº¦è§‚æµ‹ä¸‹çš„é«˜æ–¯å‡å€¼ä¼°è®¡ï¼Œç»™å‡ºäº†å‡¸åˆ†åŒºä¸‹å‡å€¼å¯è¾¨è¯†æ€§çš„åˆ»ç”»ï¼Œå¹¶åœ¨å¯è¾¨è¯†æ—¶æä¾›äº†è®¡ç®—é«˜æ•ˆçš„ä¼°è®¡ç®—æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç²—æ•°æ®åœ¨æµ‹é‡å–æ•´ã€ä¼ æ„Ÿå™¨é™åˆ¶ç­‰åœºæ™¯ä¸­å¸¸è§ï¼Œä¼šå¯¼è‡´å‡å€¼å¯èƒ½æ— æ³•ä»è§‚æµ‹ä¸­å”¯ä¸€æ¢å¤ï¼›æ­¤å‰å·¥ä½œè™½æŒ‡å‡ºå‡¸åˆ†åŒºä¸‹å¯æ ·æœ¬é«˜æ•ˆã€éå‡¸æ—¶NPéš¾ï¼Œä½†å°šä¸æ¸…æ¥šâ€œä½•æ—¶å¯è¾¨è¯†â€ä»¥åŠâ€œå¯è¾¨è¯†æ—¶æ˜¯å¦èƒ½å¤šé¡¹å¼æ—¶é—´ä¼°è®¡â€ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä½œè€…åœ¨å‡¸åˆ†åŒºè®¾å®šä¸­æ¨å¯¼å‡å€¼å¯è¾¨è¯†çš„å……è¦æ¡ä»¶ï¼ˆä»åˆ†åŒºå‡ ä½•/ä¿¡æ¯é‡è§’åº¦åˆ»ç”»è§‚æµ‹å¯¹å‡å€¼çš„çº¦æŸå¼ºåº¦ï¼‰ï¼Œå¹¶åœ¨æ»¡è¶³å¯è¾¨è¯†æ¡ä»¶æ—¶è®¾è®¡å¤šé¡¹å¼æ—¶é—´çš„ä¼°è®¡æµç¨‹ï¼ˆåœ¨ç²—è§‚æµ‹è¯±å¯¼çš„çº¦æŸä¸‹è¿›è¡Œå¯è®¡ç®—çš„å‚æ•°æ¢å¤/ä¼˜åŒ–ï¼‰ã€‚

**ä¸»è¦ç»“è®º**ï¼šå·¥ä½œå®Œæ•´å›ç­”äº†ä¸¤å¤§å¼€æ”¾é—®é¢˜ï¼šåœ¨å‡¸åˆ†åŒºä¸‹ç»™å‡ºå¯è¾¨è¯†æ€§çš„ç²¾ç¡®åˆ»ç”»ï¼Œå¹¶è¯æ˜åœ¨å¯è¾¨è¯†æ—¶ä¸ä»…ç»Ÿè®¡ä¸Šå¯ä¼°è®¡ï¼Œè€Œä¸”å­˜åœ¨è®¡ç®—é«˜æ•ˆçš„ç®—æ³•å®ç°ï¼›åŒæ—¶ä¹Ÿå¼ºåŒ–äº†â€œç¼ºä¹ç»“æ„ï¼ˆå¦‚éå‡¸ï¼‰ä¼šå¯¼è‡´è®¡ç®—å›°éš¾â€çš„è¾¹ç•Œã€‚

**å…³é”®è¯**ï¼šç²—ç²’åº¦æ•°æ®, é›†åˆå€¼è§‚æµ‹, é«˜æ–¯å‡å€¼ä¼°è®¡, å¯è¯†åˆ«æ€§, å‡¸åˆ’åˆ†, ä½ä¿¡æ¯è§‚æµ‹, æ ·æœ¬å¤æ‚åº¦, è®¡ç®—é«˜æ•ˆç®—æ³•, NPéš¾åº¦, éƒ¨åˆ†ä¿¡æ¯å­¦ä¹ , èº«ä»½åæ–¹å·®é«˜æ–¯

**è¯„åˆ†**ï¼š18

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23341v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23341v1.pdf)

---

## [25. Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336v1)

**ä½œè€…**ï¼šCamilo Gomez, Pengyang Wang, Liansheng Tang  
**åˆ†ç±»**ï¼šcs.LG, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§é€šè¿‡å¯¹è¶…å•çº¯å½¢ï¼ˆhypersimplexï¼‰è¿›è¡Œå¯å¾®æŠ•å½±æ¥è¿‘ä¼¼é›¶ä¸€æŸå¤±çš„ç®—å­ Soft-Binary-Argmaxï¼Œå¹¶åœ¨å¤§æ‰¹é‡è®­ç»ƒä¸‹æå‡åˆ†ç±»æ³›åŒ–æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šé›¶ä¸€æŸå¤±ä¸åˆ†ç±»ç›®æ ‡æœ€ä¸€è‡´ä½†ä¸å¯å¯¼ï¼Œéš¾ä»¥ç”¨äºç«¯åˆ°ç«¯æ¢¯åº¦ä¼˜åŒ–ï¼›åŒæ—¶å¤§æ‰¹é‡è®­ç»ƒå¸¸å¸¦æ¥æ³›åŒ–ä¸‹é™ï¼Œéœ€è¦æ›´è´´åˆä»»åŠ¡ä¸”å…·ç»“æ„çº¦æŸçš„å¯å¾®ç»„ä»¶ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†â€œæ¥è¿‘äºŒå€¼/argmaxâ€çš„è¾“å‡ºå»ºæ¨¡ä¸ºåˆ° n,k-ç»´ hypersimplex çš„å¹³æ»‘ã€ä¿åºï¼ˆorder-preservingï¼‰çº¦æŸæŠ•å½±ï¼Œæ„é€  Soft-Binary-Argmax ä½œä¸ºé›¶ä¸€æŸå¤±çš„å¯å¾®è¿‘ä¼¼ï¼›æ¨å¯¼å…¶æ€§è´¨å¹¶ç»™å‡ºé«˜æ•ˆé›…å¯æ¯”è®¡ç®—ä»¥ä¾¿é›†æˆåˆ°äºŒåˆ†ç±»ä¸å¤šåˆ†ç±»ç½‘ç»œä¸­ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•å¯åœ¨ä¿æŒå¯å¾®è®­ç»ƒçš„åŒæ—¶æ›´å¥½å¯¹é½é›¶ä¸€åˆ†ç±»ç›®æ ‡ï¼Œå¹¶é€šè¿‡å¯¹logitsæ–½åŠ å‡ ä½•ä¸€è‡´æ€§çº¦æŸï¼Œåœ¨å¤§æ‰¹é‡è®­ç»ƒåœºæ™¯æ˜¾è‘—æ”¹å–„æ³›åŒ–ã€ç¼©å°å¤§æ‰¹é‡è®­ç»ƒçš„æ€§èƒ½å·®è·ã€‚

**å…³é”®è¯**ï¼šå¯å¾®é›¶ä¸€æŸå¤±, è¶…å•çº¯å½¢æŠ•å½±, ç»“æ„åŒ–ä¼˜åŒ–å±‚, çº¦æŸä¼˜åŒ–, é¡ºåºä¿æŒæŠ•å½±, é›…å¯æ¯”çŸ©é˜µè®¡ç®—, å¤§æ‰¹é‡è®­ç»ƒæ³›åŒ–, å‡ ä½•ä¸€è‡´æ€§çº¦æŸ, å¤šåˆ†ç±»å­¦ä¹ 

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23336v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23336v1.pdf)

---

## [26. ParamMem: Augmenting Language Agents with Parametric Reflective Memory](https://arxiv.org/abs/2602.23320v1)

**ä½œè€…**ï¼šTianjun Yao, Yongqiang Chen, Yujia Zheng ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.MA  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šParamMemé€šè¿‡æŠŠè·¨æ ·æœ¬çš„åæ€æ¨¡å¼ç¼–ç è¿›å‚æ•°åŒ–è®°å¿†ï¼Œå¹¶ç”¨å¯æ§é‡‡æ ·æå‡åæ€å¤šæ ·æ€§ï¼Œä»è€Œç³»ç»Ÿæ€§å¢å¼ºè¯­è¨€ä»£ç†åœ¨å¤šç±»æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è‡ªåæ€ä»£ç†å¸¸äº§ç”Ÿé‡å¤åæ€ï¼Œé™åˆ¶æ¨ç†å¢ç›Šï¼›ä½œè€…å®è¯å‘ç°â€œåæ€å¤šæ ·æ€§â€ä¸ä»»åŠ¡æˆåŠŸç‡æ˜¾è‘—æ­£ç›¸å…³ï¼Œå› æ­¤éœ€è¦æ›´ä¸°å¯Œçš„åæ€ä¿¡å·æ¥æºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºParamMemå‚æ•°åŒ–è®°å¿†æ¨¡å—ï¼Œå°†è·¨æ ·æœ¬åæ€è§„å¾‹å­¦ä¹ åˆ°æ¨¡å‹å‚æ•°ä¸­ï¼Œå¹¶é€šè¿‡æ¸©åº¦é‡‡æ ·ç”Ÿæˆå¤šæ ·åæ€ï¼›åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºParamAgentï¼Œå°†ParamMemä¸æƒ…æ™¯è®°å¿†ï¼ˆepisodicï¼‰åŠè·¨æ ·æœ¬è®°å¿†èåˆä»¥é©±åŠ¨è¿­ä»£æ±‚è§£ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†å’Œå¤šè·³é—®ç­”ä¸Šç›¸å¯¹SOTAåŸºçº¿å–å¾—ç¨³å®šæå‡ï¼›æ­¤å¤–ParamMemå…·å¤‡æ ·æœ¬æ•ˆç‡é«˜ã€å¯å®ç°å¼±åˆ°å¼ºè·¨å°ºåº¦è¿ç§»ã€ä¸”æ— éœ€æ›´å¼ºå¤–éƒ¨æ¨¡å‹å³å¯è‡ªæˆ‘æ”¹è¿›ç­‰ç‰¹æ€§ã€‚

**å…³é”®è¯**ï¼šåæ€å¤šæ ·æ€§, å‚æ•°åŒ–è®°å¿†, å‚æ•°è®°å¿†æ¨¡å—, è·¨æ ·æœ¬è®°å¿†, æƒ…æ™¯è®°å¿†, æ¸©åº¦é‡‡æ ·, å¼±åˆ°å¼ºè¿ç§», ä»£ç ç”Ÿæˆ, å¤šè·³é—®ç­”

**è¯„åˆ†**ï¼š42

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23320v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23320v1.pdf)

---

## [27. A Proper Scoring Rule for Virtual Staining](https://arxiv.org/abs/2602.23305v1)

**ä½œè€…**ï¼šSamuel Tonks, Steve Hood, Ryan Musso ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºç”¨ä¿¡æ¯å¢ç›Šï¼ˆIGï¼‰ä½œä¸ºä¸¥æ ¼é€‚å½“è¯„åˆ†è§„åˆ™ï¼Œé€ç»†èƒè¯„ä¼°è™šæ‹ŸæŸ“è‰²æ¨¡å‹è¾“å‡ºçš„åéªŒåˆ†å¸ƒè´¨é‡ï¼Œå¹¶æ­ç¤ºç°æœ‰æŒ‡æ ‡éš¾ä»¥å‘ç°çš„æ€§èƒ½å·®å¼‚ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè™šæ‹ŸæŸ“è‰²æ¨¡å‹å¯ä¸ºæ¯ä¸ªç»†èƒé¢„æµ‹ç”Ÿç‰©ç‰¹å¾çš„åéªŒåˆ†å¸ƒï¼Œä½†çœŸå®åéªŒä¸å¯å¾—ï¼Œç°æœ‰è¯„ä¼°å¤šåªçœ‹æ•°æ®é›†å±‚é¢çš„è¾¹ç¼˜åˆ†å¸ƒå‡†ç¡®æ€§ï¼Œæ— æ³•ç›´æ¥è¡¡é‡â€œé¢„æµ‹åéªŒæ˜¯å¦å¯é â€ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå°†ä¿¡æ¯å¢ç›Šï¼ˆIGï¼‰å¼•å…¥ä¸ºé€ç»†èƒçš„è¯„ä»·æ¡†æ¶ï¼ŒæŠŠæ¨¡å‹ç»™å‡ºçš„åéªŒé¢„æµ‹ä½œä¸ºè¢«è¯„åˆ†å¯¹è±¡ï¼›è¯æ˜IGæ˜¯ä¸¥æ ¼é€‚å½“è¯„åˆ†è§„åˆ™ï¼Œå…·æœ‰ç†è®ºå¯è§£é‡Šæ€§ï¼Œå¹¶åœ¨é«˜é€šé‡ç­›é€‰æ•°æ®ä¸Šå¯¹æ‰©æ•£ä¸GANæ¨¡å‹ä¸å¤šç§æŒ‡æ ‡è¿›è¡Œå¯¹æ¯”è¯„æµ‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šIGèƒ½å¤Ÿæ›´ç›´æ¥ã€å¯è§£é‡Šåœ°è¡¡é‡åéªŒé¢„æµ‹è´¨é‡ï¼Œå¹¶åœ¨å®é™…è¯„æµ‹ä¸­æ­ç¤ºæ‰©æ•£/GANç­‰æ¨¡å‹ä¹‹é—´æ˜¾è‘—å·®å¼‚ï¼Œè€Œè¿™äº›å·®å¼‚å¯èƒ½è¢«ä»…çœ‹è¾¹ç¼˜åˆ†å¸ƒæˆ–ä¼ ç»ŸæŒ‡æ ‡æ‰€æ©ç›–ã€‚

**å…³é”®è¯**ï¼šè™šæ‹ŸæŸ“è‰², é«˜é€šé‡ç­›é€‰, ç”Ÿæˆæ¨¡å‹è¯„æµ‹, åéªŒåˆ†å¸ƒé¢„æµ‹, ä¿¡æ¯å¢ç›Šè¯„åˆ†, ä¸¥æ ¼é€‚å½“è¯„åˆ†è§„åˆ™, ç»†èƒçº§è¯„ä¼°, ä¸ç¡®å®šæ€§è¯„ä¼°, Diffusion, ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ, è¾¹é™…åˆ†å¸ƒè¯„ä¼°, å¯è§£é‡Šæ€§æŒ‡æ ‡

**è¯„åˆ†**ï¼š23

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23305v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23305v1.pdf)

---

## [28. Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303v1)

**ä½œè€…**ï¼šIlya Balabin, Thomas M. Kaiser  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºâ€œæ¨æ–­åŠ›å­¦ï¼ˆinferential mechanicsï¼‰â€æ¡†æ¶ï¼Œå°†åŒ–å­¦ç”Ÿç‰©ç°è±¡çš„å› æœæœºåˆ¶ç»“æ„æ˜¾å¼å¼•å…¥æœºå™¨å­¦ä¹ ï¼Œä»¥å‡å°‘é»‘ç®±é¢„æµ‹å¹¶æå‡å¯¹éšè—æœºåˆ¶çš„èšç„¦ï¼ˆfocusï¼‰èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šè‡ªç„¶ç§‘å­¦ä¸­çš„MLæ¨¡å‹å¸¸è¢«å½“ä½œé»‘ç®±ä½¿ç”¨ï¼Œå¿½ç•¥æ•°æ®èƒŒåçš„å› æœç»“æ„ï¼Œå¯¼è‡´å¯¹æœºåˆ¶çš„è§£é‡Šä¸å¤–æ¨å­˜åœ¨æ ¹æœ¬ç¼ºé™·ã€‚ä½œè€…è®¤ä¸ºç°æœ‰â€œML+å› æœâ€è®¨è®ºç¼ºä¹ç»Ÿä¸€ä¸”è´´åˆåŒ–å­¦ç”Ÿç‰©å­¦çš„å½¢å¼åŒ–ç†è®ºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ„å»ºåŒ–å­¦ç”Ÿç‰©ç°è±¡çš„åŸºç¡€å› æœç»“æ„çš„å½¢å¼æ¡†æ¶ï¼Œå¹¶æå‡ºâ€œfocusâ€æ¦‚å¿µæ¥åˆ»ç”»ç®—æ³•ä»å¤§è§„æ¨¡æ•°æ®ä¸­æ”¶æ•›åˆ°éšè—ç”Ÿæˆæœºåˆ¶çš„èƒ½åŠ›ï¼›åŒæ—¶åœ¨AktæŠ‘åˆ¶å‰‚å®¶æ—ä¸Šç»™å‡ºåˆæ­¥éªŒè¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šä½œè€…ä¸»å¼ éœ€è¦ç”¨ç»Ÿä¸€çš„å› æœ-æ¦‚ç‡-åŒ–å­¦/ç”Ÿç‰©ç†è®ºæ¥çº æ­£åŒ–å­¦ç”Ÿç‰©MLä¸­çš„å› æœç¼ºé™·ï¼Œå¹¶ä»¥â€œfocusâ€ä½œä¸ºè¿æ¥æœºåˆ¶å»ºæ¨¡ä¸MLçš„å…³é”®æŒ‡æ ‡ï¼›åˆæ­¥å®éªŒè¡¨æ˜è¯¥æ¡†æ¶å¯ç”¨äºæ­ç¤º/é€¼è¿‘æ½œåœ¨æœºåˆ¶ï¼Œåç»­ä¸¤ç¯‡å°†è¿›ä¸€æ­¥å±•å¼€ç›¸ä¼¼æ€§ç†è®ºä¸æ›´å¹¿æ³›å®è¯ã€‚

**å…³é”®è¯**ï¼šå› æœç»“æ„å»ºæ¨¡, æœºåˆ¶å› æœç†è®º, åŒ–å­¦ç”Ÿç‰©å­¦å»ºæ¨¡, é»‘ç›’é¢„æµ‹å™¨å±€é™, éšè—æœºåˆ¶è¯†åˆ«, æ¦‚ç‡è®º-å› æœèåˆ, åŒ–å­¦ç›¸ä¼¼æ€§åº¦é‡, AktæŠ‘åˆ¶å‰‚æ¡ˆä¾‹, éè¿˜åŸä¸»ä¹‰æ¡†æ¶

**è¯„åˆ†**ï¼š24

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23303v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23303v1.pdf)

---

## [29. Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity](https://arxiv.org/abs/2602.23296v1)

**ä½œè€…**ï¼šQuang-Huy Nguyen, Jiaqi Wang, Wei-Shinn Ku  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºFedWQ-CPï¼šåœ¨æ•°æ®ä¸æ¨¡å‹åŒé‡å¼‚è´¨çš„è”é‚¦å­¦ä¹ ä¸­ï¼Œç”¨ä¸€æ¬¡é€šä¿¡çš„å…±å½¢æ ¡å‡†å®ç°å¯é çš„ä¸ç¡®å®šæ€§é‡åŒ–å¹¶å¾—åˆ°æ›´ç´§çš„é¢„æµ‹åŒºé—´/é›†åˆã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è”é‚¦UQæ–¹æ³•å¤šåªå¤„ç†æ•°æ®å¼‚è´¨æˆ–æ¨¡å‹å¼‚è´¨ä¹‹ä¸€ï¼Œå¿½è§†äºŒè€…å åŠ ä¼šå¯¼è‡´å„å®¢æˆ·ç«¯è¦†ç›–ç‡ä¸ç¨³å®šï¼Œè¿›è€Œåœ¨èµ„æºä¸è¶³èŠ‚ç‚¹å‡ºç°â€œè¿‡åº¦è‡ªä¿¡â€çš„é™é»˜å¤±è´¥ã€‚éœ€è¦ä¸€ç§åˆ†å¸ƒæ— å…³ä¸”é€šä¿¡é«˜æ•ˆçš„UQæ–¹æ¡ˆï¼ŒåŒæ—¶å…¼é¡¾å…¨å±€ä¸å®¢æˆ·ç«¯å±‚é¢çš„è¦†ç›–å¯é æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæ¯ä¸ªå®¢æˆ·ç«¯åœ¨æœ¬åœ°æ ¡å‡†é›†ä¸Šè®¡ç®—å…±å½¢ä¸€è‡´æ€§åˆ†æ•°å¹¶å¾—åˆ°æœ¬åœ°åˆ†ä½æ•°é˜ˆå€¼ï¼Œåªä¸Šä¼ â€œé˜ˆå€¼+æ ¡å‡†æ ·æœ¬é‡â€åˆ°æœåŠ¡å™¨ï¼›æœåŠ¡å™¨ç”¨æ ·æœ¬é‡åŠ æƒå¹³å‡èšåˆé˜ˆå€¼ç”Ÿæˆå…¨å±€é˜ˆå€¼ï¼Œä»è€Œåœ¨å•è½®é€šä¿¡å†…å®Œæˆå®¢æˆ·ç«¯-æœåŠ¡å™¨ååŒæ ¡å‡†å¹¶ç”¨äºåˆ†ç±»/å›å½’é¢„æµ‹é›†æ„é€ ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨7ä¸ªå…¬å¼€æ•°æ®é›†çš„åˆ†ç±»ä¸å›å½’å®éªŒä¸­ï¼ŒFedWQ-CPåœ¨ä¿æŒå®¢æˆ·ç«¯çº§ä¸å…¨å±€ç»éªŒè¦†ç›–ç‡çš„åŒæ—¶ï¼Œäº§ç”Ÿæœ€å°çš„é¢„æµ‹é›†åˆæˆ–åŒºé—´ï¼Œå…¼å…·å¯é æ€§ä¸æ•ˆç‡ã€‚

**å…³é”®è¯**ï¼šè”é‚¦å­¦ä¹ , ä¸ç¡®å®šæ€§é‡åŒ–, ä¿å½¢é¢„æµ‹, åˆ†å¸ƒæ— å…³ç½®ä¿¡åŒºé—´, åŒé‡å¼‚è´¨æ€§, è¦†ç›–ç‡ä¿è¯, ä»£ç†-æœåŠ¡å™¨æ ¡å‡†, å•è½®é€šä¿¡, åŠ æƒåˆ†ä½æ•°èšåˆ, é¢„æµ‹é›†åˆ, é¢„æµ‹åŒºé—´

**è¯„åˆ†**ï¼š34

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23296v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23296v1.pdf)

---

## [30. Physics Informed Viscous Value Representations](https://arxiv.org/abs/2602.23280v1)

**ä½œè€…**ï¼šHrishikesh Viswanath, Juanwu Lu, S. Talha Bukhari ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.RO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-26

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºä¸€ç§åŸºäºHJBæ–¹ç¨‹ç²˜æ€§è§£çš„ç‰©ç†ä¿¡æ¯æ­£åˆ™ï¼Œç”¨Feynman-Kacå°†PDEè§£è½¬ä¸ºå¯è’™ç‰¹å¡æ´›ä¼°è®¡çš„æœŸæœ›ï¼Œä»è€Œæå‡ç¦»çº¿ç›®æ ‡æ¡ä»¶RLä¸­çš„ä»·å€¼ä¼°è®¡ä¸å‡ ä½•ä¸€è‡´æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç¦»çº¿GCRLç”±äºæ•°æ®è¦†ç›–ä¸è¶³ï¼Œä»·å€¼å‡½æ•°åœ¨æœªè¦†ç›–çŠ¶æ€-åŠ¨ä½œåŒºåŸŸæ˜“äº§ç”Ÿè¯¯å·®ï¼›ç°æœ‰åŸºäºä¸€é˜¶PDEï¼ˆå¦‚Eikonalï¼‰çš„ç‰©ç†æ­£åˆ™åœ¨é«˜ç»´å¤æ‚ç¯å¢ƒä¸­å¸¸å‡ºç°ä¸é€‚å®šä¸è®­ç»ƒä¸ç¨³å®šã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šä»æœ€ä¼˜æ§åˆ¶ç†è®ºå‡ºå‘ï¼Œæ„é€ æºè‡ªHamilton-Jacobi-Bellmanæ–¹ç¨‹ç²˜æ€§è§£çš„æ­£åˆ™é¡¹ï¼Œåœ¨ä»·å€¼è¿­ä»£ä¸­å¯¹æ›´æ–°è¿›è¡Œæ˜¾å¼çº¦æŸä¸ç•Œå®šï¼›å†ç”¨Feynman-Kacå®šç†æŠŠPDEæ±‚è§£æ”¹å†™ä¸ºæœŸæœ›å½¢å¼ï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡ç›®æ ‡ä»¥é¿å…é«˜é˜¶æ¢¯åº¦å¸¦æ¥çš„æ•°å€¼ä¸ç¨³å®šã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½æ”¹å–„ä»·å€¼è¡¨ç¤ºçš„å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶åœ¨å¯¼èˆªä¸é«˜ç»´å¤æ‚æ“ä½œä»»åŠ¡ä¸Šå¸¦æ¥æ›´å¥½çš„ç¦»çº¿GCRLæ€§èƒ½ä¸é€‚ç”¨æ€§ã€‚

**å…³é”®è¯**ï¼šç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ , ä»·å€¼å‡½æ•°ä¼°è®¡, ç‰©ç†ä¿¡æ¯æ­£åˆ™åŒ–, é»æ€§è§£, PDEçº¦æŸå­¦ä¹ , è’™ç‰¹å¡æ´›ä¼°è®¡, æœ€ä¼˜æ§åˆ¶ç†è®º, é«˜ç»´æœºå™¨äººå¯¼èˆª, é«˜ç»´æœºå™¨äººæ“æ§

**è¯„åˆ†**ï¼š26

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.23280v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.23280v1.pdf)

---

