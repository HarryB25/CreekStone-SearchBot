# arXiv AI è®ºæ–‡æ—¥æŠ¥ | 2026-02-11

> å…± 30 ç¯‡è®ºæ–‡ï¼Œç”±AIè‡ªåŠ¨æ€»ç»“

## ğŸ“‘ ç›®å½•

- [cs.LG](#csLG) (12 ç¯‡)
- [cs.CV](#csCV) (12 ç¯‡)
- [cs.CL](#csCL) (3 ç¯‡)
- [cs.AI](#csAI) (3 ç¯‡)

---

## cs.AI

## [1. Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.10090v1)

**ä½œè€…**ï¼šZhaoyang Wang, Canwen Xu, Boyi Liu ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI, cs.CL, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„Agent World Modelï¼ˆAWMï¼‰ç¯å¢ƒç”Ÿæˆç®¡é“ï¼Œæ”¯æŒå¤šè½®å·¥å…·ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ ï¼Œæä¾›ä¸°å¯Œçš„åˆæˆç¯å¢ƒã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç›®å‰çš„è‡ªä¸»æ™ºèƒ½ä½“è®­ç»ƒå—åˆ°ç¼ºä¹å¤šæ ·åŒ–å’Œå¯é ç¯å¢ƒçš„é™åˆ¶ï¼Œå½±å“äº†å…¶æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šAWMç”Ÿæˆ1000ä¸ªåˆæˆç¯å¢ƒï¼Œåˆ©ç”¨ä»£ç é©±åŠ¨å’Œæ•°æ®åº“æ”¯æŒï¼Œæä¾›é«˜è´¨é‡è§‚å¯Ÿå’Œä¸€è‡´çš„çŠ¶æ€è½¬ç§»ï¼Œæå‡æ™ºèƒ½ä½“äº¤äº’æ•ˆç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨åˆæˆç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒå¯è·å¾—è‰¯å¥½çš„è·¨åˆ†å¸ƒæ³›åŒ–æ•ˆæœï¼Œä¼˜äºç‰¹å®šåŸºå‡†çš„è®­ç»ƒæ–¹æ³•ã€‚

**å…³é”®è¯**ï¼šä»£ç†ä¸–ç•Œæ¨¡å‹, è‡ªä¸»ä»£ç†, å¼ºåŒ–å­¦ä¹ , ç¯å¢ƒç”Ÿæˆ, å¤šè½®äº¤äº’, ä»£ç é©±åŠ¨, è§‚å¯Ÿè´¨é‡, å¥–åŠ±å‡½æ•°, synthetic environments, agent interaction

**è¯„åˆ†**ï¼š71

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10090v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10090v1.pdf)

---

## [2. CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs](https://arxiv.org/abs/2602.10085v1)

**ä½œè€…**ï¼šRichard Bornemann, Pierluigi Vito Amadori, Antoine Cully  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†CODE-SHARPæ¡†æ¶ï¼Œé€šè¿‡åŸºç¡€æ¨¡å‹å®ç°å¼€æ”¾å¼æŠ€èƒ½çš„å‘ç°ä¸æ¼”åŒ–ï¼Œä»¥è§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡é™åˆ¶ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¼€å‘èƒ½å¤Ÿå¼€æ”¾åœ°å‘ç°å’Œå­¦ä¹ æ–°æŠ€èƒ½çš„æ™ºèƒ½ä½“æ˜¯äººå·¥æ™ºèƒ½ä¸­çš„ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è®¾è®¡å¥–åŠ±å‡½æ•°æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šCODE-SHARPæ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹æ‰©å±•å’Œä¼˜åŒ–å±‚æ¬¡åŒ–æŠ€èƒ½æ¡£æ¡ˆï¼Œæ„å»ºå¯æ‰§è¡Œå¥–åŠ±å‡½æ•°çš„æœ‰å‘å›¾ã€‚

**ä¸»è¦ç»“è®º**ï¼šé«˜æ°´å¹³çš„FMè§„åˆ’å™¨ç»“åˆå‘ç°çš„æŠ€èƒ½ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨Craftaxç¯å¢ƒä¸­è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›è¶…è¶Šäº†é¢„è®­ç»ƒæ™ºèƒ½ä½“å’Œä»»åŠ¡ç‰¹å®šä¸“å®¶ç­–ç•¥ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , ä»£ç†, åŸºç¡€æ¨¡å‹, å±‚æ¬¡å¥–åŠ±, ä»»åŠ¡è§„åˆ’, å¤æ‚æŠ€èƒ½, å¼€æ”¾å¼å‘ç°, æŠ€èƒ½æ¼”åŒ–, ä»£ç æ‰§è¡Œ, artificial intelligence

**è¯„åˆ†**ï¼š73

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10085v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10085v1.pdf)

---

## [3. Chain of Mindset: Reasoning with Adaptive Cognitive Modes](https://arxiv.org/abs/2602.10063v1)

**ä½œè€…**ï¼šTianyi Jiang, Arctanx An, Hengyi Feng ç­‰ 15 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶Chain of Mindsetï¼Œé€šè¿‡é€‚åº”æ€§æ€ç»´æ¨¡å¼æå‡æ¨ç†èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨å•ä¸€æ€ç»´æ¨¡å¼ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸åŒé˜¶æ®µæ‰€éœ€çš„å¤šæ ·åŒ–æ€ç»´èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†æ™ºèƒ½æ°´å¹³çš„æå‡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šChain of Mindset (CoM) å°†æ¨ç†åˆ†è§£ä¸ºå››ç§åŠŸèƒ½å¼‚è´¨çš„æ€ç»´æ¨¡å¼ï¼Œå¹¶é€šè¿‡Meta-AgentåŠ¨æ€é€‰æ‹©æœ€ä¼˜æ¨¡å¼ï¼ŒåŒæ—¶åˆ©ç”¨åŒå‘ä¸Šä¸‹æ–‡é—¨æ§åˆ¶æ¨¡å—é—´çš„ä¿¡æ¯æµåŠ¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šCoMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ•´ä½“å‡†ç¡®ç‡è¶…è¶Šæœ€å¼ºåŸºçº¿ï¼Œä¸”åœ¨æ¨ç†æ•ˆç‡æ–¹é¢ä¿æŒå¹³è¡¡ï¼Œå±•ç°äº†å…¶æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šé“¾å¼æ€ç»´, é€‚åº”æ€§, è®¤çŸ¥æ¨¡å¼, è®­ç»ƒ-free, ä»£ç†æ¡†æ¶, LLM, reasoning, å¤šé‡æ€ç»´, æ•ˆç‡ä¼˜åŒ–, ä»£ç ç”Ÿæˆ

**è¯„åˆ†**ï¼š74

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10063v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10063v1.pdf)

---

## cs.CL

## [4. Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing](https://arxiv.org/abs/2602.10092v1)

**ä½œè€…**ï¼šMohamed Afane, Kayla Laufer, Wenqi Wei ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè®ºæ–‡æå‡ºäº†Quantum-AuditåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡å­è®¡ç®—ç†è§£ä¸Šçš„èƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰åŸºå‡†ç¼ºä¹å¯¹è¯­è¨€æ¨¡å‹åœ¨é‡å­è®¡ç®—æ¦‚å¿µç†è§£çš„ç³»ç»Ÿæ€§æµ‹é‡ï¼Œå› æ­¤éœ€è¦å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡è®¾è®¡åŒ…å«2700ä¸ªé—®é¢˜çš„åŸºå‡†ï¼Œè¯„ä¼°äº†26ä¸ªé¢†å…ˆç»„ç»‡çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“å®¶ç¼–å†™çš„é—®é¢˜å’ŒåŸºäºç ”ç©¶è®ºæ–‡ç”Ÿæˆçš„é—®é¢˜ã€‚

**ä¸»è¦ç»“è®º**ï¼šå°½ç®¡é¡¶å°–æ¨¡å‹çš„è¡¨ç°ä¼˜äºä¸“å®¶å¹³å‡æ°´å¹³ï¼Œä½†åœ¨è¯†åˆ«é”™è¯¯å‰ææ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¸”åœ¨é«˜çº§ä¸»é¢˜ä¸Šå‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚

**å…³é”®è¯**ï¼šé‡å­è®¡ç®—, è¯­è¨€æ¨¡å‹, æ·±åº¦å­¦ä¹ , è¯„ä¼°, ç†è®ºæ¦‚å¿µ, é‡å­å®¡è®¡, ç”Ÿæˆæ¨¡å‹, äººç±»å‚ä¸, æ„å›¾é¢„æµ‹, å¤šæ¨¡æ€, llm

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10092v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10092v1.pdf)

---

## [5. Anagent For Enhancing Scientific Table & Figure Analysis](https://arxiv.org/abs/2602.10081v1)

**ä½œè€…**ï¼šXuehang Guo, Zhiyong Lu, Tom Hope ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§å¤šä»£ç†æ¡†æ¶Anagentï¼Œä»¥æé«˜ç§‘å­¦è¡¨æ ¼å’Œå›¾å½¢åˆ†æçš„èƒ½åŠ›ï¼Œå…‹æœäº†ç°æœ‰AIç³»ç»Ÿåœ¨å¤æ‚æ€§å’Œä¸Šä¸‹æ–‡è¦æ±‚ä¸Šçš„å±€é™ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰AIç³»ç»Ÿåœ¨ç§‘å­¦ç ”ç©¶ä¸­å¯¹å¤æ‚å¤šæ¨¡æ€çŸ¥è¯†çš„è§£è¯»å­˜åœ¨å›°éš¾ï¼Œéœ€è¦æ›´å¥½åœ°æ•´åˆä¸åŒæ¥æºçš„è¯æ®å¹¶è¿›è¡Œé¢†åŸŸç‰¹å®šæ¨ç†ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šAnagenté€šè¿‡å››ä¸ªä¸“ä¸šä»£ç†ï¼ˆPlannerã€Expertã€Solverå’ŒCriticï¼‰æ¥åˆ†è§£ä»»åŠ¡ã€æ£€ç´¢ä¿¡æ¯ã€åˆæˆåˆ†æå¹¶è¿›è¡Œè´¨é‡è¯„ä¼°ï¼ŒåŒæ—¶é‡‡ç”¨æ¨¡å—åŒ–è®­ç»ƒç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šAnagentåœ¨170ä¸ªå­é¢†åŸŸçš„è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹å–„ï¼Œè¯æ˜ä»»åŠ¡å¯¼å‘æ¨ç†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥é—®é¢˜è§£å†³æ˜¯é«˜è´¨é‡ç§‘å­¦è¡¨æ ¼å’Œå›¾å½¢åˆ†æçš„å…³é”®ã€‚

**å…³é”®è¯**ï¼šå¤šæ™ºèƒ½ä½“, ç§‘å­¦åˆ†æ, æ·±åº¦å­¦ä¹ , ä»»åŠ¡åˆ†è§£, æ¨¡å—åŒ–è®­ç»ƒ, å¼ºåŒ–å­¦ä¹ , ä¿¡æ¯æ£€ç´¢, è´¨é‡è¯„ä¼°, ä¸Šä¸‹æ–‡æ„ŸçŸ¥, artificial intelligence

**è¯„åˆ†**ï¼š73

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10081v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10081v1.pdf)

---

## [6. SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation](https://arxiv.org/abs/2602.10017v1)

**ä½œè€…**ï¼šHomaira Huda Shomee, Rochana Chaturvedi, Yangxinyu Xie ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CL  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§å¤šç»´åº¦çš„æ— å‚è€ƒè¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©é¢†åŸŸçš„å›ç­”è´¨é‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰è¯„ä¼°æ¡†æ¶ä¸»è¦ä¾èµ–è¡¨é¢ç›¸ä¼¼æ€§ï¼Œæœªèƒ½æœ‰æ•ˆè¯„ä¼°é¢†åŸŸç‰¹å®šå†³ç­–æ‰€éœ€çš„å…·ä½“ä¿¡æ¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºäº†ä¸€ä¸ªåŸºäºå››ä¸ªç»´åº¦ï¼ˆç‰¹å¼‚æ€§ã€é²æ£’æ€§ã€ç­”æ¡ˆç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡åˆ©ç”¨ï¼‰çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«1,412ä¸ªé¢†åŸŸç‰¹å®šé—®ç­”å¯¹çš„æ•°æ®é›†ã€‚

**ä¸»è¦ç»“è®º**ï¼šå•ä¸€æŒ‡æ ‡ä¸è¶³ä»¥å…¨é¢æ•æ‰ç­”æ¡ˆè´¨é‡ï¼Œå¼ºè°ƒäº†åœ¨é«˜é£é™©åº”ç”¨ä¸­éœ€è¦ç»“æ„åŒ–çš„å¤šæŒ‡æ ‡è¯„ä¼°æ¡†æ¶ã€‚

**å…³é”®è¯**ï¼šæ·±åº¦å­¦ä¹ , è¯­è¨€æ¨¡å‹, ç”Ÿæˆå¼, è¯­ä¹‰æœç´¢, å¤šç»´è¯„ä¼°, é¢†åŸŸç‰¹å®š, å‚è€ƒæ— å…³, é—®ç­”ç³»ç»Ÿ, è¯„ä¼°æ¡†æ¶, llm

**è¯„åˆ†**ï¼š65

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10017v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10017v1.pdf)

---

## cs.CV

## [7. SAGE: Scalable Agentic 3D Scene Generation for Embodied AI](https://arxiv.org/abs/2602.10116v1)

**ä½œè€…**ï¼šHongchi Xia, Xuan Li, Zhaoshuo Li ç­‰ 12 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.RO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šSAGEæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œè‡ªåŠ¨ç”Ÿæˆç¬¦åˆç”¨æˆ·æŒ‡å®šä»»åŠ¡çš„3Dåœºæ™¯ï¼Œä»¥ä¿ƒè¿›ä½“æ€AIçš„è®­ç»ƒå’Œåº”ç”¨ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ”¶é›†çœŸå®ä¸–ç•Œæ•°æ®å¯¹äºä½“æ€ä»£ç†è€Œè¨€æˆæœ¬é«˜ä¸”å­˜åœ¨å®‰å…¨é£é™©ï¼Œå› æ­¤éœ€è¦å¯æ‰©å±•ã€ç°å®ä¸”é€‚ç”¨äºæ¨¡æ‹Ÿçš„3Dç¯å¢ƒã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šSAGEç»“åˆå¤šä¸ªç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œé€šè¿‡è¿­ä»£æ¨ç†å’Œè‡ªé€‚åº”å·¥å…·é€‰æ‹©ï¼Œè‡ªåŠ¨ç”Ÿæˆæ»¡è¶³ç”¨æˆ·æ„å›¾å’Œç‰©ç†æœ‰æ•ˆæ€§çš„åœºæ™¯ã€‚

**ä¸»è¦ç»“è®º**ï¼šä½¿ç”¨SAGEç”Ÿæˆçš„æ•°æ®è®­ç»ƒçš„ç­–ç•¥è¡¨ç°å‡ºæ˜æ˜¾çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„å¯¹è±¡å’Œå¸ƒå±€ä¸Šè¿›è¡Œæ³›åŒ–ï¼Œå±•ç¤ºäº†åŸºäºæ¨¡æ‹Ÿçš„æ‰©å±•æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šåœºæ™¯ç”Ÿæˆ, ä»£ç†æ¡†æ¶, 3Dç¯å¢ƒ, è¯­ä¹‰å¯è¡Œæ€§, è‡ªé€‚åº”å·¥å…·é€‰æ‹©, embodied AI, simulation-ready, æ„å›¾ç†è§£, è¿­ä»£æ¨ç†

**è¯„åˆ†**ï¼š75

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10116v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10116v1.pdf)

---

## [8. Quantum Multiple Rotation Averaging](https://arxiv.org/abs/2602.10115v1)

**ä½œè€…**ï¼šShuteng Wang, Natacha Kuete Meli, Michael MÃ¶ller ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé‡å­é€€ç«çš„å¤šé‡æ—‹è½¬å¹³å‡ç®—æ³•IQARSï¼Œèƒ½å¤Ÿåœ¨é«˜å™ªå£°æƒ…å†µä¸‹æ›´å‡†ç¡®åœ°æ¢å¤ç»å¯¹æ—‹è½¬ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿçš„å¤šé‡æ—‹è½¬å¹³å‡æ–¹æ³•åœ¨é«˜å™ªå£°ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼ŒäºŸéœ€ä¸€ç§æ–°æ–¹æ³•ä»¥å…‹æœå±€éƒ¨æœ€å°å€¼å’Œå‡ ä½•å¤±çœŸé—®é¢˜ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šIQARSé€šè¿‡å°†å¤šé‡æ—‹è½¬å¹³å‡é—®é¢˜é‡æ„ä¸ºå¯åœ¨é‡å­é€€ç«å™¨ä¸Šæ‰§è¡Œçš„å±€éƒ¨äºŒæ¬¡éå‡¸å­é—®é¢˜ï¼Œåˆ©ç”¨é‡å­éš§ç©¿å’Œå¹¶è¡Œæ€§ä¼˜åŒ–è§£ç©ºé—´æ¢ç´¢ã€‚

**ä¸»è¦ç»“è®º**ï¼šå°½ç®¡å½“å‰çš„é‡å­é€€ç«å™¨æ€§èƒ½æœ‰é™ï¼Œä½†IQARSåœ¨D-Waveé€€ç«å™¨ä¸Šçš„å‡†ç¡®ç‡æ¯”ä¼ ç»Ÿæ–¹æ³•Shonané«˜å‡ºçº¦12%ã€‚

**å…³é”®è¯**ï¼šé‡å­, å¤šé‡æ—‹è½¬å¹³å‡, ä¼˜åŒ–é—®é¢˜, 3Dè§†è§‰, æœºå™¨äººæŠ€æœ¯, IQARS, é‡å­é€€ç«, éæ¬§å‡ é‡Œå¾—, æ—‹è½¬åŒæ­¥, è§£å†³æ–¹æ¡ˆæ¢ç´¢, rag

**è¯„åˆ†**ï¼š60

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10115v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10115v1.pdf)

---

## [9. ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation](https://arxiv.org/abs/2602.10113v1)

**ä½œè€…**ï¼šMingyang Wu, Ashirbad Mishra, Soumik Dey ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šConsID-Genæ˜¯ä¸€ç§æ–°é¢–çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å¤šè§†å›¾ä¸€è‡´æ€§å¢å¼ºè§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œè§£å†³äº†ç‰©ä½“èº«ä»½ä¿æŒå’Œè§†è§’å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç‰©ä½“èº«ä»½ä¿æŒå’Œå‡ ä½•æ‰­æ›²æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒäºŸéœ€æ”¹è¿›ä»¥é€‚åº”çœŸå®ä¸–ç•Œåœºæ™¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šConsID-Genæ¡†æ¶ç»“åˆäº†æœªå§¿æ€è¾…åŠ©è§†å›¾ï¼Œé€šè¿‡åŒæµè§†è§‰-å‡ ä½•ç¼–ç å™¨å’Œæ–‡æœ¬-è§†è§‰è¿æ¥å™¨ï¼Œæä¾›ç»Ÿä¸€çš„æ¡ä»¶è¾“å…¥ï¼Œå¢å¼ºäº†ç”Ÿæˆæ•ˆæœã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒConsID-Genåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå±•ç°å‡ºæ›´å¥½çš„èº«ä»½ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

**å…³é”®è¯**ï¼šå›¾åƒç”Ÿæˆ, è§†é¢‘ç”Ÿæˆ, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, ç”Ÿæˆæ¨¡å‹, è¯­ä¹‰æœç´¢, ä¸€è‡´æ€§, å¤šè§†è§’, æ•°æ®é›†, diffusion

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10113v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10113v1.pdf)

---

## [10. Olaf-World: Orienting Latent Actions for Video World Modeling](https://arxiv.org/abs/2602.10104v1)

**ä½œè€…**ï¼šYuxin Jiang, Yuchao Gu, Ivor W. Tsang ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Î”$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šOlaf-Worldé€šè¿‡å¼•å…¥Seq$Î”$-REPAæ–¹æ³•ï¼Œæå‡äº†æ— æ ‡ç­¾è§†é¢‘ä¸­è¡Œä¸ºæ§åˆ¶ä¸–ç•Œæ¨¡å‹çš„å­¦ä¹ æ•ˆæœï¼Œä½¿å¾—åŠ¨ä½œå¯ä»¥åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­æ›´å¥½åœ°è¿ç§»ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰è¡Œä¸ºæ§åˆ¶ä¸–ç•Œæ¨¡å‹å› ç¼ºä¹åŠ¨ä½œæ ‡ç­¾è€Œå—é™ï¼Œè€Œæ½œåœ¨åŠ¨ä½œå­¦ä¹ åœ¨æ— æ ‡ç­¾è§†é¢‘ä¸­æå–æ§åˆ¶æ¥å£çš„èƒ½åŠ›ä¸è¶³ä»¥æ”¯æŒè·¨ä¸Šä¸‹æ–‡çš„è¿ç§»ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºSeq$Î”$-REPAç›®æ ‡ï¼Œé€šè¿‡è§‚å¯Ÿåˆ°çš„è¯­ä¹‰æ•ˆåº”å¯¹é›†æˆçš„æ½œåœ¨åŠ¨ä½œè¿›è¡Œæ—¶é—´ç‰¹å¾å·®å¼‚çš„å¯¹é½ï¼Œä»è€Œåœ¨å¤§è§„æ¨¡è¢«åŠ¨è§†é¢‘ä¸­é¢„è®­ç»ƒè¡Œä¸ºæ¡ä»¶çš„è§†é¢‘ä¸–ç•Œæ¨¡å‹ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒOlaf-Worldå­¦ä¹ åˆ°äº†æ›´ç»“æ„åŒ–çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼Œæ˜¾è‘—æé«˜äº†é›¶-shotåŠ¨ä½œè¿ç§»èƒ½åŠ›å’Œå¯¹æ–°æ§åˆ¶æ¥å£çš„é€‚åº”æ•ˆç‡ã€‚

**å…³é”®è¯**ï¼šæ½œåœ¨åŠ¨ä½œ, åŠ¨ä½œæ§åˆ¶, è§†é¢‘å»ºæ¨¡, è‡ªç›‘ç£å­¦ä¹ , ç»“æ„åŒ–æ½œåœ¨ç©ºé—´, é›¶-shotè½¬ç§», æ•°æ®é«˜æ•ˆé€‚åº”, SeqÎ”-REPA, Olaf-World, context

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10104v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10104v1.pdf)

---

## [11. VideoWorld 2: Learning Transferable Knowledge from Real-world Videos](https://arxiv.org/abs/2602.10102v1)

**ä½œè€…**ï¼šZhongwei Ren, Yunchao Wei, Xiao Yu ç­‰ 8 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šVideoWorld 2é€šè¿‡åŠ¨æ€å¢å¼ºçš„æ½œåœ¨åŠ¨æ€æ¨¡å‹ä»çœŸå®ä¸–ç•Œè§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çŸ¥è¯†ï¼Œæ˜¾è‘—æé«˜ä»»åŠ¡æˆåŠŸç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ™ºèƒ½ä½“ä»æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ä¸­å­¦ä¹ å¯è½¬ç§»çŸ¥è¯†å¹¶åœ¨æ–°ç¯å¢ƒä¸­åº”ç”¨æ˜¯å…¶åŸºæœ¬èƒ½åŠ›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šVideoWorld 2å¼•å…¥åŠ¨æ€å¢å¼ºçš„æ½œåœ¨åŠ¨æ€æ¨¡å‹(dLDM)ï¼Œå°†åŠ¨ä½œåŠ¨æ€ä¸è§†è§‰å¤–è§‚è§£è€¦ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰å»ºæ¨¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šVideoWorld 2åœ¨å®é™…æ‰‹å·¥åˆ¶ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä»»åŠ¡æˆåŠŸç‡æå‡ï¼Œå±•ç°äº†ä»åŸå§‹è§†é¢‘ç›´æ¥å­¦ä¹ å¯è½¬ç§»çŸ¥è¯†çš„æ½œåŠ›ã€‚

**å…³é”®è¯**ï¼šè§†é¢‘, è§†é¢‘æ•°æ®, å­¦ä¹ , è½¬ç§»çŸ¥è¯†, åŠ¨æ€æ¨¡å‹, ä»»åŠ¡ç­–ç•¥, æœºå™¨äºº, Open-X, è§†é¢‘ç”Ÿæˆ, é•¿æœŸæ¨ç†, agent

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10102v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10102v1.pdf)

---

## [12. Causality in Video Diffusers is Separable from Denoising](https://arxiv.org/abs/2602.10095v1)

**ä½œè€…**ï¼šXingjian Bai, Guande He, Zhengqi Li ç­‰ 6 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.AI, cs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¶æ„ï¼Œå°†å› æœæ¨ç†ä¸å¤šæ­¥éª¤å»å™ªè¿‡ç¨‹åˆ†ç¦»ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰å› æœæ‰©æ•£æ¨¡å‹å°†æ—¶é—´æ¨ç†ä¸å»å™ªè¿‡ç¨‹æ··åˆï¼Œå¯¼è‡´å†—ä½™è®¡ç®—å’Œæ•ˆç‡ä½ä¸‹ï¼Œå› æ­¤éœ€è¦å¯»æ‰¾åˆ†ç¦»è¿™ä¸¤è€…çš„æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºäº†å¯åˆ†ç¦»å› æœæ‰©æ•£ï¼ˆSCDï¼‰æ¶æ„ï¼Œé€šè¿‡å› æœå˜æ¢å™¨ç¼–ç å™¨è§£è€¦æ¯å¸§çš„æ—¶é—´æ¨ç†ä¸è½»é‡çº§å»å™ªè§£ç å™¨çš„å¸§æ¸²æŸ“ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼ŒSCDåœ¨åˆæˆå’ŒçœŸå®åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ååé‡å’Œæ¯å¸§å»¶è¿Ÿï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¸å¼ºåŸºçº¿ç›¸åª²ç¾æˆ–è¶…è¿‡å…¶æ€§èƒ½ã€‚

**å…³é”®è¯**ï¼šå› æœå…³ç³», è§†é¢‘æ‰©æ•£, ç”Ÿæˆæ¨¡å‹, æ·±åº¦å­¦ä¹ , å˜æ¢å™¨, è‡ªå›å½’, å¤„ç†æ•ˆç‡, è®¡ç®—å†—ä½™, é€å¸§æ¸²æŸ“, generative

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10095v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10095v1.pdf)

---

## [13. 4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere](https://arxiv.org/abs/2602.10094v1)

**ä½œè€…**ï¼šYihang Luo, Shangchen Zhou, Yushi Lan ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼š4RCæ˜¯ä¸€ç§ç»Ÿä¸€çš„å‰é¦ˆæ¡†æ¶ï¼Œå¯ä»¥ä»å•ç›®è§†é¢‘ä¸­è¿›è¡Œ4Dé‡å»ºï¼Œæ•æ‰å¯†é›†åœºæ™¯å‡ ä½•å’Œè¿åŠ¨åŠ¨æ€ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰æ–¹æ³•é€šå¸¸å°†è¿åŠ¨ä¸å‡ ä½•åˆ†ç¦»æˆ–ç”Ÿæˆæœ‰é™çš„4Då±æ€§ï¼Œæ— æ³•å…¨é¢æ•è·åœºæ™¯ä¿¡æ¯ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–°çš„æ–¹æ³•æ¥å®ç°æ›´å®Œæ•´çš„4Dé‡å»ºã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼š4RCé‡‡ç”¨äº†ä¸€ç§æ–°çš„ç¼–ç ä¸€æ¬¡ã€éšæ—¶éšåœ°æŸ¥è¯¢çš„èŒƒå¼ï¼Œé€šè¿‡å˜å‹å™¨éª¨å¹²ç½‘ç»œå°†æ•´ä¸ªè§†é¢‘ç¼–ç ä¸ºç´§å‡‘çš„æ—¶ç©ºæ½œåœ¨ç©ºé—´ï¼Œå¹¶ä½¿ç”¨æ¡ä»¶è§£ç å™¨é«˜æ•ˆæŸ¥è¯¢3Då‡ ä½•å’Œè¿åŠ¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šå¤§é‡å®éªŒè¡¨æ˜ï¼Œ4RCåœ¨å¤šç§4Dé‡å»ºä»»åŠ¡ä¸­ä¼˜äºä¹‹å‰çš„å’ŒåŒæ—¶æœŸçš„æ–¹æ³•ã€‚

**å…³é”®è¯**ï¼š4Dé‡å»º, å˜æ¢å™¨, æ·±åº¦å­¦ä¹ , è¿åŠ¨åŠ¨æ€, åœºæ™¯å‡ ä½•, æ¡ä»¶æŸ¥è¯¢, spatio-temporal, ç»Ÿä¸€æ¡†æ¶, 4Dè¡¨ç¤º, transformer

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10094v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10094v1.pdf)

---

## [14. Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach](https://arxiv.org/abs/2602.10079v1)

**ä½œè€…**ï¼šSoumyaroop Nandi, Prem Natarajan  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šForensimæ˜¯ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›çš„çŠ¶æ€ç©ºé—´æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶æ£€æµ‹å›¾åƒæ‹¼æ¥å’Œå¤åˆ¶ç§»åŠ¨ä¼ªé€ ï¼Œæä¾›ç²¾ç¡®çš„æºç›®æ ‡åŒºåŸŸå®šä½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šä¼ ç»Ÿçš„ä¼ªé€ æ£€æµ‹æ–¹æ³•å¾€å¾€åªä¾èµ–ä¼ªé€ åŒºåŸŸçš„ä¼ªå½±ç‰¹å¾ï¼Œæ— æ³•å…¨é¢ç†è§£å›¾åƒä¸Šä¸‹æ–‡ï¼Œå°¤å…¶åœ¨ç‰¹å®šåœºæ™¯ä¸­å®¹æ˜“å¯¼è‡´è¯¯è§£ï¼Œå› æ­¤éœ€è¦è”åˆæºç›®æ ‡åŒºåŸŸçš„å®šä½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šForensimé‡‡ç”¨è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œç»“åˆå½’ä¸€åŒ–æ³¨æ„åŠ›å›¾å’ŒåŒºåŸŸå—æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥è¯†åˆ«å†…éƒ¨ç›¸ä¼¼æ€§å’ŒåŒºåˆ†è¢«æ“æ§åŒºåŸŸï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šForensimåœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶å‘å¸ƒäº†CMFD-Anythingæ•°æ®é›†ï¼Œä»¥è§£å†³ç°æœ‰å¤åˆ¶ç§»åŠ¨ä¼ªé€ æ•°æ®é›†çš„å±€é™æ€§ã€‚

**å…³é”®è¯**ï¼šå›¾åƒä¼ªé€ , å¤åˆ¶ç§»åŠ¨ä¼ªé€ , æ³¨æ„åŠ›æœºåˆ¶, çŠ¶æ€ç©ºé—´æ¨¡å‹, ç›®æ ‡åŒºåŸŸå®šä½, ç”Ÿæˆæ¨¡å‹, æ·±åº¦å­¦ä¹ , è¯­ä¹‰æœç´¢, ç«¯åˆ°ç«¯è®­ç»ƒ, rag

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10079v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10079v1.pdf)

---

## [15. Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving](https://arxiv.org/abs/2602.10052v1)

**ä½œè€…**ï¼šSerin Varghese, Kevin Ross, Fabian Hueger ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜è‡ªåŠ¨é©¾é©¶ä¸­è§†é¢‘è¯­ä¹‰åˆ†å‰²çš„æ—¶é—´ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„è§†é¢‘åˆ†å‰²æ¨¡å‹ç‹¬ç«‹å¤„ç†å¸§ï¼Œæœªèƒ½åˆ©ç”¨æ—¶é—´ä¸€è‡´æ€§ï¼Œå½±å“åŠ¨æ€åœºæ™¯ä¸­çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºçš„æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSTAï¼‰æ‰©å±•äº†å˜æ¢å™¨æ³¨æ„åŠ›å—ï¼Œé€šè¿‡å¤„ç†å¤šå¸§ä¸Šä¸‹æ–‡æ¥å¢å¼ºè§†é¢‘è¯­ä¹‰åˆ†å‰²çš„æ—¶ç©ºç‰¹å¾è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨Cityscapeså’ŒBDD100kæ•°æ®é›†ä¸Šï¼ŒSTAåœ¨æ—¶é—´ä¸€è‡´æ€§æŒ‡æ ‡ä¸Šæé«˜äº†9.20ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨å¹³å‡äº¤å¹¶æ¯”ä¸Šæé«˜äº†1.76ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯æ˜å…¶åœ¨è§†é¢‘è¯­ä¹‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**å…³é”®è¯**ï¼šæ—¶ç©ºæ³¨æ„åŠ›, æ·±åº¦å­¦ä¹ , è¯­ä¹‰åˆ†å‰², å˜æ¢å™¨, è‡ªåŠ¨é©¾é©¶, è§†é¢‘åˆ†æ, å¤šå¸§ä¸Šä¸‹æ–‡, è®¡ç®—æ•ˆç‡, ç»“æ„ä¼˜åŒ–, neural network

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10052v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10052v1.pdf)

---

## [16. Conformal Prediction Sets for Instance Segmentation](https://arxiv.org/abs/2602.10045v1)

**ä½œè€…**ï¼šKerri Lu, Dan M. Kluger, Stephen Bates ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV, cs.LG, stat.ME, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¬¦åˆé¢„æµ‹ç®—æ³•ï¼Œä¸ºå®ä¾‹åˆ†å‰²ç”Ÿæˆè‡ªé€‚åº”ç½®ä¿¡é›†ï¼Œä»¥é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„å®ä¾‹åˆ†å‰²æ¨¡å‹åœ¨å¹³å‡é¢„æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†ç¼ºä¹ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œå¯¼è‡´é¢„æµ‹çš„é®ç½©ä¸çœŸå®æƒ…å†µä¹‹é—´ç¼ºä¹ä¿è¯ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç¬¦åˆé¢„æµ‹ç®—æ³•ï¼Œé’ˆå¯¹ç»™å®šå›¾åƒå’Œåƒç´ åæ ‡ç”Ÿæˆå…·æœ‰é«˜IoUä¿è¯çš„å®ä¾‹é¢„æµ‹ç½®ä¿¡é›†ï¼Œå¹¶åº”ç”¨äºå†œä¸šã€ç»†èƒå’Œè½¦è¾†æ£€æµ‹ç­‰å®ä¾‹åˆ†å‰²ä»»åŠ¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•çš„é¢„æµ‹é›†åœ¨æŸ¥è¯¢éš¾åº¦ä¸Šè¡¨ç°å‡ºä¸åŒçš„è§„æ¨¡ï¼Œå¹¶ä¸”åœ¨è¦†ç›–ç‡ä¸Šä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œæä¾›äº†æ¸è¿‘å’Œæœ‰é™æ ·æœ¬ä¿è¯çš„ç®—æ³•ç‰ˆæœ¬ã€‚

**å…³é”®è¯**ï¼šå®ä¾‹åˆ†å‰², ç½®ä¿¡é›†, ä¸ç¡®å®šæ€§é‡åŒ–, é€‚åº”æ€§ç®—æ³•, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , è¯­ä¹‰æœç´¢, ç”Ÿæˆæ¨¡å‹, å†œä¸šå›¾åƒå¤„ç†, ç»†èƒåˆ†å‰², è½¦è¾†æ£€æµ‹, rag

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10045v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10045v1.pdf)

---

## [17. Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI](https://arxiv.org/abs/2602.10043v1)

**ä½œè€…**ï¼šGaurang Sharma, Harri Polonen, Juha Pajula ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.   Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ ‡å‡†å›¾åƒå¤„ç†å’Œç›¸ä¼¼æ€§è®¡ç®—ï¼Œåœ¨ä¸åŒæ•°æ®åº“ä¸­é“¾æ¥è„‘MRIæ•°æ®æ ·æœ¬ï¼Œä»¥åº”å¯¹éšç§é£é™©ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨ä¸¥æ ¼çš„æ³•è§„æ¡†æ¶ä¸‹ï¼Œè„‘MRIæ•°æ®çš„å…±äº«éœ€è¦å»é™¤æ½œåœ¨æ ‡è¯†ç¬¦ï¼Œä½†ä»å­˜åœ¨éšç§é£é™©ï¼Œå› æ­¤éœ€è¦è¯„ä¼°æ•°æ®é“¾æ¥å¯èƒ½å¸¦æ¥çš„é£é™©ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç ”ç©¶é€šè¿‡å¯¹å»é¢…éª¨çš„T1åŠ æƒMRIè¿›è¡Œæ ‡å‡†é¢„å¤„ç†ï¼Œå¹¶è®¡ç®—å›¾åƒç›¸ä¼¼æ€§ï¼ŒæˆåŠŸå®ç°äº†ä¸åŒæ—¶é—´ã€æ‰«æä»ªç±»å‹åŠé‡‡é›†åè®®ä¸‹çš„æ•°æ®æ ·æœ¬åŒ¹é…ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥ç ”ç©¶ç»“æœä¸ºåŒ»ç–—æ•°æ®å…±äº«æ”¿ç­–çš„åˆ¶å®šæä¾›äº†é‡è¦çš„æ”¯æŒï¼Œå°¤å…¶æ˜¯åœ¨è€ƒè™‘éšç§ä¿æŠ¤çš„æƒ…å†µä¸‹ã€‚

**å…³é”®è¯**ï¼šè„‘æˆåƒ, MRI, æ•°æ®å…±äº«, éšç§é£é™©, å›¾åƒç›¸ä¼¼æ€§, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, æ•°æ®æ ·æœ¬åŒ¹é…, agent

**è¯„åˆ†**ï¼š62

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10043v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10043v1.pdf)

---

## [18. Perception with Guarantees: Certified Pose Estimation via Reachability Analysis](https://arxiv.org/abs/2602.10032v1)

**ä½œè€…**ï¼šTobias Ladner, Yasser Shoukry, Matthias Althoff  
**åˆ†ç±»**ï¼šcs.CV, cs.RO  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Agents in cyber-physical systems are increasingly entrusted with safety-critical tasks. Ensuring safety of these agents often requires localizing the pose for subsequent actions. Pose estimates can, e.g., be obtained from various combinations of lidar sensors, cameras, and external services such as GPS. Crucially, in safety-critical domains, a rough estimate is insufficient to formally determine safety, i.e., guaranteeing safety even in the worst-case scenario, and external services might additionally not be trustworthy. We address this problem by presenting a certified pose estimation in 3D solely from a camera image and a well-known target geometry. This is realized by formally bounding the pose, which is computed by leveraging recent results from reachability analysis and formal neural network verification. Our experiments demonstrate that our approach efficiently and accurately localizes agents in both synthetic and real-world experiments.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç›¸æœºå›¾åƒå’Œå·²çŸ¥ç›®æ ‡å‡ ä½•å½¢çŠ¶çš„è®¤è¯å§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œç¡®ä¿åœ¨æœ€åæƒ…å†µä¸‹çš„å®‰å…¨æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨å®‰å…¨å…³é”®çš„ç½‘ç»œç‰©ç†ç³»ç»Ÿä¸­ï¼Œç¡®ä¿ä»£ç†çš„å®‰å…¨æ€§éœ€è¦å¯é çš„å§¿æ€å®šä½ï¼Œè€Œå¸¸è§„ä¼°è®¡æ— æ³•æ»¡è¶³è¿™ä¸€è¦æ±‚ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡åˆ©ç”¨å¯è¾¾æ€§åˆ†æå’Œå½¢å¼ç¥ç»ç½‘ç»œéªŒè¯çš„æœ€æ–°æˆæœï¼Œæ­£å¼ç•Œå®šå§¿æ€çš„è¾¹ç•Œï¼Œä»è€Œå®ç°3Dè®¤è¯å§¿æ€ä¼°è®¡ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­å‡èƒ½é«˜æ•ˆä¸”å‡†ç¡®åœ°å®šä½ä»£ç†ã€‚

**å…³é”®è¯**ï¼šå§¿æ€ä¼°è®¡, ä»£ç†, å®‰å…¨, è®¡ç®—æœºè§†è§‰, reachability analysis, ç¥ç»ç½‘ç»œ, 3Då®šä½, å½¢å¼éªŒè¯, ä¼ æ„Ÿå™¨èåˆ, neural network

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10032v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10032v1.pdf)

---

## cs.LG

## [19. Biases in the Blind Spot: Detecting What LLMs Fail to Mention](https://arxiv.org/abs/2602.10117v1)

**ä½œè€…**ï¼šIvÃ¡n Arcuschin, David Chanin, AdriÃ  Garriga-Alonso ç­‰ 4 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model's CoTs. We evaluate our pipeline across six LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality). In the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity). More broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºä¸€ç§å…¨è‡ªåŠ¨é»‘ç®±ç®¡é“ï¼Œæ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æœªè¡¨è¿°åè§ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„ä»»åŠ¡ç‰¹å®šåè§å‘ç°æ–¹æ³•ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹å¸¸å¸¸éšè—å†…åœ¨åè§ï¼Œç°æœ‰çš„åè§è¯„ä¼°æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰ç±»åˆ«å’Œæ‰‹å·¥æ•°æ®é›†ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ›´æœ‰æ•ˆçš„æ£€æµ‹æ–¹å¼ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šç ”ç©¶ä¸­å¼•å…¥äº†ä¸€ç§é»‘ç®±ç®¡é“ï¼Œåˆ©ç”¨LLMè‡ªåŠ¨è¯„åˆ†ç”Ÿæˆå€™é€‰åè§æ¦‚å¿µï¼Œå¹¶é€šè¿‡ç»Ÿè®¡æŠ€æœ¯è¿›è¡Œå¤šæ¬¡æµ‹è¯•å’Œæ—©æœŸåœæ­¢ï¼Œä»¥è¯†åˆ«æœªè¡¨è¿°çš„åè§ã€‚

**ä¸»è¦ç»“è®º**ï¼šè¯¥æ–¹æ³•èƒ½è‡ªåŠ¨å‘ç°æ¨¡å‹ä¸­çš„æœªçŸ¥åè§ï¼Œå¹¶éªŒè¯å·²æœ‰ç ”ç©¶è¯†åˆ«çš„åè§ï¼Œæä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å¯æ‰©å±•çš„åè§å‘ç°è·¯å¾„ã€‚

**å…³é”®è¯**ï¼šåè§, LLM, è‡ªåŠ¨åŒ–, é»‘ç®±, ä»»åŠ¡ç‰¹å®š, ç»Ÿè®¡æŠ€æœ¯, è¯­è¨€æ¨¡å‹, ç”Ÿæˆ, è¯„ä¼°, å‘ç°

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10117v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10117v1.pdf)

---

## [20. Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy](https://arxiv.org/abs/2602.10100v1)

**ä½œè€…**ï¼šJÃºlio Oliveira, Rodrigo Ferreira, AndrÃ© Riker ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.CR  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå·®åˆ†éšç§ä¸å¯è§£é‡Šæ€§çš„è”é‚¦å­¦ä¹ æ¨¡å‹FEXT-DPï¼Œä»¥æå‡æ•°æ®éšç§ä¿æŠ¤å’Œå¯è§£é‡Šæ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€æ•°æ®éšç§å’Œå¯è§£é‡Šæ€§åœ¨ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„é‡è¦æ€§å¢åŠ ï¼Œç ”ç©¶æ—¨åœ¨å°†è¿™ä¸¤è€…ç»“åˆï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºçš„FEXT-DPåŸºäºå†³ç­–æ ‘ï¼Œå¹¶åœ¨æ¨¡å‹ä¸­åº”ç”¨å·®åˆ†éšç§ï¼Œä»¥å¢å¼ºæ•°æ®éšç§ä¿æŠ¤ï¼ŒåŒæ—¶è€ƒè™‘å¯è§£é‡Šæ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ€§èƒ½è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFEXT-DPåœ¨è®­ç»ƒé€Ÿåº¦ã€å‡æ–¹è¯¯å·®å’Œå¯è§£é‡Šæ€§ç­‰æ–¹é¢å‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚

**å…³é”®è¯**ï¼šè”é‚¦å­¦ä¹ , è§£é‡Šæ€§, æ•°æ®éšç§, æœºå™¨å­¦ä¹ , å·®åˆ†éšç§, å†³ç­–æ ‘, æ¨¡å‹è§£é‡Šæ€§, è®­ç»ƒæ•ˆç‡, ç‰¹å¾é€‰æ‹©, artificial intelligence

**è¯„åˆ†**ï¼š57

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10100v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10100v1.pdf)

---

## [21. Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders](https://arxiv.org/abs/2602.10099v1)

**ä½œè€…**ï¼šAmandeep Kumar, Vishal M. Patel  
**åˆ†ç±»**ï¼šcs.LG, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹æ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•æµåŒ¹é…è§£å†³æ ‡å‡†æ‰©æ•£å˜æ¢å™¨åœ¨è¡¨ç¤ºç¼–ç å™¨ä¸Šçš„æ”¶æ•›é—®é¢˜ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šæ ‡å‡†æ‰©æ•£å˜æ¢å™¨åœ¨å¤„ç†è¡¨ç¤ºç¼–ç å™¨æ—¶å­˜åœ¨æ”¶æ•›é—®é¢˜ï¼Œç°æœ‰çš„å®½åº¦æ‰©å±•è§£å†³æ–¹æ¡ˆæ—¢æ˜‚è´µåˆæœªèƒ½è§£å†³æ ¹æœ¬åŸå› ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰é›…å¯æ¯”æ­£åˆ™åŒ–çš„é»æ›¼æµåŒ¹é…ï¼ˆRJFï¼‰ï¼Œé€šè¿‡çº¦æŸç”Ÿæˆè¿‡ç¨‹åœ¨æµå½¢æµ‹åœ°çº¿ä¸Šå¹¶çº æ­£æ›²ç‡å¼•èµ·çš„è¯¯å·®ä¼ æ’­ï¼Œä»è€Œæ”¹å–„äº†æ‰©æ•£å˜æ¢å™¨çš„æ”¶æ•›æ€§ã€‚

**ä¸»è¦ç»“è®º**ï¼šåº”ç”¨RJFåï¼Œæ ‡å‡†DiT-Bæ¶æ„èƒ½å¤Ÿæœ‰æ•ˆæ”¶æ•›ï¼ŒFIDå€¼è¾¾åˆ°3.37ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•çš„è¡¨ç°ã€‚

**å…³é”®è¯**ï¼šç”Ÿæˆæ¨¡å‹, è¡¨å¾ç¼–ç å™¨, æ‰©æ•£å˜æ¢å™¨, ç”Ÿæˆå»ºæ¨¡, å‡ ä½•å¹²æ‰°, Riemannian Flow Matching, Jacobi Regularization, é«˜ä¿çœŸåˆæˆ, ä½å¯†åº¦ç‰¹å¾ç©ºé—´, generative

**è¯„åˆ†**ï¼š55

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10099v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10099v1.pdf)

---

## [22. Step-resolved data attribution for looped transformers](https://arxiv.org/abs/2602.10097v1)

**ä½œè€…**ï¼šGeorgios Kaissis, David Mildenberger, Juan Felipe Gomez ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $Ï„$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$Ï„$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•SDIï¼Œç”¨äºåˆ†æå¾ªç¯å˜æ¢å™¨ä¸­è®­ç»ƒæ ·æœ¬çš„å…·ä½“å½±å“ï¼Œæä¾›åˆ†æ­¥éª¤çš„è§£é‡Šèƒ½åŠ›ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„æ•°æ®å½±å“è¯„ä¼°æ–¹æ³•æ— æ³•æ­ç¤ºè®­ç»ƒæ ·æœ¬åœ¨å¾ªç¯è®¡ç®—ä¸­çš„å…·ä½“ä½œç”¨æ—¶é—´ï¼Œé™åˆ¶äº†å¯¹æ¨¡å‹å†…éƒ¨è®¡ç®—çš„ç†è§£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šå¼•å…¥Step-Decomposed Influence (SDI)ï¼Œé€šè¿‡å±•å¼€å¾ªç¯è®¡ç®—å›¾å°†å½±å“åˆ†è§£ä¸ºé•¿åº¦ä¸ºÏ„çš„è½¨è¿¹ï¼Œå¹¶æå‡ºTensorSketchå®ç°ä»¥æé«˜æ•ˆç‡ã€‚

**ä¸»è¦ç»“è®º**ï¼šSDIåœ¨å¾ªç¯GPTæ¨¡å‹å’Œç®—æ³•æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæä¾›é€æ­¥çš„è§£é‡Šï¼Œæ”¯æŒå¤šç§æ•°æ®å½’å› å’Œå¯è§£é‡Šæ€§ä»»åŠ¡ã€‚

**å…³é”®è¯**ï¼šå¾ªç¯å˜æ¢å™¨, è®­ç»ƒç¤ºä¾‹, å½±å“ä¼°è®¡, æ•°æ®å½’å› , æ·±åº¦å­¦ä¹ , GPT, å½±å“è½¨è¿¹, è§£é‡Šæ€§ä»»åŠ¡, TensorSketch

**è¯„åˆ†**ï¼š62

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10097v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10097v1.pdf)

---

## [23. Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability](https://arxiv.org/abs/2602.10067v1)

**ä½œè€…**ï¼šAaditya Vikram Prasad, Connor Watts, Jack Merullo ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç‰¹å¾ä½œä¸ºå¥–åŠ±æ¥è¿›è¡Œå¼€æ”¾å¼ä»»åŠ¡ç›‘ç£çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç°è±¡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šéšç€è¯­è¨€æ¨¡å‹åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ™®åŠï¼Œç ”ç©¶è€…å‘ç°è¿™äº›æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç¼–ç æŠ½è±¡æ¦‚å¿µçš„ç‰¹å¾ï¼Œè€Œè¿™äº›ç‰¹å¾å¯ä»¥ç”¨äºæ”¹è¿›æ¨¡å‹çš„è¡Œä¸ºå’Œç›‘ç£ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šè®ºæ–‡è®¾è®¡äº†ä¸€ç§åä¸ºRLFRçš„å¼ºåŒ–å­¦ä¹ ç®¡é“ï¼Œåˆ©ç”¨ç‰¹å¾ä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œé€šè¿‡è¯†åˆ«å€™é€‰å¹»è§‰å£°æ˜æ¥æŒ‡å¯¼æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶è¿›è¡Œå¹²é¢„å’Œä¿®æ­£ã€‚

**ä¸»è¦ç»“è®º**ï¼šå®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•çš„æ¨¡å‹åœ¨å¹»è§‰å‘ç”Ÿç‡ä¸Šå‡å°‘äº†58%ï¼ŒåŒæ—¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¿æŒä¸å˜ï¼Œå±•ç¤ºäº†ç‰¹å¾å¯¼å‘ç›‘ç£çš„æ–°èŒƒå¼ã€‚

**å…³é”®è¯**ï¼šç‰¹å¾å¥–åŠ±, å¯æ‰©å±•ç›‘ç£, å¼€æ”¾å¼ä»»åŠ¡, å¼ºåŒ–å­¦ä¹ , RLFR, ç‰¹å¾å‡½æ•°, æ¨¡å‹å¹²é¢„, äº‹å®æ€§, è¯­è¨€æ¨¡å‹, ç›‘æ§æœºåˆ¶, agent

**è¯„åˆ†**ï¼š72

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10067v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10067v1.pdf)

---

## [24. Vendi Novelty Scores for Out-of-Distribution Detection](https://arxiv.org/abs/2602.10062v1)

**ä½œè€…**ï¼šAmey P. Pasarkar, Adji Bousso Dieng  
**åˆ†ç±»**ï¼šcs.LG, cs.CV  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems. Existing post-hoc detectors typically rely on model confidence scores or likelihood estimates in feature space, often under restrictive distributional assumptions. In this work, we introduce a third paradigm and formulate OOD detection from a diversity perspective. We propose the Vendi Novelty Score (VNS), an OOD detector based on the Vendi Scores (VS), a family of similarity-based diversity metrics. VNS quantifies how much a test sample increases the VS of the in-distribution feature set, providing a principled notion of novelty that does not require density modeling. VNS is linear-time, non-parametric, and naturally combines class-conditional (local) and dataset-level (global) novelty signals. Across multiple image classification benchmarks and network architectures, VNS achieves state-of-the-art OOD detection performance. Remarkably, VNS retains this performance when computed using only 1% of the training data, enabling deployment in memory- or access-constrained settings.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºVendiåˆ†æ•°çš„Vendiæ–°é¢–æ€§è¯„åˆ†ï¼ˆVNSï¼‰ï¼Œç”¨äºé«˜æ•ˆçš„å¼‚å¸¸æ£€æµ‹ï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½å’Œè¾ƒä½çš„èµ„æºéœ€æ±‚ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šé«˜æ•ˆçš„å¼‚å¸¸æ£€æµ‹å¯¹äºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ¨¡å‹çš„ç½®ä¿¡åº¦åˆ†æ•°æˆ–ç‰¹å¾ç©ºé—´çš„ä¼¼ç„¶ä¼°è®¡ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šVNSé€šè¿‡é‡åŒ–æµ‹è¯•æ ·æœ¬å¯¹å†…éƒ¨åˆ†å¸ƒç‰¹å¾é›†Vendiåˆ†æ•°çš„å¢ç›Šï¼Œæä¾›äº†ä¸€ç§åŸºäºå¤šæ ·æ€§çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦å’Œéå‚æ•°æ€§è´¨ã€‚

**ä¸»è¦ç»“è®º**ï¼šVNSåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»åŸºå‡†å’Œç½‘ç»œæ¶æ„ä¸­è¡¨ç°å‡ºå…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œä¸”åœ¨ä»…ä½¿ç”¨1%è®­ç»ƒæ•°æ®æ—¶ä»èƒ½ä¿æŒè‰¯å¥½æ•ˆæœï¼Œé€‚åˆèµ„æºå—é™çš„ç¯å¢ƒã€‚

**å…³é”®è¯**ï¼šæœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , OODæ£€æµ‹, Vendi Novelty Score, ç›¸ä¼¼æ€§åº¦é‡, éå‚æ•°æ–¹æ³•, æ•°æ®é›†çº§æ–°é¢–æ€§, å›¾åƒåˆ†ç±», machine learning

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10062v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10062v1.pdf)

---

## [25. WildCat: Near-Linear Attention in Theory and Practice](https://arxiv.org/abs/2602.10056v1)

**ä½œè€…**ï¼šTobias SchrÃ¶der, Lester Mackey  
**åˆ†ç±»**ï¼šcs.LG, stat.ML  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

We introduce WildCat, a high-accuracy, low-cost approach to compressing the attention mechanism in neural networks. While attention is a staple of modern network architectures, it is also notoriously expensive to deploy due to resource requirements that scale quadratically with the input sequence length $n$. WildCat avoids these quadratic costs by only attending over a small weighted coreset. Crucially, we select the coreset using a fast but spectrally-accurate subsampling algorithm -- randomly pivoted Cholesky -- and weight the elements optimally to minimise reconstruction error. Remarkably, given bounded inputs, WildCat approximates exact attention with super-polynomial $O(n^{-\sqrt{\log(\log(n))}})$ error decay while running in near-linear $O(n^{1+o(1)})$ time. In contrast, prior practical approximations either lack error guarantees or require quadratic runtime to guarantee such high fidelity. We couple this advance with a GPU-optimized PyTorch implementation and a suite of benchmark experiments demonstrating the benefits of WildCat for image generation, image classification, and language model KV cache compression.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šWildCatæ˜¯ä¸€ç§é«˜å‡†ç¡®ç‡ã€ä½æˆæœ¬çš„ç¥ç»ç½‘ç»œæ³¨æ„åŠ›æœºåˆ¶å‹ç¼©æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¿‘çº¿æ€§æ—¶é—´å†…å®ç°ç²¾ç¡®çš„æ³¨æ„åŠ›è¿‘ä¼¼ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°ä»£ç¥ç»ç½‘ç»œå¹¿æ³›ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†å…¶èµ„æºéœ€æ±‚éšè¾“å…¥åºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡å¢é•¿ï¼Œå› æ­¤éœ€è¦æœ‰æ•ˆçš„å‹ç¼©æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šWildCaté€šè¿‡é€‰æ‹©ä¸€ä¸ªå°çš„åŠ æƒæ ¸å¿ƒé›†ï¼Œå¹¶é‡‡ç”¨å¿«é€Ÿçš„è°±ç²¾ç¡®å­é‡‡æ ·ç®—æ³•ï¼ˆéšæœºä¸»è½´Choleskyï¼‰æ¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œå®ç°è¿‘çº¿æ€§è¿è¡Œæ—¶é—´ã€‚

**ä¸»è¦ç»“è®º**ï¼šWildCatåœ¨å›¾åƒç”Ÿæˆã€å›¾åƒåˆ†ç±»å’Œè¯­è¨€æ¨¡å‹KVç¼“å­˜å‹ç¼©ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå¹¶ä¸”å…¶å®ç°å·²ä¼˜åŒ–ä¸ºGPUå…¼å®¹çš„PyTorchä»£ç ã€‚

**å…³é”®è¯**ï¼šæ³¨æ„åŠ›æœºåˆ¶, ç¥ç»ç½‘ç»œ, æ·±åº¦å­¦ä¹ , è¿‘çº¿æ€§, å›¾åƒç”Ÿæˆ, è¯­è¨€æ¨¡å‹, PyTorch, ä½æˆæœ¬, é«˜å‡†ç¡®ç‡, è¯¯å·®æœ€å°åŒ–, ml

**è¯„åˆ†**ï¼š70

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10056v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10056v1.pdf)

---

## [26. Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization](https://arxiv.org/abs/2602.10048v1)

**ä½œè€…**ï¼šXinchen Han, Hossam Afifi, Michel Marot ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \textbf{F}ine-grained \textbf{G}roup policy \textbf{O}ptimization (\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»†ç²’åº¦çš„ç¾¤ä½“ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆFGOï¼‰ï¼Œæœ‰æ•ˆå‹ç¼©äº†é“¾å¼æ¨ç†è¿‡ç¨‹è€Œä¸æŸå¤±æ€§èƒ½ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé“¾å¼æ¨ç†æ—¶å¾€å¾€è¿‡äºå†—é•¿ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿå¢åŠ ï¼Œå› æ­¤éœ€è¦ä¸€ç§æœ‰æ•ˆçš„å‹ç¼©æ–¹æ³•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šFGOç®—æ³•é€šè¿‡ç»†åˆ†ç¾¤ä½“å“åº”å¹¶æ ¹æ®é•¿åº¦å’Œç†µåˆ†é…æƒé‡ï¼Œä¼˜åŒ–äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„ä¸è¶³ä¹‹å¤„ã€‚

**ä¸»è¦ç»“è®º**ï¼šFGOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜æ•ˆçš„é“¾å¼æ¨ç†å‹ç¼©èƒ½åŠ›ï¼ŒåŒæ—¶è§£å†³äº†GRPOçš„ä¸»è¦é™åˆ¶ï¼Œæœªé™ä½æ€§èƒ½ã€‚

**å…³é”®è¯**ï¼šé•¿é“¾æ€ç»´å‹ç¼©, å¼ºåŒ–å­¦ä¹ , å¤§è¯­è¨€æ¨¡å‹, CoT, ç»„ç­–ç•¥ä¼˜åŒ–, FGO, æ•°æ®åˆ©ç”¨æ•ˆç‡, ç†µå´©æºƒ, ç”Ÿæˆæ¨¡å‹, llm

**è¯„åˆ†**ï¼š68

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10048v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10048v1.pdf)

---

## [27. Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning](https://arxiv.org/abs/2602.10044v1)

**ä½œè€…**ï¼šAkshay Mete, Shahid Aamir Sheikh, Tzu-Hsiang Lin ç­‰ 5 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI, eess.SY  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæå‡ºäº†ä¸€ç§ä¹è§‚ä¸–ç•Œæ¨¡å‹ï¼ˆOWMsï¼‰ï¼Œæ—¨åœ¨æé«˜ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ æ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šé«˜æ•ˆæ¢ç´¢æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šOWMsé€šè¿‡å¼•å…¥ä¹è§‚åŠ¨æ€æŸå¤±æ¥å¢å¼ºæ¨¡å‹å­¦ä¹ ï¼Œåå‘äºé«˜å¥–åŠ±ç»“æœï¼Œä¸”æ— éœ€ä¼°è®¡ä¸ç¡®å®šæ€§æˆ–è¿›è¡Œçº¦æŸä¼˜åŒ–ã€‚

**ä¸»è¦ç»“è®º**ï¼šåœ¨ä¸¤ç§æœ€å…ˆè¿›çš„ä¸–ç•Œæ¨¡å‹æ¶æ„ä¸­åº”ç”¨OWMsï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œç´¯è®¡å›æŠ¥ã€‚

**å…³é”®è¯**ï¼šä¼˜åŒ–ä¸–ç•Œæ¨¡å‹, æ·±åº¦å¼ºåŒ–å­¦ä¹ , é‡‡æ ·æ•ˆç‡, æ¨¡å‹å­¦ä¹ , å¼ºåŒ–å­¦ä¹ , ä»£ç†äºº, ml

**è¯„åˆ†**ï¼š66

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10044v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10044v1.pdf)

---

## [28. Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems](https://arxiv.org/abs/2602.10037v1)

**ä½œè€…**ï¼šTetsuro Abe, Masashi Yamashita, Shu Tanaka  
**åˆ†ç±»**ï¼šcs.LG, cond-mat.stat-mech, quant-ph  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutations, the choice of binary encoding strongly affects search efficiency. If the encoding fails to reflect the original neighborhood structure, small Hamming moves may not correspond to meaningful modifications in the original solution space, and constrained problems can yield many infeasible candidates that waste evaluations. Recent work combines FMQA with a binary autoencoder (bAE) that learns a compact binary latent code from feasible solutions, yet the mechanism behind its performance gains is unclear. Using a small traveling salesman problem as an interpretable testbed, we show that the bAE reconstructs feasible tours accurately and, compared with manually designed encodings at similar compression, better aligns tour distances with latent Hamming distances, yields smoother neighborhoods under small bit flips, and produces fewer local optima. These geometric properties explain why bAE+FMQA improves the approximation ratio faster while maintaining feasibility throughout optimization, and they provide guidance for designing latent representations for black-box optimization.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šç ”ç©¶è¡¨æ˜ï¼ŒäºŒå…ƒè‡ªç¼–ç å™¨ï¼ˆbAEï¼‰åœ¨QUBOä¼˜åŒ–é—®é¢˜ä¸­èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é‡æ„å¯è¡Œè§£ï¼Œä»è€Œæå‡ä¼˜åŒ–æ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šåœ¨é»‘ç®±ç»„åˆä¼˜åŒ–ä¸­ï¼Œå¯»æ±‚é«˜è´¨é‡è§£çš„åŒæ—¶éœ€æ§åˆ¶è¯„ä¼°æˆæœ¬ï¼Œå› æ­¤æœ‰æ•ˆçš„ç¼–ç æ–¹å¼è‡³å…³é‡è¦ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šé€šè¿‡ä½¿ç”¨å°å‹æ—…è¡Œæ¨é”€å‘˜é—®é¢˜ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œç ”ç©¶bAEåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸æ‰‹åŠ¨è®¾è®¡çš„ç¼–ç è¿›è¡Œæ¯”è¾ƒã€‚

**ä¸»è¦ç»“è®º**ï¼šbAEç»“åˆFMQAèƒ½å¤Ÿæ›´å¿«æé«˜è¿‘ä¼¼æ¯”ï¼ŒåŒæ—¶ä¿æŒå¯è¡Œæ€§ï¼Œä¸”å…¶å‡ ä½•ç‰¹æ€§ä¸ºé»‘ç®±ä¼˜åŒ–ä¸­çš„æ½œåœ¨è¡¨ç¤ºè®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

**å…³é”®è¯**ï¼šäºŒè¿›åˆ¶è‡ªç¼–ç å™¨, ç»„åˆä¼˜åŒ–, é‡å­é€€ç«, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ, æœ€ä¼˜è§£, æ—…è¡Œæ¨é”€å‘˜é—®é¢˜, è¿‘ä¼¼æ¯”ç‡, æ½œåœ¨è¡¨ç¤º, agent

**è¯„åˆ†**ï¼š62

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10037v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10037v1.pdf)

---

## [29. Position: Message-passing and spectral GNNs are two sides of the same coin](https://arxiv.org/abs/2602.10031v1)

**ä½œè€…**ï¼šAntonis Vasileiou, Juan Cervino, Pascal Frossard ç­‰ 10 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Graph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as different parametrizations of permutation-equivariant operators acting on graph signals. From this perspective, many popular architectures are equivalent in expressive power, while genuine gaps arise only in specific regimes. We further argue that MPNNs and spectral GNNs offer complementary strengths. That is, MPNNs provide a natural language for discrete structure and expressivity analysis using tools from logic and graph isomorphism research, while the spectral perspective provides principled tools for understanding smoothing, bottlenecks, stability, and community structure. Overall, we posit that progress in graph learning will be accelerated by clearly understanding the key similarities and differences between these two types of GNNs, and by working towards unifying these perspectives within a common theoretical and conceptual framework rather than treating them as competing paradigms.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬è®ºæ–‡è®¤ä¸ºä¿¡æ¯ä¼ é€’ç¥ç»ç½‘ç»œå’Œè°±å›¾ç¥ç»ç½‘ç»œæ˜¯ç†è§£å›¾ä¿¡å·çš„ä¸åŒå‚æ•°åŒ–æ–¹å¼ï¼Œå¼ºè°ƒä¸¤è€…çš„äº’è¡¥æ€§ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šå½“å‰å¯¹å›¾ç¥ç»ç½‘ç»œçš„ç ”ç©¶åˆ†ä¸ºä¿¡æ¯ä¼ é€’å’Œè°±æ–¹æ³•ï¼Œè¿™ç§åˆ’åˆ†é˜»ç¢äº†é¢†åŸŸçš„å‘å±•ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºå°†MPNNså’Œè°±GNNsè§†ä¸ºåœ¨å›¾ä¿¡å·ä¸Šä½œç”¨çš„æ’åˆ—ä¸å˜ç®—å­çš„ä¸åŒå‚æ•°åŒ–ï¼Œä»è€Œæ­ç¤ºå…¶åœ¨è¡¨ç°åŠ›ä¸Šçš„ç­‰ä»·æ€§åŠäº’è¡¥ä¼˜åŠ¿ã€‚

**ä¸»è¦ç»“è®º**ï¼šæ·±å…¥ç†è§£è¿™ä¸¤ç§GNNçš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ï¼Œå°†ä¿ƒè¿›å›¾å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ï¼Œå»ºè®®åœ¨å…±åŒçš„ç†è®ºæ¡†æ¶ä¸‹ç»Ÿä¸€ç ”ç©¶è§†è§’ã€‚

**å…³é”®è¯**ï¼šå›¾ç¥ç»ç½‘ç»œ, æ¶ˆæ¯ä¼ é€’, è°±å›¾ç¥ç»ç½‘ç»œ, æœºå™¨å­¦ä¹ , è¡¨ç¤ºèƒ½åŠ›, å›¾ä¿¡å·, ç»“æ„åˆ†æ, äº’è¡¥ä¼˜åŠ¿, ç†è®ºæ¡†æ¶, æ·±åº¦å­¦ä¹ , machine learning

**è¯„åˆ†**ï¼š52

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10031v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10031v1.pdf)

---

## [30. ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning](https://arxiv.org/abs/2602.10019v1)

**ä½œè€…**ï¼šQingnan Ren, Shiting Huang, Zhen Fang ç­‰ 7 ä½ä½œè€…  
**åˆ†ç±»**ï¼šcs.LG, cs.AI  
**å‘å¸ƒæ—¶é—´**ï¼š2026-02-10

### ğŸ“„ è®ºæ–‡æ‘˜è¦

Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \textbf{ADORA} (\textbf{A}dvantage \textbf{D}ynamics via \textbf{O}nline \textbf{R}ollout \textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function's weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.

### ğŸ¤– AI æ€»ç»“

**ä¸€å¥è¯æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºADORAçš„åŠ¨æ€ä¼˜åŠ¿ä¼°è®¡æ¡†æ¶ï¼Œä»¥æé«˜å¼ºåŒ–å­¦ä¹ ä¸­æ¨ç†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚

**ç ”ç©¶åŠ¨æœº**ï¼šç°æœ‰çš„é™æ€ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•å¯¼è‡´äº†ä½æ•ˆçš„ä¿¡ç”¨åˆ†é…å’Œæ¨¡å‹çš„å­¦ä¹ ä¸ç¨³å®šæ€§ï¼Œè¿«åˆ‡éœ€è¦æ”¹è¿›ã€‚

**æ ¸å¿ƒæ–¹æ³•**ï¼šADORAé€šè¿‡åœ¨çº¿å›æ»šè°ƒæ•´ä¼˜åŠ¿å‡½æ•°çš„æƒé‡ï¼ŒåŠ¨æ€åŒºåˆ†è®­ç»ƒæ•°æ®ä¸­çš„æœ‰åˆ©å’Œä¸åˆ©æ ·æœ¬ï¼Œä»è€Œä¼˜åŒ–ç­–ç•¥æ›´æ–°ã€‚

**ä¸»è¦ç»“è®º**ï¼šADORAåœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®è§„æ¨¡ä¸‹è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†å‡ ä½•å’Œæ•°å­¦ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€æ•æ„Ÿçš„è¶…å‚æ•°è°ƒä¼˜ã€‚

**å…³é”®è¯**ï¼šå¼ºåŒ–å­¦ä¹ , åŠ¨æ€ä¼˜åŠ¿ä¼°è®¡, ç­–ç•¥ä¼˜åŒ–, æ¨¡å‹è®­ç»ƒ, ADORA, åœ¨çº¿å­¦ä¹ , æ•°æ®å·®å¼‚åŒ–, æ”¶ç›Šæ¨¡å‹, å¤šæ™ºèƒ½ä½“, ml

**è¯„åˆ†**ï¼š69

**è®ºæ–‡é“¾æ¥**ï¼š[æŸ¥çœ‹åŸæ–‡](https://arxiv.org/abs/2602.10019v1) | [ä¸‹è½½PDF](https://arxiv.org/pdf/2602.10019v1.pdf)

---

