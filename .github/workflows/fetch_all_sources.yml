name: Fetch All Sources

on:
  schedule:
    - cron: "10 9 * * *" # 每天 UTC 09:10 统一执行（北京时间 17:10）
  workflow_dispatch:
    inputs:
      force_run:
        description: "忽略现有数据检查，强制执行全部来源抓取"
        required: false
        default: false
        type: boolean

permissions:
  contents: write

jobs:
  fetch-all-sources:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.x"

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip list

    - name: Pre-check existing data by source and date
      id: precheck
      env:
        FORCE_RUN: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') && 'true' || 'false' }}
      run: |
        python - <<'PY'
        import os
        from datetime import datetime, timedelta, timezone
        from pathlib import Path

        import pandas as pd

        now = datetime.now(timezone.utc)
        today = now.strftime("%Y-%m-%d")
        yesterday = (now - timedelta(days=1)).strftime("%Y-%m-%d")
        force_run = os.getenv("FORCE_RUN", "false").lower() == "true"
        structured_dir = Path("data/structured")
        targets = {
            "producthunt": {"fetch_date": yesterday, "output_date": yesterday},
            "arxiv": {"fetch_date": yesterday, "output_date": today},
            "github": {"fetch_date": today, "output_date": today},
        }

        outputs = {}
        for source, cfg in targets.items():
            fetch_date = cfg["fetch_date"]
            output_date = cfg["output_date"]
            outputs[f"{source}_fetch_date"] = fetch_date
            outputs[f"{source}_output_date"] = output_date
            skip = False
            parquet_path = structured_dir / f"{output_date}.parquet"
            if force_run:
                skip = False
            elif parquet_path.exists():
                try:
                    df = pd.read_parquet(parquet_path, columns=["source"])
                    present_sources = set(
                        df["source"].dropna().astype(str).str.lower().tolist()
                    )
                    skip = source in present_sources
                except Exception as exc:
                    print(f"[WARN] 读取 {parquet_path} 失败，改为执行抓取: {exc}")
                    skip = False
            outputs[f"skip_{source}"] = "true" if skip else "false"
            action = "skip" if skip else "run"
            print(
                f"{source}: fetch_date={fetch_date}, output_date={output_date}, action={action}"
            )

        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            for key, value in outputs.items():
                f.write(f"{key}={value}\n")
        PY

    - name: Fetch Product Hunt
      if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') || steps.precheck.outputs.skip_producthunt != 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
        PRODUCTHUNT_DEVELOPER_TOKEN: ${{ secrets.PRODUCTHUNT_DEVELOPER_TOKEN }}
        PRODUCTHUNT_TARGET_DATE: ${{ steps.precheck.outputs.producthunt_output_date }}
      run: |
        python scripts/product_hunt_list_to_md.py

    - name: Fetch arXiv Papers
      if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') || steps.precheck.outputs.skip_arxiv != 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
        ARXIV_CATEGORIES: ${{ vars.ARXIV_CATEGORIES }}
        ARXIV_MAX_RESULTS: ${{ vars.ARXIV_MAX_RESULTS }}
        ARXIV_MODEL_NAME: ${{ vars.ARXIV_MODEL_NAME }}
        ARXIV_TARGET_DATE: ${{ steps.precheck.outputs.arxiv_fetch_date }}
        ARXIV_OUTPUT_DATE: ${{ steps.precheck.outputs.arxiv_output_date }}
      run: |
        export ARXIV_CATEGORIES="${ARXIV_CATEGORIES:-cs.AI,cs.LG,cs.CL,cs.CV}"
        export ARXIV_MAX_RESULTS="${ARXIV_MAX_RESULTS:-30}"
        python scripts/arxiv_papers_to_md.py

    - name: Fetch GitHub Trending
      if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') || steps.precheck.outputs.skip_github != 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
        GITHUB_LANGUAGE: ${{ vars.GITHUB_LANGUAGE }}
        GITHUB_SINCE: ${{ vars.GITHUB_SINCE }}
        GITHUB_MAX_RESULTS: ${{ vars.GITHUB_MAX_RESULTS }}
        GITHUB_MODEL_NAME: ${{ vars.GITHUB_MODEL_NAME }}
        GITHUB_TARGET_DATE: ${{ steps.precheck.outputs.github_output_date }}
      run: |
        export GITHUB_SINCE="${GITHUB_SINCE:-daily}"
        export GITHUB_MAX_RESULTS="${GITHUB_MAX_RESULTS:-25}"
        python scripts/github_trending_to_md.py

    - name: Generate Keyword Trends
      run: |
        python scripts/keyword_trends.py

    - name: Commit files
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add .
        git commit -m "Generated daily multi-source data" || echo "No changes to commit"

    - name: Push changes
      run: |
        git push https://${{ secrets.PAT }}@github.com/${{ github.repository }}.git HEAD:${{ github.ref_name }}
