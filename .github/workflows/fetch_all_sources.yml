name: Fetch All Sources

on:
  schedule:
    - cron: "10 9 * * *" # 每天 UTC 09:10 统一执行（北京时间 17:10）
  workflow_dispatch:
    inputs:
      force_run:
        description: "忽略现有数据检查，强制执行全部来源抓取"
        required: false
        default: false
        type: boolean

permissions:
  contents: write

jobs:
  fetch-all-sources:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.x"

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip list

    - name: Debug workflow context
      run: |
        echo "event_name=${{ github.event_name }}"
        echo "repository=${{ github.repository }}"
        echo "ref_name=${{ github.ref_name }}"
        echo "actor=${{ github.actor }}"

    - name: Validate required secrets
      id: secrets
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || vars.OPENAI_API_KEY }}
        PRODUCTHUNT_DEVELOPER_TOKEN: ${{ secrets.PRODUCTHUNT_DEVELOPER_TOKEN || vars.PRODUCTHUNT_DEVELOPER_TOKEN }}
      run: |
        if [ -z "${OPENAI_API_KEY}" ]; then
          echo "ERROR: OPENAI_API_KEY is required."
          exit 1
        fi

        if [ -z "${PRODUCTHUNT_DEVELOPER_TOKEN}" ]; then
          echo "ERROR: PRODUCTHUNT_DEVELOPER_TOKEN is required."
          exit 1
        fi

        echo "has_openai=true" >> "${GITHUB_OUTPUT}"
        echo "has_ph_token=true" >> "${GITHUB_OUTPUT}"

    - name: Pre-check existing data by source and date
      id: precheck
      env:
        FORCE_RUN: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') && 'true' || 'false' }}
      run: |
        python - <<'PY'
        import os
        from datetime import datetime, timedelta, timezone

        now = datetime.now(timezone.utc)
        today = now.strftime("%Y-%m-%d")
        yesterday = (now - timedelta(days=1)).strftime("%Y-%m-%d")
        force_run = os.getenv("FORCE_RUN", "false").lower() == "true"
        targets = {
            "producthunt": {"fetch_date": yesterday, "output_date": yesterday},
            # arXiv 采用当日窗口抓取并写当日，避免“抓昨天”导致空结果。
            "arxiv": {"fetch_date": today, "output_date": today},
            "github": {"fetch_date": today, "output_date": today},
        }

        outputs = {}
        for source, cfg in targets.items():
            fetch_date = cfg["fetch_date"]
            output_date = cfg["output_date"]
            outputs[f"{source}_fetch_date"] = fetch_date
            outputs[f"{source}_output_date"] = output_date
            # 始终执行抓取。若同日已存在数据，将在 storage 层按 source+date 覆盖。
            skip = False
            outputs[f"skip_{source}"] = "true" if skip else "false"
            action = "skip" if skip else "run"
            print(
                f"{source}: fetch_date={fetch_date}, output_date={output_date}, action={action}, force_run={force_run}"
            )

        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            for key, value in outputs.items():
                f.write(f"{key}={value}\n")
        PY

    - name: Fetch Product Hunt
      if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') || steps.precheck.outputs.skip_producthunt != 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || vars.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL || vars.OPENAI_BASE_URL }}
        PRODUCTHUNT_DEVELOPER_TOKEN: ${{ secrets.PRODUCTHUNT_DEVELOPER_TOKEN || vars.PRODUCTHUNT_DEVELOPER_TOKEN }}
        PRODUCTHUNT_MODEL_NAME: ${{ vars.PRODUCTHUNT_MODEL_NAME }}
        PRODUCTHUNT_TARGET_DATE: ${{ steps.precheck.outputs.producthunt_output_date }}
      run: |
        export PRODUCTHUNT_MODEL_NAME="${PRODUCTHUNT_MODEL_NAME:-gpt-5.1-2025-11-13}"
        python scripts/product_hunt_list_to_md.py

    - name: Fetch arXiv Papers
      if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') || steps.precheck.outputs.skip_arxiv != 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || vars.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL || vars.OPENAI_BASE_URL }}
        ARXIV_CATEGORIES: ${{ vars.ARXIV_CATEGORIES }}
        ARXIV_MAX_RESULTS: ${{ vars.ARXIV_MAX_RESULTS }}
        ARXIV_MODEL_NAME: ${{ vars.ARXIV_MODEL_NAME }}
        ARXIV_TARGET_DATE: ${{ steps.precheck.outputs.arxiv_fetch_date }}
        ARXIV_OUTPUT_DATE: ${{ steps.precheck.outputs.arxiv_output_date }}
      run: |
        export ARXIV_CATEGORIES="${ARXIV_CATEGORIES:-cs.AI,cs.LG,cs.CL,cs.CV}"
        export ARXIV_MAX_RESULTS="${ARXIV_MAX_RESULTS:-30}"
        export ARXIV_MODEL_NAME="${ARXIV_MODEL_NAME:-gpt-5.1-2025-11-13}"
        python scripts/arxiv_papers_to_md.py

    - name: Fetch GitHub Trending
      if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.force_run == 'true') || steps.precheck.outputs.skip_github != 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || vars.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL || vars.OPENAI_BASE_URL }}
        GITHUB_LANGUAGE: ${{ vars.GITHUB_LANGUAGE }}
        GITHUB_SINCE: ${{ vars.GITHUB_SINCE }}
        GITHUB_MAX_RESULTS: ${{ vars.GITHUB_MAX_RESULTS }}
        GITHUB_MODEL_NAME: ${{ vars.GITHUB_MODEL_NAME }}
        GITHUB_TARGET_DATE: ${{ steps.precheck.outputs.github_output_date }}
      run: |
        export GITHUB_SINCE="${GITHUB_SINCE:-daily}"
        export GITHUB_MAX_RESULTS="${GITHUB_MAX_RESULTS:-25}"
        export GITHUB_MODEL_NAME="${GITHUB_MODEL_NAME:-gpt-5.1-2025-11-13}"
        python scripts/github_trending_to_md.py

    - name: Post-check source completeness and quality
      env:
        HAS_OPENAI: ${{ steps.secrets.outputs.has_openai }}
      run: |
        python - <<'PY'
        import os
        from pathlib import Path
        import pandas as pd

        targets = {
            "producthunt": "${{ steps.precheck.outputs.producthunt_output_date }}",
            "arxiv": "${{ steps.precheck.outputs.arxiv_output_date }}",
            "github": "${{ steps.precheck.outputs.github_output_date }}",
        }

        def score_total(value):
            if isinstance(value, dict):
                return int(value.get("total", 0) or 0)
            return 0

        has_openai = os.getenv("HAS_OPENAI", "false").lower() == "true"
        errors = []
        for source, date_str in targets.items():
            parquet_path = Path("data/structured") / f"{date_str}.parquet"
            if not parquet_path.exists():
                errors.append(f"{source} {date_str}: parquet missing")
                continue
            try:
                df = pd.read_parquet(parquet_path)
            except Exception as exc:
                errors.append(f"{source} {date_str}: read parquet failed ({exc})")
                continue
            if "source" not in df.columns:
                errors.append(f"{source} {date_str}: source column missing")
                continue
            src_df = df[df["source"].astype(str).str.lower() == source]
            if src_df.empty:
                errors.append(f"{source} {date_str}: no rows")
                continue
            if "score" not in src_df.columns:
                errors.append(f"{source} {date_str}: score column missing")
                continue
            if has_openai and not src_df["score"].apply(score_total).gt(0).any():
                errors.append(f"{source} {date_str}: all scores are 0")

        if errors:
            print("Post-check failed:")
            for err in errors:
                print(" -", err)
            raise SystemExit(1)

        print("Post-check passed for all sources.")
        PY

    - name: Generate Keyword Trends
      run: |
        python scripts/keyword_trends.py

    - name: Commit files
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add .
        git commit -m "Generated daily multi-source data" || echo "No changes to commit"

    - name: Push changes
      run: |
        BRANCH="${{ github.ref_name }}"
        REMOTE_URL="https://${{ secrets.PAT }}@github.com/${{ github.repository }}.git"
        for i in 1 2 3; do
          git pull --rebase "${REMOTE_URL}" "${BRANCH}" && \
          git push "${REMOTE_URL}" "HEAD:${BRANCH}" && exit 0
          echo "Push failed on attempt ${i}, retrying..."
          sleep 5
        done
        echo "Push failed after retries."
        exit 1
